{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Publication PDFs to extract additional data for classification and filtering.\n",
    "The for publications in the UK Catalysis Hub app db contains a titles, IDs and DOIs. We can use PDF miner to look for keywords in the file. Wa can also use Chem Data Extractor to get lists of terms which can also be used to classify the publications.\n",
    "\n",
    "The steps of the process are: \n",
    " 1. get a Title, DOI, URL and pdf file name for each publication\n",
    " 2. get a list of terms (keywords) to search for\n",
    " 3. open a pdf file\n",
    " 4. search for the terms using PDFMiner\n",
    " 5. store terms found with a count of occurences per publication, assuming that repeated mentions imply relevance of term.\n",
    " 6. search for terms using chem data extractor\n",
    " 7. store chem data extractor terms with a counter of occurences, again assuming that repeated mentions indicate relevance\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "\n",
    "# import pdfminer functions\n",
    "from pdfminer import high_level as pdfmnr_hl\n",
    "\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "# custom libraries\n",
    "# import custom functions (common to various notebooks)\n",
    "import processing_functions as pr_fns\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get list of publications from DB\n",
    "Open publications DB and get list of publications including Title, ID and DOI.\n",
    "The list will be matched against list of publications. If the data file contains title in top pages, then it must be the article, proceed to analise in step 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set the name of currend app DB\n",
    "ukchapp_db = \"./db_files/development_2024.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "\n",
    "# Get publication data from the ukch app\n",
    "db_pubs = pr_fns.get_pub_data(ukchapp_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define list of search terms\n",
    "The initial list consists of experimental techniques commonly mentioned in UKCH papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of terms\n",
    "list_of_techniques = ['STEM', 'Scanning transmission electron microscopy',\n",
    "                      'TEM ','Transmission Electron Microscopy',\n",
    "                      'UV-VIS','Ultraviolet–visible spectroscopy','ultraviolet–visible spectrophotometry',\n",
    "                      'TGA','Thermogravimetric analysis','thermal gravimetric analysis',\n",
    "                      'XAFS','X-ray absorption fine structure',\n",
    "                      'SAXS','Small-angle X-ray scattering',\n",
    "                      'GC-MS','Gas chromatography–mass spectrometry',\n",
    "                      'HPLC','High-performance liquid chromatography','high-pressure liquid chromatography',\n",
    "                      'EXAFS','Extended X-Ray Absorption Fine Structure ',\n",
    "                      'FTIR','Fourier-transform infrared spectroscopy',\n",
    "                      'batch reaction testing',\n",
    "                      'flow reaction testing',\n",
    "                      'catalyst synthesis',\n",
    "                      'SEM','Scanning Electron Microscopy',\n",
    "                      'XAS','X-ray absorption spectroscopy',\n",
    "                      'XPS','X-ray photoelectron spectroscopy',\n",
    "                      'MD','Molecular dynamics',\n",
    "                      'QM/MM','quantum mechanics/molecular mechanics',\n",
    "                      'cluster model','Cluster analysis','clustering',\n",
    "                      'FIB','Focused Ion Beam',\n",
    "                      'XANES','X-ray absorption near edge structure', \n",
    "                      'NEXAFS','near edge X-ray absorption fine structure',\n",
    "                      'DFT','density functional theory',\n",
    "                      'XRD','X-ray Powder Diffraction',\n",
    "                      'pre-edge','pre-edge of XANES spectrum',\n",
    "                      'edge','edge of XANES spectrum',\n",
    "                      'near-edge','near-edge of XANES spectrum',\n",
    "                      'white line','white line of XANES spectrum',\n",
    "                      'EXAFS','Extended X-Ray Absorption Fine Structure ',\n",
    "                      'PDF','Pair Distribution Function',\n",
    "                      'INS', 'inelastic neutron scattering',\n",
    "                      'in elastic neutron scattering','in-elastic neutron scattering',\n",
    "                      'QENS', 'quasi-elastic neutron scattering', \n",
    "                      'quasi elastic neutron scattering', 'quasielastic neutron scattering',]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PDFMiner search functions\n",
    "The functions below use pdfminer and a list of keywords to look up for matches in publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for PDFminer\n",
    "\n",
    "def get_pdf_text(pdf_file):\n",
    "    return pdfmnr_hl.extract_text(pdf_file)\n",
    "\n",
    "def is_keyword(sentence, kwd_find):\n",
    "    #print(sentence)\n",
    "    idx_kw = sentence.lower().index(kwd_find.lower())\n",
    "    after_kw = idx_kw + len(kwd_find)\n",
    "    before_kw = idx_kw - 1\n",
    "    before_char = after_char =  \"\"\n",
    "    if before_kw >= 0:\n",
    "        before_char = sentence[before_kw].lower()\n",
    "    if after_kw < len(sentence):\n",
    "        after_char = sentence[after_kw].lower()\n",
    "    if not(after_char.isalpha()) and not(before_char.isalpha()):\n",
    "        #print(before_char, sentence[idx_kw], after_char)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_all_keyword_sentences(pdf_text, kwd_find):\n",
    "    return_group = []\n",
    "    #print('Looking up: ', kwd_find)\n",
    "    #try:\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if kwd_find.lower() in sentence.lower():\n",
    "            idx = sentences.index(sentence)\n",
    "            if is_keyword(sentence, kwd_find):\n",
    "                groups.append([idx-1, idx, idx+1])\n",
    "\n",
    "    #if len(groups)>0:\n",
    "    #    print(groups)\n",
    "\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        if not full_sentence in return_group:\n",
    "            return_group.append(full_sentence)\n",
    "    #except:\n",
    "    #    print(\"Error reading pdf\")\n",
    "    return return_group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Search in pdf files for terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scman1\\AppData\\Local\\Temp\\ipykernel_26356\\1295564229.py:68: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for a_pub in tqdm_notebook(db_pubs):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210dc3fa9c9d47d999d55172a2fb39b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/741 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF filename pdf_files/ijms-24-14779-v2.pdf\n",
      "PDF filename pdf_files/ChemPlusChem-2023-Price-Impact_of_Porous_Silica_Nanosphere_Architectures.pdf\n",
      "PDF filename pdf_files/10.1016_j.apcata.2023.119442.pdf\n",
      "PDF filename pdf_files/ChemPlusChem - 2023 - Aljohani - Enhancing Hydrogen Production from the Photoreforming of Lignin.pdf\n",
      "PDF filename pdf_files/ChemPlusChem - 2023 - Peng - A Facile Synthesis Route to AuPd Alloys for the Selective Oxidation of 5‐Hydroxymethylfurfural.pdf\n",
      "PDF filename pdf_files/d3cp03167e.pdf\n",
      "PDF filename pdf_files/sun-et-al-2023-potassium-promoted-limestone-for-preferential-direct-hydrogenation-of-carbonates-in-integrated-co2.pdf\n",
      "PDF filename pdf_files/smith-et-al-2023-evaluating-heterodinuclear-mg(ii)m(ii)-(m-mn-fe-ni-cu-and-zn)-catalysts-for-the-chemical-recycling-of.pdf\n",
      "PDF filename pdf_files/catalysts-13-01489-v2.pdf\n",
      "PDF filename pdf_files/qiu-et-al-2023-compositional-evolution-of-individual-conps-on-co-tio2-during-co-and-syngas-treatment-resolved-through.pdf\n",
      "PDF filename pdf_files/catalysts-13-01499.pdf\n",
      "PDF filename pdf_files/ChemSusChem - 2023 - Al Sobhi - A Comparison of the Reactivity of the Lattice Nitrogen in Tungsten Substituted Co3Mo3N and.pdf\n",
      "PDF filename pdf_files/ChemBioChem - 2023 - Wahart - Harnessing a Biocatalyst to Bioremediate the Purification of Alkylglycosides.pdf\n",
      "PDF filename pdf_files/10.1016_j.apsadv.2023.100469.pdf\n",
      "PDF filename pdf_files/d2cy01699k.pdf\n",
      "PDF filename pdf_files/d3cy00289f.pdf\n",
      "PDF filename pdf_files/ledendecker-et-al-2020-isolated-pd-sites-as-selective-catalysts-for-electrochemical-and-direct-hydrogen-peroxide.pdf\n",
      "PDF filename pdf_files/10.1016_j.apcatb.2020.119062.pdf\n",
      "PDF filename pdf_files/10.1016_j.apcatb.2021.120652.pdf\n",
      "PDF filename pdf_files/10.1016_j.cej.2022.135394.pdf\n",
      "PDF filename pdf_files/10.1016_j.electacta.2021.138811.pdf\n",
      "PDF filename pdf_files/ruiz-esquius-et-al-2023-lithium-directed-transformation-of-amorphous-iridium-(oxy)hydroxides-to-produce-active-water.pdf\n",
      "PDF filename pdf_files/d3cy01064c.pdf\n",
      "PDF filename pdf_files/d3sc05516g.pdf\n",
      "PDF filename pdf_files/d3sc05105f.pdf\n",
      "PDF filename pdf_files/s10563-014-9173-z.pdf\n",
      "PDF filename pdf_files/c4cp04488f.pdf\n",
      "PDF filename pdf_files/ncomms7540.pdf\n",
      "PDF filename pdf_files/10.1002_anie.201504227.pdf\n",
      "PDF filename pdf_files/gould-et-al-2015-influence-of-composition-and-chemical-arrangement-on-the-kinetic-stability-of-147-atom-au-ag.pdf\n",
      "PDF filename pdf_files/gould-et-al-2015-understanding-the-thermal-stability-of-silver-nanoparticles-embedded-in-a-si.pdf\n",
      "PDF filename pdf_files/srep15979.pdf\n",
      "PDF filename pdf_files/lezcano-gonzalez-et-al-2015-determination-of-the-nature-of-the-cu-coordination-complexes-formed-in-the-presence-of-no.pdf\n",
      "PDF filename pdf_files/10.1016_j.carbon.2016.05.065.pdf\n",
      "PDF filename pdf_files/c6cp04622c.pdf\n",
      "PDF filename pdf_files/gould-et-al-2016-controlling-structural-transitions-in-auag-nanoparticles-through-precise-compositional-design.pdf\n",
      "PDF filename pdf_files/10.1002_anie.202006246.pdf\n",
      "PDF filename pdf_files/catalysts-10-01229.pdf\n",
      "PDF filename pdf_files/10.1016_j.cclet.2023.108737.pdf\n",
      "PDF filename pdf_files/ChemCatChem - 2024 - Bowker - Ensemble Effects on Methanol Oxidation to Formaldehyde on Ferric Molybdate Catalysts.pdf\n",
      "PDF filename pdf_files/cowie-et-al-2024-exploiting-organometallic-chemistry-to-functionalize-small-cuprous-oxide-colloidal-nanocrystals.pdf\n",
      "PDF filename pdf_files/ChemCatChem - 2024 - Siangwata - Room Temperature Hydrogenation of CO2 Utilizing a Cooperative Phosphorus Pyridone‐Based.pdf\n",
      "PDF filename pdf_files/10.1016_j.jcou.2024.102693.pdf\n",
      "PDF filename pdf_files/111903_1_5.0202197.pdf\n",
      "PDF filename pdf_files/rosetto-et-al-2024-high-molar-mass-polycarbonates-as-closed-loop-recyclable-thermoplastics.pdf\n",
      "PDF filename pdf_files/sun-et-al-2024-multiscale-investigation-of-the-mechanism-and-selectivity-of-co2-hydrogenation-over-rh(111).pdf\n",
      "PDF filename pdf_files/nanomaterials-14-00615-v2.pdf\n",
      "PDF filename pdf_files/2021hawkinsphd.pdf\n",
      "PDF filename pdf_files/2021ZachariouPhD.pdf\n",
      "PDF filename pdf_files/Farmer_10133548_thesis_sig_removed.pdf\n",
      "PDF filename pdf_files/ThesisDonatoDecarolis2019.pdf\n",
      "PDF filename pdf_files/dorota_matras_phd.pdf\n",
      "PDF filename pdf_files/EmmaCampbell_HydrocarbonPoolMechanismsAsStudiedByKerrGatedRaman.pdf\n",
      "PDF filename pdf_files/final_thesis_jherbert.pdf\n",
      "PDF filename pdf_files/ThesisMirenAgoteAran.pdf\n",
      "PDF filename pdf_files/Thesis_Final_NEO_Accepted.pdf\n",
      "*************************\n",
      "Missing file for: pdf_files/NA for \n",
      "*************************\n",
      "PDF filename pdf_files/PipHellier-Thesis.pdf\n",
      "PDF filename pdf_files/Rachel_Blackmore_PhD_thesis_for_Award.pdf\n",
      "PDF filename pdf_files/CathBrookesThesis.pdf\n",
      "PDF filename pdf_files/AnnaGould_entire_thesis.pdf\n",
      "PDF filename pdf_files/JonesWilmFinalThesis.pdf\n",
      "PDF filename pdf_files/Alexander_OMalley_Thesis_Corrected.pdf\n",
      "PDF filename pdf_files/Thesis_AVamvakeros.pdf\n",
      "PDF filename pdf_files/MonikPanchal_Thesis.pdf\n",
      "PDF filename pdf_files/Stefan Kucharski_Thesis.pdf\n"
     ]
    }
   ],
   "source": [
    "# provide the list of terms to search for\n",
    "\n",
    "terms_to_find = list_of_techniques\n",
    "\n",
    "net_zero = ['water', 'energy', 'waste minimisation', 'material reuse', 'gaseous emissions', \n",
    "            'circular economy', 'sustainable manufacturing', 'environment', 'innovation', 'water energy',\n",
    "            'waste water', 'energy efficient', 'Energy and fuels from waste water', \n",
    "            'Life cycle sustainability', 'water shortages', 'changes in the climate',\n",
    "            'sustainable energy supplies', 'life cycle analysis', 'treatment of water',\n",
    "            'polluted water', 'sustainable energy', 'biomass', 'reducing the process energy demands', \n",
    "            'biofuels', 'Catalysis at the Water and Energy Nexus', 'chemical recycling', 'waste CO2', \n",
    "            'plastics recycle/reuse', 'conversion of carbon dioxide', 'renewable feedstocks', \n",
    "            'environmental impact', 'Earth-abundant metals', 'plastics will be recycled', \n",
    "            'high performance plastics', 'recycling of polymers', 'reusable monomers', \n",
    "            'sustainable polymer production', 'Catalysis for the Circular Economy and Sustainable Manufacturing',\n",
    "            'more effective use of water and energy', 'reduction in gaseous emissions', \n",
    "            'End Of Life Plastic', 'ELP', 'Renewable Energy Technologies', 'Carbon Dioxide', 'Methanol', \n",
    "            'Hybrid Vehicle Emissions', 'emission control', 'pollutants', 'Solar-driven', \n",
    "            'regeneration', 'water treatment', 'microplastics', 'Reduction of Carbon Dioxide',\n",
    "            'plastics waste', 'mixed plastics waste (MPW)', 'hydrogen', 'renewable hydrogen', \n",
    "            'Recycling of Waste Oxygenated Plastic', 'conversion of CO2',\n",
    "            'catalyst modelling for the circular economy', 'solar fuels', 'emissions control',\n",
    "            'water purification', 'particulate destruction in automotive exhaust', \n",
    "            'clean hydrogen production', 'energy materials', 'catalysis for deNOx reactions', \n",
    "            'hydrogen effect promotion of the CO oxidation', 'methane conversion', \n",
    "            'methanol conversion', 'renewable sources of energy', 'green hydrogen generation', \n",
    "            'renewables', 'sustainable alternatives', 'regenerative fuel cells', \n",
    "            'sustainable manufacturing of pharmaceuticals', 'sustainability considerations', 'waste plastics', \n",
    "            'recovery of energy', 'recycling process', 'marine debris', \n",
    "            'The Use of Sound for the Development of Water Electrolyser and Fuel', 'hydrogen production',\n",
    "            'Hydrogen peroxide', 'clean chemical processes']\n",
    "net_zero_singles = ['alternatives', 'automotive', 'automotive exhaust', 'biofuels', 'biomass', 'Carbon',\n",
    "                    'carbon dioxide', 'Carbon Dioxide', 'Catalysis', 'cells', 'changes', 'chemical',\n",
    "                    'circular', 'Circular', 'circular economy', 'clean', 'clean hydrogen', 'climate', 'CO',\n",
    "                    'CO oxidation', 'CO2', 'considerations', 'control', 'conversion', 'debris', 'demands',\n",
    "                    'deNOx', 'deNOx reactions', 'destruction', 'Development', 'Dioxide', 'Earth-abundant',\n",
    "                    'economy', 'efficient', 'Electrolyser', 'ELP', 'emission', 'emissions', 'Emissions', \n",
    "                    'emissions', 'End Of Life', 'End Of Life Plastic', 'energy', 'environment', \n",
    "                    'environmental', 'exhaust', 'feedstocks', 'fuel', 'fuel cells', 'fuels', 'gaseous', \n",
    "                    'generation', 'green', 'high performance', 'Hybrid', 'hydrogen', 'Hydrogen peroxide', \n",
    "                    'innovation', 'Life cycle', 'manufacturing', 'Manufacturing', 'manufacturing', 'marine',\n",
    "                    'material', 'methane', 'Methanol', 'methanol', 'microplastics', 'minimisation', \n",
    "                    'mixed plastic', 'mixed plastics', 'mixed plastics waste', 'monomers', \n",
    "                    'effective use', 'MPW', 'Nexus', 'oxidation', 'Oxygenated', 'Oxygenated Plastic', \n",
    "                    'particulate', 'particulate destruction', 'peroxide', 'pharmaceuticals', 'plastics', \n",
    "                    'pollutants', 'polluted', 'polymer', 'polymers', 'process', 'processes', 'production',\n",
    "                    'promotion', 'purification', 'reactions', 'recovery', 'recycle', 'recycled', 'recycling',\n",
    "                    'reducing', 'reduction', 'Reduction', 'regeneration', 'regenerative', 'renewable', \n",
    "                    'Renewable', 'renewable', 'renewables', 'reusable', 'reuse', 'shortages', 'solar', \n",
    "                    'solar fuels', 'Solar-driven', 'Sound', 'sources', 'supplies', 'sustainability', \n",
    "                    'sustainable', 'Sustainable', 'sustainable', 'treatment', 'Vehicle', 'Vehicle Emissions',\n",
    "                    'waste', 'waste plastics', 'water']\n",
    "\n",
    "just_neutrons = ['ISIS', 'ISIS Facility', 'ISIS Neutron and Muon Source',\n",
    "                 'INS', 'inelastic neutron scattering',\n",
    "                 'in elastic neutron scattering','in-elastic neutron scattering',\n",
    "                 'QENS', 'quasi-elastic neutron scattering', \n",
    "                 'quasi elastic neutron scattering', 'quasielastic neutron scattering',\n",
    "                 'TOSCA', 'Merlin', 'Iris', 'Osiris','neutron']\n",
    "\n",
    "muon_list =    ['MUON', 'Muon spectroscopy', 'ARGUS', 'CHRONUS', 'EMU', 'HiFi', 'MuSR']\n",
    "\n",
    "terms_to_find = list_of_techniques + muon_list\n",
    "data_records = {}\n",
    "term_mentions = {}\n",
    "\n",
    "\n",
    "for a_pub in tqdm_notebook(db_pubs):\n",
    "    if a_pub[0] > 972:\n",
    "        data_refs = []\n",
    "        data_sents = []\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        if pub_pdf == 'None':\n",
    "            print(\"*************************\")\n",
    "            print(\"Missing PDF for:\", pub_doi)\n",
    "            print(\"*************************\")\n",
    "        else:\n",
    "            pdf_file = \"pdf_files/\" + pub_pdf\n",
    "            if not Path(pdf_file).is_file():\n",
    "                print(\"*************************\")\n",
    "                print(\"Missing file for:\", pdf_file, \"for\", pub_doi)\n",
    "                print(\"*************************\")\n",
    "            else: \n",
    "                print(\"PDF filename\", pdf_file)\n",
    "                pdf_text = get_pdf_text(pdf_file)\n",
    "                for a_term in terms_to_find:\n",
    "                    data_sentences = get_all_keyword_sentences(pdf_text,a_term)\n",
    "                    for d_sentence in data_sentences:\n",
    "                        data_sents.append({'keyword':a_term,\"desc\":d_sentence, 'pub_id':pub_id})\n",
    "        terms_in_pub = {}\n",
    "        terms_in_pub['id'] = pub_id\n",
    "        terms_in_pub['doi'] = pub_doi\n",
    "        terms_in_pub['title'] = pub_title\n",
    "        for sent in data_sents:\n",
    "            if sent['keyword'] in terms_in_pub:\n",
    "                terms_in_pub[sent['keyword']] +=1\n",
    "            else:\n",
    "                terms_in_pub[sent['keyword']] = 1\n",
    "\n",
    "        term_mentions[pub_id]=terms_in_pub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. store terms found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'all_techniques_202405b.csv'\n",
    "if len(term_mentions) > 0:\n",
    "        csvh.write_csv_data(term_mentions, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_QENS = [20,22,23,26,41,98,99,107,108,110,112,116,\n",
    "                 119,120,247,268,272,366,585,587,594,595,\n",
    "                 601,605,665,668,673,725,729,731,]\n",
    "\n",
    "for a_pub in tqdm_notebook(db_pubs):   \n",
    "    if int(a_pub[0]) in previous_QENS:\n",
    "        data_refs = []\n",
    "        data_sents = []\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        if pub_pdf == 'None':\n",
    "            print(\"*************************\")\n",
    "            print(\"Missing PDF for:\", pub_doi)\n",
    "            print(\"*************************\")\n",
    "        else:\n",
    "            pdf_file = \"pdf_files/\" + pub_pdf\n",
    "            if not Path(pdf_file).is_file():\n",
    "                print(\"*************************\")\n",
    "                print(\"Missing file for:\", pdf_file, \"for\", pub_doi)\n",
    "                print(\"*************************\")\n",
    "            else: \n",
    "                print(\"PDF filename\", pdf_file)\n",
    "                pdf_text = get_pdf_text(pdf_file)\n",
    "                for a_term in terms_to_find:\n",
    "                    data_sentences = get_all_keyword_sentences(pdf_text,a_term)\n",
    "                    for d_sentence in data_sentences:\n",
    "                        data_sents.append({'keyword':a_term,\"desc\":d_sentence, 'pub_id':pub_id})\n",
    "        terms_in_pub = {}\n",
    "        terms_in_pub['id'] = pub_id\n",
    "        terms_in_pub['doi'] = pub_doi\n",
    "        terms_in_pub['title'] = pub_title\n",
    "        for sent in data_sents:\n",
    "            if sent['keyword'] in terms_in_pub:\n",
    "                terms_in_pub[sent['keyword']] +=1\n",
    "            else:\n",
    "                terms_in_pub[sent['keyword']] = 1\n",
    "\n",
    "        term_mentions[pub_id]=terms_in_pub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_mentions\n",
    "#terms_to_find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. search for terms using chem data extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. store chem data extractor terms found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer import high_level as pdfmnr_hl\n",
    "\n",
    "# functions for PDFminer\n",
    "def get_pdf_text(pdf_file):\n",
    "    return pdfmnr_hl.extract_text(pdf_file)\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_ref_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if pr_fns.is_data_stmt(sentence.lower()):\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1,idx,idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_all_data_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if 'data' in sentence.lower() or 'inform' in sentence.lower():\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1, idx, idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        if not full_sentence in return_group:\n",
    "            return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the http strings from references to data\n",
    "def get_http_ref(sentence):\n",
    "    http_frag = \"\"\n",
    "    if 'http' in sentence.lower():\n",
    "        idx_http = sentence.lower().index('http')\n",
    "        http_frag = sentence[idx_http:]\n",
    "        space_in_ref = True\n",
    "        while \" \" in http_frag:\n",
    "            space_idx = http_frag.rfind(\" \")\n",
    "            http_frag = http_frag[:space_idx]\n",
    "        if(http_frag[-1:]==\".\"):\n",
    "            http_frag = http_frag[:-1]\n",
    "    return http_frag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n",
    "# library for handling http requests\n",
    "import requests\n",
    "\n",
    "# to iterate on lists\n",
    "import itertools as itools\n",
    "\n",
    "# validate urls\n",
    "import validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom Functions\n",
    "# ** will migrate to lib if needed for more than one notebook\n",
    "\n",
    "# get the crossreference json page from doi\n",
    "def get_cr_json_object(cr_doi):\n",
    "  crjd = None\n",
    "  doi_file = 'json_files/' + cr_doi.replace('/','_').lower() + '.json'\n",
    "  if not Path(doi_file).is_file():\n",
    "    crjd = cr_api.getBibData(cr_doi)\n",
    "    with open(doi_file, 'w', encoding='utf-8-sig', errors='ignore') as f:\n",
    "                json.dump(crjd, f, ensure_ascii=False, indent=4)\n",
    "  else:\n",
    "    with open(doi_file, 'r', encoding='utf-8-sig') as jf:\n",
    "        crjd = json.load(jf)\n",
    "  # return the content and the file name \n",
    "  return crjd, doi_file\n",
    "\n",
    "# get the landing page for the publication from uri\n",
    "def get_pub_html_doi(cr_doi):\n",
    "    html_file = 'html_files/' + cr_doi.replace('/','_').lower() + '.html'\n",
    "    if not Path(html_file).is_file():\n",
    "        page_content = urlh.getPageFromDOI(doi_text)\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(page_content.decode(\"utf-8\") )\n",
    "    else:\n",
    "        f = open(html_file, \"r\")\n",
    "        page_content = f.read()\n",
    "    return page_content, html_file\n",
    "\n",
    "# get a list of titles from the previous searches database\n",
    "def get_titles(str_pub_title, db_name = \"prev_search.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'prev_pop_searches'\n",
    "    fields_required = \"Num, Title\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "# get a list of ids, titles, and dois from the app database\n",
    "def get_titles_and_dois(str_pub_title, db_name = \"app_db.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "# get a list of ids, titles, dois, links, pdf_file and \n",
    "# html_file names from the app database\n",
    "def get_pub_app_data(db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi, link\"\n",
    "    filter_str = \"status = 'Added'\"\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    #print(db_titles)\n",
    "    search_in = 'pdf_files'\n",
    "    filter_str = \"id >= 1\"\n",
    "    fields_required = \"id, doi, pdf_file\"\n",
    "    db_pdfs = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    #print(db_pdfs)\n",
    "    return_list = []\n",
    "    zip_obj = itools.zip_longest(db_titles,db_pdfs)\n",
    "    for an_art, a_pdf in zip_obj:\n",
    "        if(a_pdf != None and an_art[0] == a_pdf[0]):\n",
    "            return_list.append(tuple(an_art + (a_pdf[2], None)))\n",
    "        else:\n",
    "            return_list.append(tuple(an_art + (None, None)))\n",
    "        \n",
    "    db_conn.close()\n",
    "    return return_list\n",
    "\n",
    "# get the current csv working file\n",
    "def get_working_file(nr_wf):\n",
    "    working_file = wf_fields = None\n",
    "    current_pass = 0\n",
    "    if Path(nr_wf).is_file():\n",
    "        working_file, wf_fields = csvh.get_csv_data(nr_wf,'Num')\n",
    "        for art_num in tqdm_notebook(working_file):\n",
    "            if 'ignore' in working_file[art_num].keys():\n",
    "                if current_pass < int(working_file[art_num]['ignore']):\n",
    "                    current_pass = int(working_file[art_num]['ignore'])\n",
    "            else:\n",
    "                break\n",
    "    print(\"Current pass:\", current_pass)\n",
    "    return working_file, wf_fields, current_pass\n",
    "\n",
    "# get an htm file saved locally in the html_file folder \n",
    "def get_pub_html_url(text_url, entry_id):\n",
    "    html_file = 'html_files/' +  entry_id + '.html'\n",
    "    if not Path(html_file).is_file():\n",
    "        print(\"\")\n",
    "        page_content = urlh.getPageFromURL(text_url)\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(page_content)\n",
    "    else:\n",
    "        f = open(html_file, \"r\")\n",
    "        page_content = f.read()\n",
    "    return page_content, html_file\n",
    "\n",
    "# use regular expression to check if a given string\n",
    "# is a valid DOI, using pattern from CR\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    if cr_doi == None:\n",
    "        return False\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# get a semicolon separated list of authors from CR json data\n",
    "def get_cr_author_list(article_data):\n",
    "    authors = []\n",
    "    if 'author' in article_data.keys():\n",
    "        for author in article_data['author']:\n",
    "            new_author=\"\"\n",
    "            new_author = author['family']\n",
    "            if 'given' in author.keys():\n",
    "                new_author += \", \" + author['given']\n",
    "            authors.append(new_author)\n",
    "    return (\"; \").join(authors)\n",
    "\n",
    "# get the publication year from CR json data\n",
    "def get_cr_year_published(article_data):\n",
    "    year_print = 0\n",
    "    if 'published-print' in article_data.keys() \\\n",
    "        and article_data['published-print'] != None \\\n",
    "        and article_data['published-print']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['published-print']['date-parts'][0][0])    \n",
    "    elif 'journal-issue' in article_data.keys() \\\n",
    "        and article_data['journal-issue'] != None \\\n",
    "        and 'published-print' in article_data['journal-issue'].keys() \\\n",
    "        and article_data['journal-issue']['published-print'] != None \\\n",
    "        and article_data['journal-issue']['published-print']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['journal-issue']['published-print']['date-parts'][0][0])\n",
    "    year_online = 0\n",
    "    if 'published-online' in article_data.keys() \\\n",
    "        and article_data['published-online'] != None \\\n",
    "        and article_data['published-online']['date-parts'][0] != None:\n",
    "        year_online = int(article_data['published-online']['date-parts'][0][0])    \n",
    "    elif 'journal-issue' in article_data.keys() \\\n",
    "        and article_data['journal-issue'] != None \\\n",
    "        and 'published-online' in article_data['journal-issue'].keys() \\\n",
    "        and article_data['journal-issue']['published-online'] != None \\\n",
    "        and article_data['journal-issue']['published-online']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['journal-issue']['published-online']['date-parts'][0][0])\n",
    "    if year_print != 0 and year_online != 0:\n",
    "        return year_print if year_print < year_online else year_online\n",
    "    else:\n",
    "        return year_print if year_online == 0 else year_online\n",
    "    return 0\n",
    "\n",
    "# try to download a pdf from a given url\n",
    "def get_pdf_from_url(pdf_url):\n",
    "    fname = \"\"\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        content_type = response.headers['content-type']\n",
    "        print(\"Content Type:\", content_type )\n",
    "        print(response.headers)\n",
    "        cd= response.headers['content-disposition']\n",
    "        print(cd)\n",
    "        fname = re.findall(\"filename=(.+)\", cd)[0]\n",
    "        print(fname)\n",
    "        if not 'text' in content_type:\n",
    "            with open('pdf_files/'+ fname +'.pdf', 'wb') as f:\n",
    "                f.write(response.content)\n",
    "    except:\n",
    "        print(\"Error getting file from: \", pdf_url)\n",
    "    finally:\n",
    "        return fname\n",
    "# add name of the pdf file for a publication record in the app database     \n",
    "def set_pdf_file_value(file_name, pub_id, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    done = db_conn.set_value_table(table, pub_id, \"pdf_file\", file_name)\n",
    "    db_conn.close()\n",
    "    return done\n",
    "\n",
    "# try to get a pdf from elsevier\n",
    "def get_elsevier_pdf(doi):\n",
    "    pdf_url = f'http://api.elsevier.com/content/article/doi:{doi}?view=FULL'\n",
    "    print(\"\\t\", pdf_url) \n",
    "    return get_pdf_from_url(pdf_url)\n",
    "\n",
    "# try to get a pdf from wiley\n",
    "def get_wiley_pdf(doi):\n",
    "    pdf_url = f'https://onlinelibrary.wiley.com/doi/pdf/{doi}'\n",
    "    print(\"\\t\", pdf_url) \n",
    "    return get_pdf_from_url(pdf_url)\n",
    "\n",
    "def get_not_matched_files(db_name):\n",
    "    files_list = get_files_list(Path(\"pdf_files\"))\n",
    "    db_pubs = get_pub_app_data(db_name)\n",
    "    missing=[]\n",
    "    # check which files are really missing linking\n",
    "    for file in files_list:\n",
    "        found_in_db = False\n",
    "        for db_pub in db_pubs:\n",
    "            if file.name == db_pub[4]:\n",
    "                found_in_db = True\n",
    "                break\n",
    "        if not found_in_db:\n",
    "           missing.append(file) \n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for ChemDataExtractor\n",
    "# not used for mining data references (suplementary/raw) or to get pdf metadata\n",
    "from chemdataextractor import Document\n",
    "\n",
    "# A function for getting a list of files from the directory\n",
    "# This will be modified to get the list from a csv file\n",
    "def get_files_list (source_dir):\n",
    "    i_counter = 0\n",
    "    files_list = []\n",
    "    for filepath in sorted(source_dir.glob('*.pdf')):\n",
    "        i_counter += 1\n",
    "        files_list.append(filepath)\n",
    "    return files_list\n",
    "\n",
    "def cde_read_pdfs(a_file):\n",
    "    pdf_f = open(a_file, 'rb')\n",
    "    doc = Document.from_file(pdf_f)\n",
    "    return doc\n",
    "\n",
    "def find_doi(element_text):\n",
    "    cr_re_01 = '10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.search(cr_re_01, element_text, re.IGNORECASE)\n",
    "    if compare != None:\n",
    "        return compare.group()\n",
    "    return \"\"\n",
    "\n",
    "def get_db_id(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"id\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_db_title(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"title\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_close_dois(str_name, db_name = \"prev_search.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, doi, title, pdf_file\"\n",
    "    filter_str = \"doi like '%\"+str_name+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"db_files/app_db20211005.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pdf files for publications\n",
    "\n",
    "Read database and try to recover pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get publication data from the ukch app\n",
    "db_pubs = get_pub_app_data(ukchapp_db)\n",
    "for a_pub in tqdm_notebook(db_pubs):\n",
    "    if a_pub[0] > 688:\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        pub_html = a_pub[5]\n",
    "        if pub_pdf == None:\n",
    "            not_in_url = True\n",
    "            print(\"ID: \", pub_id, \"Publication: \",pub_title,\n",
    "                  \"\\n\\tDOI: \", pub_doi, \" URL: \", pub_url)\n",
    "            if \"pdf\" in pub_url and validators.url(pub_url):\n",
    "                print (\"\\tTry to get the pdf from URL: \", pub_url)\n",
    "                try:\n",
    "                    response = requests.get(pub_url)\n",
    "                    content_type = response.headers['content-type']\n",
    "                    if not 'text' in content_type:\n",
    "                        #print(response.headers)\n",
    "                        cd= response.headers['content-disposition']\n",
    "                        #print(cd)\n",
    "                        fname = re.findall(\"filename=(.+)\", cd)[0]\n",
    "                        #print(fname)\n",
    "                        if not Path('pdf_files/' + pdf_file).is_file():\n",
    "                            with open('pdf_files/'+ fname +'.pdf', 'wb') as f:\n",
    "                                f.write(response.content)\n",
    "                        else:\n",
    "                            set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                        not_in_url = False\n",
    "                except:\n",
    "                    print(\"ID: \", pub_id, \"\\nPublication: \",pub_title, \n",
    "                           \"\\nDOI: \", pub_doi, \"\\nURL: \", pub_url) \n",
    "            if not_in_url:\n",
    "                print(\"\\tTry to see if json file has link to pdf: \")\n",
    "                if valid_doi(pub_doi):\n",
    "                    crjd, doi_file = get_cr_json_object(pub_doi)\n",
    "                    got_pdf = False\n",
    "                    if \"link\" in crjd.keys():\n",
    "                        for a_link in crjd[\"link\"]:\n",
    "                            if \"\\tURL\" in a_link.keys() and (\"pdf\" in a_link[\"URL\"] or \"pdf\" in a_link[\"content-type\"]):\n",
    "                                cr_url = a_link[\"URL\"]\n",
    "                                #print(\"URL: \", cr_url)\n",
    "                                pdf_file = get_pdf_from_url(cr_url)\n",
    "                                # if the name corresponds to a existing file, assign value to db_record\n",
    "                                if Path('pdf_files/' + pdf_file).is_file():\n",
    "                                    print(\"\\tFile name:\", pdf_file)\n",
    "                                    set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                                    got_pdf = True\n",
    "                                else:\n",
    "                                    print(\"\\tcould not get file from\", cr_url)\n",
    "                    else: \n",
    "                        print(\"\\tno links in json\", pub_doi)\n",
    "                if not got_pdf and \"elsevier\" in pub_url:\n",
    "                    print(\"\\tTrying elsevier doi:\" )\n",
    "                    pdf_file = get_elsevier_pdf(pub_doi)\n",
    "                    if Path('pdf_files/' + pdf_file).is_file():\n",
    "                        print(\"\\tFile name:\", pdf_file)\n",
    "                        set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                        got_pdf = True\n",
    "                elif not got_pdf and \"wiley\" in pub_url:\n",
    "                    print(\"\\tTrying wiley doi:\" )\n",
    "                    pdf_file = get_wiley_pdf(pub_doi)\n",
    "                    if Path('pdf_files/' + pdf_file).is_file():\n",
    "                        print(\"\\tFile name:\", pdf_file)\n",
    "                        set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                        got_pdf = True\n",
    "\n",
    "                if not got_pdf:\n",
    "                    print(\"\\tTry doi:  https://doi.org/\" + pub_doi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('Accept', 'application/vnd.crossref.unixsd+xml')]\n",
    "r = opener.open('http://dx.doi.org/10.1002/anie.201901592')\n",
    "print (r.info()['Link'])\n",
    "\n",
    "import json\n",
    "s=[{\"URL\"=>\"https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1002%2Fanie.201901592\", \"content-type\"=>\"application/pdf\", \"content-version\"=>\"vor\", \"intended-application\"=>\"text-mining\"}, {\"URL\"=>\"https://onlinelibrary.wiley.com/doi/pdf/10.1002/anie.201901592\", \"content-type\"=>\"application/pdf\", \"content-version\"=>\"vor\", \"intended-application\"=>\"text-mining\"}, {\"URL\"=>\"https://onlinelibrary.wiley.com/doi/full-xml/10.1002/anie.201901592\", \"content-type\"=>\"application/xml\", \"content-version\"=>\"vor\", \"intended-application\"=>\"text-mining\"}, {\"URL\"=>\"https://onlinelibrary.wiley.com/doi/pdf/10.1002/anie.201901592\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
    "d = json.loads(s)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File name match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if file name matches some part of a doi\n",
    "files_list = get_not_matched_files(ukchapp_db)\n",
    "\n",
    "not_assigned = []\n",
    "for a_file in tqdm_notebook(files_list):\n",
    "    search_this = a_file.name.replace(\".pdf\", \"\").lower()\n",
    "    print(a_file.name,\"\\t\",search_this)\n",
    "    close_dois = get_close_dois(search_this, ukchapp_db)\n",
    "    print(len(close_dois))\n",
    "    \n",
    "    if len(close_dois) == 1 :\n",
    "        doi_dat = close_dois[0]\n",
    "        selected = False\n",
    "        if doi_dat[3] == None:\n",
    "            while not selected:\n",
    "                print(\"Assign file: \", a_file.name, \" to:\\n\\t\", doi_dat[0],doi_dat[1],doi_dat[2], doi_dat[3])\n",
    "                print('***************************************************************')\n",
    "                print(\"Options:\\n\\ta) assign\\n\\tb)go to next\")\n",
    "                print(\"selection:\")\n",
    "                usr_select = input()\n",
    "                if usr_select == 'a':\n",
    "                    selected = True\n",
    "                    set_pdf_file_value(a_file.name, doi_dat[0], ukchapp_db)\n",
    "                    print(\"assing and go to next\")\n",
    "                elif usr_select == 'b':\n",
    "                    #working_file[art_num]['ignore']=3 # visual inspection\n",
    "                    selected = True\n",
    "                    print(\"going to next\")\n",
    "        else:\n",
    "            print(\"Assigned in db: \",  doi_dat[0],doi_dat[1],doi_dat[2], doi_dat[3])\n",
    "    else:\n",
    "        not_assigned.append(a_file)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pdfminer to get metadata from pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = get_files_list(Path(\"pdf_files\"))\n",
    "db_pubs = get_pub_app_data(ukchapp_db)\n",
    "missing=[]\n",
    "# check which files are really missing linking\n",
    "for file in files_list:\n",
    "    found_in_db = False\n",
    "    for db_pub in db_pubs:\n",
    "        if file.name == db_pub[4]:\n",
    "            found_in_db = True\n",
    "            break\n",
    "    if not found_in_db:\n",
    "        missing.append(file)\n",
    "\n",
    "# check if all linked files are in the folder\n",
    "missing2=[]\n",
    "for db_pub in db_pubs:\n",
    "    found_in_system = False\n",
    "    for file in files_list:\n",
    "        if file.name == db_pub[4] or db_pub[4] == None:\n",
    "            found_in_system = True\n",
    "            break\n",
    "    if not found_in_system:\n",
    "        missing2.append(db_pub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ChemDataExtractor to read pdf and get DOIs in document\n",
    "for a_file in tqdm_notebook(not_assigned):\n",
    "    pdf_doc = cde_read_pdfs(a_file)\n",
    "    print(a_file.name)\n",
    "    dois_list = []\n",
    "    for element in pdf_doc.elements:\n",
    "        if 'doi' in str(element):\n",
    "            found_doi = find_doi(str(element))\n",
    "            if found_doi[-1:] == \".\":\n",
    "                found_doi = found_doi[:-1]\n",
    "            if not found_doi in dois_list:\n",
    "                dois_list.append(found_doi)       \n",
    "    \n",
    "    if dois_list != [] and len(dois_list) == 1:\n",
    "        for a_doi in dois_list:\n",
    "            close_dois = get_close_dois(a_doi, ukchapp_db)\n",
    "            selected = False\n",
    "            if len(close_dois) == 1:\n",
    "                doi_dat = close_dois[0]\n",
    "                if doi_dat[3] == None:\n",
    "                    while not selected:\n",
    "                        print(\"Assign file: \",a_file.name, \" to:\\n\\t\", doi_dat[0],doi_dat[1],doi_dat[2], doi_dat[3])\n",
    "                        print('***************************************************************')\n",
    "                        print(\"Options:\\n\\ta) assign\\n\\tb)go to next\")\n",
    "                        print(\"selection:\")\n",
    "                        usr_select = input()\n",
    "                        if usr_select == 'a':\n",
    "                            selected = True\n",
    "                            set_pdf_file_value(a_file.name, doi_dat[0], ukchapp_db)\n",
    "                            print(\"assing and go to next\")\n",
    "                        elif usr_select == 'b':\n",
    "                            #working_file[art_num]['ignore']=3 # visual inspection\n",
    "                            selected = True\n",
    "                            print(\"going to next\")\n",
    "                else: \n",
    "                    print(\"Already assingned to:\\n\\t\", doi_dat[0],doi_dat[1],doi_dat[2], doi_dat[3])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch to update most recent version of app DB\n",
    "ukchapp_db = \"db_files/app_db2.sqlite3\"\n",
    "ukchapp_db_prev = \"db_files/app_db.sqlite3\"\n",
    "db_pubs = get_pub_app_data(ukchapp_db)\n",
    "db_pubs_prev = get_pub_app_data(ukchapp_db_prev)\n",
    "\n",
    "for a_pub in tqdm_notebook(db_pubs):\n",
    "    for old_pub in db_pubs_prev:\n",
    "        if old_pub[0] == a_pub[0]:\n",
    "            if a_pub[4] != None:\n",
    "                print (\"************** Assigned:\", a_pub[4], old_pub[4])\n",
    "            elif a_pub[4] == None:\n",
    "                print(\"Assign file: \", old_pub[4], \" to:\\n\\t\",  a_pub[0], a_pub[1], a_pub[2]  )\n",
    "                set_pdf_file_value(old_pub[4], a_pub[0], ukchapp_db)\n",
    "                \n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
