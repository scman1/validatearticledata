{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data for new articles and list of all articles not citing data\n",
    "A list of publications is obtainded from the app database. This list will contain a titles, IDs and DOIs which need to be explored to look for the corresponding pdf files. \n",
    "The steps of the process are: \n",
    " 1. get a Title, DOI, and URL for each publication\n",
    " 2. convert the DOI to a pdf file name and try to open de file\n",
    " 3. use pdfMiner and/or CDE to get the reference to data\n",
    " 4. add a new dataset entry each time a new data object is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "from tqdm import tqdm\n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n",
    "# library for handling http requests\n",
    "import requests\n",
    "# import custom functions (common to various notebooks)\n",
    "import processing_functions as pr_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pdf and HTML names into app DB\n",
    "\n",
    "0. Add fields to articles table for holding pdf and html names\n",
    "1. Open the previously verified DB and get the publications list\n",
    "2. Open the current publication list from the appdb\n",
    "3. Get pdf and html file names from previous and put it in current\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scman1\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:21: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12947c0b41b149419786afafd032edba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=365.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# (0) Add fields\n",
    "# ALTER TABLE articles \n",
    "# ADD COLUMN pdf_file text;\n",
    "# ALTER TABLE articles \n",
    "# ADD COLUMN html_file text;\n",
    "\n",
    "#(1) previously verified files:\n",
    "prevapp_db = \"db_files/app_db2.sqlite3\"\n",
    "while not Path(prevapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    prevapp_db = input()\n",
    "    \n",
    "# get publication data from the db\n",
    "prev_pubs = pr_fns.get_pub_data(prevapp_db)\n",
    "    \n",
    "#2 currend app DB\n",
    "ukchapp_db = \"db_files/app_db20210702.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "    \n",
    "# get publication data from the ukch app\n",
    "app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "# 3 get pdf and html name from previous and put it in current\n",
    "for a_pub in tqdm_notebook(prev_pubs):\n",
    "    pub_id = a_pub[0]\n",
    "    pub_title = a_pub[1]\n",
    "    pub_doi = a_pub[2]\n",
    "    pub_url = a_pub[3]\n",
    "    pub_pdf = a_pub[4]\n",
    "    pub_html = a_pub[5]\n",
    "    match_found = False\n",
    "    for curr_pub in app_pubs:\n",
    "        if curr_pub[2] == pub_doi and pub_doi != None:\n",
    "            pr_fns.set_pdf_file_value(pub_pdf, curr_pub[0], ukchapp_db)\n",
    "            match_found = True\n",
    "            break\n",
    "        elif curr_pub[1] == pub_title:\n",
    "            pr_fns.set_pdf_file_value(pub_pdf, curr_pub[0], ukchapp_db)\n",
    "            match_found = True\n",
    "            break\n",
    "    if not match_found:\n",
    "        print(\"*******************\\n\",a_pub)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that pdf files exist \n",
    "\n",
    "Use the data on the articles table to verify if file are stored in the corresponding folder\n",
    "We also check that the files in the folder are all accounted for (have a corersponding record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scman1\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23bcc477028412598e2037d4c9873c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=403.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "1 Missing PDF for: 10.1002/9783527804085.ch10 64\n",
      "*************************\n",
      "2 Missing PDF for: 10.1016/b978-0-12-805324-9.09989-1 599\n",
      "*************************\n",
      "3 Missing PDF for: 10.1142/q0035 603\n",
      "*************************\n",
      "4 Missing PDF for: 10.1039/d0sc02253e 623\n",
      "*************************\n",
      "5 Missing PDF for: 10.1016/j.cell.2021.04.001 652\n",
      "*************************\n",
      "6 Missing PDF for: 10.1016/j.xcrp.2021.100349 653\n",
      "*************************\n",
      "7 Missing PDF for: 10.1039/D0CY01608J 654\n",
      "*************************\n",
      "8 Missing PDF for: 10.1021/acs.est.0c04279 655\n",
      "*************************\n",
      "9 Missing PDF for: 10.1021/jacsau.1c00145 656\n",
      "*************************\n",
      "10 Missing PDF for: 10.1039/D0CP01192D 657\n",
      "*************************\n",
      "11 Missing PDF for: 10.1021/acscatal.1c00997 658\n",
      "*************************\n",
      "12 Missing PDF for: 10.1007/s11244-018-0902-9 659\n",
      "*************************\n",
      "13 Missing PDF for: 10.1039/d1fd00027f 660\n",
      "*************************\n",
      "14 Missing PDF for: 10.1021/acscatal.0c05356 661\n",
      "*************************\n",
      "15 Missing PDF for: 10.1039/d0cy01779e 662\n",
      "*************************\n",
      "16 Missing PDF for: 10.1021/acsenergylett.0c02614 663\n",
      "*************************\n",
      "17 Missing PDF for: 10.1039/d1fd00004g 664\n",
      "*************************\n",
      "18 Missing PDF for: 10.3390/catal10121370 665\n",
      "*************************\n",
      "19 Missing PDF for: 10.1039/d1gc00901j 666\n",
      "*************************\n",
      "20 Missing PDF for: 10.1039/d1cp00979f 667\n",
      "*************************\n",
      "21 Missing PDF for: 10.1038/s41467-021-21062-1 668\n",
      "*************************\n",
      "22 Missing PDF for: 10.1021/acscatal.0c05413 669\n",
      "*************************\n",
      "23 Missing PDF for: 10.1021/acscatal.0c04858 670\n",
      "*************************\n",
      "24 Missing PDF for: 10.1002/ange.202015016 671\n",
      "*************************\n",
      "25 Missing PDF for: 10.1007/s11244-021-01449-6 672\n",
      "*************************\n",
      "26 Missing PDF for: 10.1039/d1cy00048a 673\n",
      "*************************\n",
      "27 Missing PDF for: 10.1088/1361-648x/abfe16 674\n",
      "*************************\n",
      "28 Missing PDF for: 10.1039/d0fd00135j 675\n",
      "*************************\n",
      "29 Missing PDF for: 10.3390/cryst10100941 676\n",
      "*************************\n",
      "30 Missing PDF for: 10.1088/1361-6463/abe9e1 677\n",
      "*************************\n",
      "31 Missing PDF for: 10.1039/d0sc03113e 678\n",
      "*************************\n",
      "32 Missing PDF for: 10.1021/acsaem.0c02060 679\n",
      "*************************\n",
      "33 Missing PDF for: 10.1007/s11244-021-01447-8 680\n",
      "*************************\n",
      "34 Missing PDF for: 10.1088/2515-7655/abdd82 681\n",
      "*************************\n",
      "35 Missing PDF for: 10.3390/catal10111289 682\n",
      "*************************\n",
      "36 Missing PDF for: 10.1021/acs.organomet.1c00055 683\n",
      "*************************\n",
      "37 Missing PDF for: 10.1021/acscatal.0c05019 684\n",
      "*************************\n",
      "38 Missing PDF for: 10.1021/acs.inorgchem.1c00327 685\n",
      "*************************\n",
      "39 Missing PDF for: 10.1039/d1fd00023c 686\n",
      "*************************\n",
      "40 Missing PDF for: 10.1002/smsc.202100032 687\n",
      "*************************\n",
      "41 Missing PDF for: 10.1039/d0gc02295k 688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get publication data from the ukch app\n",
    "app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "i_indx = 1\n",
    "for a_pub in tqdm_notebook(app_pubs):\n",
    "    pub_id = a_pub[0]\n",
    "    pub_title = a_pub[1]\n",
    "    pub_doi = a_pub[2]\n",
    "    pub_url = a_pub[3]\n",
    "    pub_pdf = a_pub[4]\n",
    "    pub_html = a_pub[5]\n",
    "    if pub_pdf == None:\n",
    "        print(\"*************************\")\n",
    "        print(i_indx, \"Missing PDF for:\", pub_doi, pub_id)\n",
    "        i_indx +=1\n",
    "    else:\n",
    "        pdf_file = \"pdf_files/\" + pub_pdf\n",
    "        if not Path(pdf_file).is_file():\n",
    "            print(\"*************************\")\n",
    "            print(i_indx, \"Missing file for:\", pdf_file, \"for\", pub_doi, pub_id)\n",
    "            i_indx +=1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scman1\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748dfeda1d444760ae89bfdbe94b0687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in DB: 1-s2.0-S0926860X18301686-main.pdf\n",
      "Not in DB: 287598212.pdf\n",
      "Not in DB: Bahruji2015_Article_RutileTiO2PdPhotocatalystsForH.pdf.pdf\n",
      "Not in DB: Bowker2015_Article_ThePhotocatalyticWindowPhoto-R.pdf.pdf\n",
      "Not in DB: c6nr00053c.pdf\n",
      "Not in DB: cs400683e.pdf.pdf\n",
      "Not in DB: cs502038y.pdf.pdf\n",
      "Not in DB: d0sc02253e1.pdf\n",
      "Not in DB: s23.pdf\n",
      "Not in DB: s5.pdf\n",
      "Not in DB: telluriumdoped_lanthanum_manganite_as_catalysts_for_the_oxygen_reduction_reaction.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for infile in tqdm_notebook(Path(\"pdf_files\").glob('*.pdf')):\n",
    "    file_found = False\n",
    "    for a_pub in app_pubs:\n",
    "        if infile.name == a_pub[4]:\n",
    "            file_found = True\n",
    "            break\n",
    "    if not file_found:\n",
    "        print(\"Not in DB:\", infile.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for ChemDataExtractor\n",
    "# not used for mining data references (suplementary/raw) or to get pdf metadata\n",
    "from chemdataextractor import Document\n",
    "\n",
    "# A function for getting a list of files from the directory\n",
    "# This will be modified to get the list from a csv file\n",
    "def get_files_list (source_dir):\n",
    "    i_counter = 0\n",
    "    files_list = []\n",
    "    for filepath in sorted(source_dir.glob('*.pdf')):\n",
    "        i_counter += 1\n",
    "        files_list.append(filepath)\n",
    "    return files_list\n",
    "\n",
    "def cde_read_pdfs(a_file):\n",
    "    pdf_f = open(a_file, 'rb')\n",
    "    doc = Document.from_file(pdf_f)\n",
    "    return doc\n",
    "\n",
    "def find_doi(element_text):\n",
    "    cr_re_01 = '10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.search(cr_re_01, element_text, re.IGNORECASE)\n",
    "    if compare != None:\n",
    "        return compare.group()\n",
    "    return \"\"\n",
    "\n",
    "def get_db_id(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"id\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_db_title(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"title\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_close_dois(str_name, db_name = \"prev_search.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, doi, title, pdf_file\"\n",
    "    filter_str = \"doi like '%\"+str_name+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# functions for PDFminer\n",
    "\n",
    "def get_pdf_text(pdf_file):\n",
    "    return extract_text(pdf_file)\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_ref_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if pr_fns.is_data_stmt(sentence.lower()):\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1,idx,idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_all_data_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if 'data' in sentence.lower() or 'inform' in sentence.lower():\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1, idx, idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        if not full_sentence in return_group:\n",
    "            return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the http strings from references to data\n",
    "def get_http_ref(sentence):\n",
    "    http_frag = \"\"\n",
    "    if 'http' in sentence.lower():\n",
    "        idx_http = sentence.lower().index('http')\n",
    "        http_frag = sentence[idx_http:]\n",
    "        space_in_ref = True\n",
    "        while \" \" in http_frag:\n",
    "            space_idx = http_frag.rfind(\" \")\n",
    "            http_frag = http_frag[:space_idx]\n",
    "        if(http_frag[-1:]==\".\"):\n",
    "            http_frag = http_frag[:-1]\n",
    "    return http_frag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"db_files/app_db2.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "ukchapp_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pdfminer to get metadata from pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get publication data from the ukch app\n",
    "db_pubs = pr_fns.get_pub_app_data(ukchapp_db)\n",
    "\n",
    "# get the list of dois already mined for data \n",
    "input_file = 'pub_data_add202012.csv'\n",
    "id_field = 'num'\n",
    "processed, headings = csvh.get_csv_data(input_file, id_field)\n",
    "for id_num in processed:\n",
    "    current_title = processed[id_num]['doi']\n",
    "processed[1]['num']\n",
    "\n",
    "processed_dois = []\n",
    "for entry in processed:\n",
    "    if not processed[entry]['doi'] in processed_dois:\n",
    "        processed_dois.append( processed[entry]['doi'])\n",
    "\n",
    "data_records = {}\n",
    "data_mentions = {}\n",
    "ref_count = mention_count = 0\n",
    "for a_pub in tqdm_notebook(db_pubs):\n",
    "    data_refs = []\n",
    "    data_sents = []\n",
    "    if a_pub[0] > 616:\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        pub_html = a_pub[5]\n",
    "        if pub_pdf == 'None':\n",
    "            print(\"*************************\")\n",
    "            print(\"Missing PDF for:\", pub_doi)\n",
    "            print(\"*************************\")\n",
    "        else:\n",
    "            pdf_file = \"pdf_files/\" + pub_pdf\n",
    "            if not Path(pdf_file).is_file():\n",
    "                print(\"*************************\")\n",
    "                print(\"Missing file for:\", pdf_file, \"for\", pub_doi)\n",
    "                print(\"*************************\")\n",
    "            else: \n",
    "                print(\"PDF filename\", pdf_file)\n",
    "                pdf_text = get_pdf_text(pdf_file)\n",
    "                ref_sentences = get_ref_sentences(pdf_text)\n",
    "                data_sentences = get_all_data_sentences(pdf_text)\n",
    "                for r_sentence in ref_sentences:\n",
    "                    dt_link = get_http_ref(r_sentence)\n",
    "                    if 'supplem' in r_sentence.lower():\n",
    "                        data_refs.append({'type':'supplementary',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                    else:\n",
    "                        data_refs.append({'type':'supporting',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                for d_sentence in data_sentences:\n",
    "                    dt_link = get_http_ref(d_sentence)\n",
    "                    if 'supplem' in d_sentence.lower():\n",
    "                        data_sents.append({'type':'supplementary',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "                    else:\n",
    "                        data_sents.append({'type':'supporting',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "        if data_refs != []:\n",
    "            for data_ref in data_refs:\n",
    "                data_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                data_record.update(data_ref)\n",
    "                data_records[ref_count] = data_record\n",
    "                ref_count += 1\n",
    "        if data_sents != []:\n",
    "            for data_sent in data_sents:\n",
    "                sentence_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                sentence_record.update(data_sent)\n",
    "                data_mentions[mention_count] = sentence_record\n",
    "                mention_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if len(data_records) > 0:\n",
    "#    csvh.write_csv_data(data_records, 'pdf_data.csv')\n",
    "    \n",
    "if len(data_mentions) > 0:\n",
    "    csvh.write_csv_data(data_mentions, 'pdf_mentions202012.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names and links for references in data mentions\n",
    "data_mentions, dm_fields = csvh.get_csv_data('pdf_mentions_filtered_02.csv', 'num')\n",
    "\n",
    "for dm in data_mentions:\n",
    "    print(\"https://doi.org/\" + data_mentions[dm]['doi'])\n",
    "    ref_name = data_mentions[dm]['ref_name']\n",
    "    while ref_name == \"\":\n",
    "        print('Please enter the name of data object:')\n",
    "        ref_name = input()\n",
    "    ref_link = data_mentions[dm]['ref_link']\n",
    "    while ref_link == \"\":\n",
    "        print('Please enter the data object link:')\n",
    "        ref_link = input()\n",
    "    data_mentions[dm]['ref_name'] = ref_name\n",
    "    data_mentions[dm]['ref_link'] = ref_link\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
