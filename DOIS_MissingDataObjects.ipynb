{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data for new articles and list of all articles not citing data\n",
    "A list of publications is obtainded from the app database. This list will contain a titles, IDs and DOIs which need to be explored to look for the corresponding pdf files. \n",
    "The steps of the process are: \n",
    " 1. get a Title, DOI, and URL for each publication\n",
    " 2. convert the DOI to a pdf file name and try to open de file\n",
    " 3. use pdfMiner and/or CDE to get the reference to data\n",
    " 4. add a new dataset entry each time a new data object is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import notebook \n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n",
    "# library for handling http requests\n",
    "import requests\n",
    "# import custom functions (common to various notebooks)\n",
    "import processing_functions as pr_fns\n",
    "\n",
    "current_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pdf_column_populated(data_db):\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    \n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "    i_indx = 0\n",
    "    for a_pub in app_pubs:\n",
    "        if a_pub[4] != None:\n",
    "            i_indx += 1\n",
    "    #print (i_indx/len(app_pubs) > 0.9, i_indx/len(app_pubs))\n",
    "    return (i_indx/len(app_pubs) > 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pdf and HTML names into app DB\n",
    "\n",
    "0. Add fields to articles table for holding pdf file names\n",
    "1. Open the previously verified DB and get the publications list\n",
    "2. Open the current publication list from the appdb\n",
    "3. Get pdf and html file names from previous and put it in current\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pdf_file_column(db_name, table_name, column_name, column_type):\n",
    "    if not column_exists(db_name, table_name, column_name):\n",
    "        ukchapp_db = \"db_files/\" + db_name + \".sqlite3\"\n",
    "        db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "        db_conn.add_column(table_name, column_name, column_type)\n",
    "    else:\n",
    "        print (column_name, \"Alredy exists in \", table_name)\n",
    "        \n",
    "def column_exists(db_name, table_name, column_name):\n",
    "    ukchapp_db = \"db_files/\" + db_name + \".sqlite3\"\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    ti=db_conn.get_table_info('articles')\n",
    "    for a_col in ti:\n",
    "        if a_col[1] == column_name:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "    \n",
    "def add_pdf_file_names(prev_db, curr_db):\n",
    "    has_file_names = False\n",
    "    \n",
    "    prevapp_db = \"db_files/\"+prev_db +\".sqlite3\"\n",
    "\n",
    "    while not Path(prevapp_db).is_file():\n",
    "        print('Please enter the name of app db file:')\n",
    "        prevapp_db = input()\n",
    "\n",
    "    # get publication data from the db\n",
    "    prev_pubs = pr_fns.get_pub_data(prevapp_db)\n",
    "\n",
    "    #2 currend app DB\n",
    "    ukchapp_db = \"db_files/\" + curr_db + \".sqlite3\"\n",
    "    while not Path(ukchapp_db).is_file():\n",
    "        print('Please enter the name of app db file:')\n",
    "        curr_db = input()\n",
    "        ukchapp_db = \"db_files/\" + curr_db + \".sqlite3\"\n",
    "    \n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "    \n",
    "    # check if file names have been added\n",
    "    # return true if OK\n",
    "    if pdf_column_populated(curr_db):\n",
    "        has_file_names = True\n",
    "        return curr_db, has_file_names\n",
    "\n",
    "    # 3 get pdf file name from previous and put it in current\n",
    "    for a_pub in notebook.tqdm(prev_pubs):\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        match_found = False\n",
    "        for curr_pub in app_pubs:\n",
    "            if curr_pub[2] == pub_doi and pub_doi != None:\n",
    "                pr_fns.set_pdf_file_value(pub_pdf, curr_pub[0], ukchapp_db)\n",
    "                match_found = True\n",
    "                break\n",
    "            elif curr_pub[1] == pub_title:\n",
    "                pr_fns.set_pdf_file_value(pub_pdf, curr_pub[0], ukchapp_db)\n",
    "                match_found = True\n",
    "                break\n",
    "        if not match_found:\n",
    "            print(\"*************\\n\",a_pub)\n",
    "\n",
    "        has_file_names = True\n",
    "    \n",
    "    return curr_db, has_file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that idexed pdf files exist \n",
    "\n",
    "Use the data on the articles table to verify if file are stored in the corresponding folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_data_exists(data_db):\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    \n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "    i_indx = 0\n",
    "    for a_pub in notebook.tqdm(app_pubs):\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        if pub_pdf == None:\n",
    "            print(\"*************************\")\n",
    "            i_indx +=1\n",
    "            print(i_indx, \"Missing PDF for:\", pub_doi, pub_id)\n",
    "            \n",
    "        else:\n",
    "            pdf_file = \"pdf_files/\" + pub_pdf\n",
    "            if not Path(pdf_file).is_file():\n",
    "                print(\"*************************\")\n",
    "                i_indx +=1\n",
    "                print(i_indx, \"Missing file for:\", pdf_file, \"for\", pub_doi, pub_id)\n",
    "                \n",
    "    #print(i_indx/len(app_pubs) )\n",
    "    # If less than 1% if missing that is OK\n",
    "    return (i_indx/len(app_pubs) < 0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that all PDF files are indexed \n",
    "\n",
    "Check that the files in the folder are all accounted for (have a corersponding record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_files_in_db(data_db):\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    \n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "    files_not_in_DB = 0\n",
    "    for infile in notebook.tqdm(Path(\"pdf_files\").glob('*.pdf')):\n",
    "        file_found = False\n",
    "        for a_pub in app_pubs:\n",
    "            if infile.name == a_pub[4]:\n",
    "                file_found = True\n",
    "                break\n",
    "        if not file_found:\n",
    "            print(\"Not in DB:\", infile.name)\n",
    "            files_not_in_DB += 1\n",
    "    return files_not_in_DB < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get missing pdfs\n",
    "If there are more than 1% missing try to get them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use regular expression to check if a given string\n",
    "# is a valid DOI, using pattern from CR\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    if cr_doi == None:\n",
    "        return False\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_missing_pdfs(data_db):\n",
    "    return_val = False\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    # get publication data from the ukch app\n",
    "    db_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "    for a_pub in notebook.tqdm(db_pubs):\n",
    "        if a_pub[0] > 0:\n",
    "            pub_id = a_pub[0]\n",
    "            pub_title = a_pub[1]\n",
    "            pub_doi = a_pub[2]\n",
    "            pub_url = a_pub[3]\n",
    "            pub_pdf = a_pub[4]\n",
    "            if pub_pdf == None:\n",
    "                not_in_url = True\n",
    "                print(\"ID: \", pub_id, \"Publication: \",pub_title,\n",
    "                      \"\\n\\tDOI: \", pub_doi, \" URL: \", pub_url)\n",
    "                if \"pdf\" in pub_url:\n",
    "                    print (\"\\tTry to get the pdf from URL: \", pub_url)\n",
    "                    try:\n",
    "                        response = requests.get(pub_url)\n",
    "                        content_type = response.headers['content-type']\n",
    "                        if not 'text' in content_type:\n",
    "                            #print(response.headers)\n",
    "                            cd= response.headers['content-disposition']\n",
    "                            #print(cd)\n",
    "                            fname = re.findall(\"filename=(.+)\", cd)[0]\n",
    "                            #print(fname)\n",
    "                            if not Path('pdf_files/' + pdf_file).is_file():\n",
    "                                with open('pdf_files/'+ fname +'.pdf', 'wb') as f:\n",
    "                                    f.write(response.content)\n",
    "                            else:\n",
    "                                set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            not_in_url = False\n",
    "                    except:\n",
    "                        print(\"ID: \", pub_id, \"\\nPublication: \",pub_title, \n",
    "                               \"\\nDOI: \", pub_doi, \"\\nDOI: \", pub_url) \n",
    "                got_pdf = False\n",
    "                if not_in_url:\n",
    "                    print(\"\\tTry to see if json file has link to pdf: \")\n",
    "                    if valid_doi(pub_doi):\n",
    "                        crjd, doi_file = pr_fns.get_cr_json_object(pub_doi)\n",
    "                        \n",
    "                        if \"link\" in crjd.keys():\n",
    "                            for a_link in crjd[\"link\"]:\n",
    "                                if \"\\tURL\" in a_link.keys() and (\"pdf\" in a_link[\"URL\"] or \"pdf\" in a_link[\"content-type\"]):\n",
    "                                    cr_url = a_link[\"URL\"]\n",
    "                                    #print(\"URL: \", cr_url)\n",
    "                                    pdf_file = get_pdf_from_url(cr_url)\n",
    "                                    # if the name corresponds to a existing file, assign value to db_record\n",
    "                                    if Path('pdf_files/' + pdf_file).is_file():\n",
    "                                        print(\"\\tFile name:\", pdf_file)\n",
    "                                        set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                                        got_pdf = True\n",
    "                                    else:\n",
    "                                        print(\"\\tcould not get file from\", cr_url)\n",
    "                        else: \n",
    "                            print(\"\\tno links in json\", pub_doi)\n",
    "                    if not got_pdf and \"elsevier\" in pub_url:\n",
    "                        print(\"\\tTrying elsevier doi:\" )\n",
    "                        pdf_file = pr_fns.get_elsevier_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    elif not got_pdf and \"wiley\" in pub_url:\n",
    "                        print(\"\\tTrying wiley doi:\" )\n",
    "                        pdf_file = pr_fns.get_wiley_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    elif not got_pdf and \"pubs.acs\" in pub_url:\n",
    "                        print(\"\\tTrying acs doi:\" )\n",
    "                        pdf_file = pr_fns.get_acs_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    if not got_pdf:\n",
    "                        print(\"\\tTry doi:  https://doi.org/\" + pub_doi)\n",
    "    return return_val\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pdfminer to get metadata from pdf file\n",
    "\n",
    "Functions which use pdf miner to get data from pdf_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer import high_level as pdfmnr_hl\n",
    "\n",
    "# functions for PDFminer\n",
    "\n",
    "def get_pdf_text(pdf_file):\n",
    "    return pdfmnr_hl.extract_text(pdf_file)\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_ref_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if pr_fns.is_data_stmt(sentence.lower()):\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1, idx, idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += \" \" + sentences[single_sentence].strip()\n",
    "        return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_all_data_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if 'data' in sentence.lower() or 'inform' in sentence.lower():\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1, idx, idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        if not full_sentence in return_group:\n",
    "            return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the http strings from references to data\n",
    "def get_http_ref(sentence):\n",
    "    http_frag = \"\"\n",
    "    if 'http' in sentence.lower():\n",
    "        idx_http = sentence.lower().index('http')\n",
    "        http_frag = sentence[idx_http:]\n",
    "        space_in_ref = True\n",
    "        while \" \" in http_frag:\n",
    "            space_idx = http_frag.rfind(\" \")\n",
    "            http_frag = http_frag[:space_idx]\n",
    "        if(http_frag[-1:]==\".\"):\n",
    "            http_frag = http_frag[:-1]\n",
    "    return http_frag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data mentions from pdf files\n",
    "Write the results to a csv file to be checked to verify data mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_refs(data_db, last_processed, work_dir):\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    out_name =  'pdf_mentions' + data_db\n",
    "    out_file = Path(work_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked for data refences in:\", data_db)\n",
    "        return out_name\n",
    "\n",
    "    # get publication data from the ukch app\n",
    "    db_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "    # get the list of dois already mined for data \n",
    "    input_file = './data_load/pub_data_add202012.csv'\n",
    "    id_field = 'num'\n",
    "    processed, headings = csvh.get_csv_data(input_file, id_field)\n",
    "    for id_num in processed:\n",
    "        current_title = processed[id_num]['doi']\n",
    "    processed[1]['num']\n",
    "\n",
    "    processed_dois = []\n",
    "    for entry in processed:\n",
    "        if not processed[entry]['doi'] in processed_dois:\n",
    "            processed_dois.append( processed[entry]['doi'])\n",
    "\n",
    "    data_records = {}\n",
    "    data_mentions = {}\n",
    "    ref_count = mention_count = 0\n",
    "    for a_pub in notebook.tqdm(db_pubs):\n",
    "        data_refs = []\n",
    "        data_sents = []\n",
    "        if a_pub[0] > last_processed:\n",
    "            pub_id = a_pub[0]\n",
    "            pub_title = a_pub[1]\n",
    "            pub_doi = a_pub[2]\n",
    "            pub_url = a_pub[3]\n",
    "            pub_pdf = a_pub[4]\n",
    "            if pub_pdf == 'None':\n",
    "                print(\"*************************\")\n",
    "                print(\"Missing PDF for:\", pub_doi)\n",
    "                print(\"*************************\")\n",
    "            else:\n",
    "                pdf_file = \"pdf_files/\" + pub_pdf\n",
    "                if not Path(pdf_file).is_file():\n",
    "                    print(\"*************************\")\n",
    "                    print(\"Missing file for:\", pdf_file, \"for\", pub_doi)\n",
    "                    print(\"*************************\")\n",
    "                else: \n",
    "                    print(\"PDF filename\", pdf_file)\n",
    "                    pdf_text = get_pdf_text(pdf_file)\n",
    "                    ref_sentences = get_ref_sentences(pdf_text)\n",
    "                    data_sentences = get_all_data_sentences(pdf_text)\n",
    "                    for r_sentence in ref_sentences:\n",
    "                        dt_link = get_http_ref(r_sentence)\n",
    "                        if 'supplem' in r_sentence.lower():\n",
    "                            data_refs.append({'type':'supplementary',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                        else:\n",
    "                            data_refs.append({'type':'supporting',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                    for d_sentence in data_sentences:\n",
    "                        dt_link = get_http_ref(d_sentence)\n",
    "                        if 'supplem' in d_sentence.lower():\n",
    "                            data_sents.append({'type':'supplementary',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "                        else:\n",
    "                            data_sents.append({'type':'supporting',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "            if data_refs != []:\n",
    "                for data_ref in data_refs:\n",
    "                    data_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                    data_record.update(data_ref)\n",
    "                    data_records[ref_count] = data_record\n",
    "                    ref_count += 1\n",
    "            if data_sents != []:\n",
    "                for data_sent in data_sents:\n",
    "                    sentence_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                    sentence_record.update(data_sent)\n",
    "                    data_mentions[mention_count] = sentence_record\n",
    "                    mention_count += 1\n",
    "                    \n",
    "    # csvh.write_csv_data(data_records, 'pdf_data.csv')\n",
    "    if len(data_mentions) > 0:\n",
    "        csvh.write_csv_data(data_mentions, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark for review\n",
    "\n",
    "Verify if the mentions of data or information actually can be linked to data objects.\n",
    "\n",
    "Results need to be reviewed interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_interactivex(data_refs, work_dir):\n",
    "    in_name = data_refs + \"_int\"\n",
    "    out_name = data_refs + \"_rev\"\n",
    "    out_file = Path(work_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked data refences see:\", out_file)\n",
    "        return out_name\n",
    "        \n",
    "    print('Input File: ', in_name)\n",
    "    # Open results file\n",
    "    data_mentions, dm_headers = csvh.get_csv_data(Path(work_dir, in_name+'.csv'))\n",
    "    print(dm_headers)\n",
    "    art_id = ''\n",
    "    terminate = False\n",
    "    additional_rows = {}\n",
    "    for dm in data_mentions:\n",
    "        if data_mentions[dm]['action']=='review':\n",
    "            clear_output()\n",
    "            print (\"*******************************************\")\n",
    "            print (\"Article id  :\", data_mentions[dm]['id'])\n",
    "            print (\"DOI         :\", data_mentions[dm]['doi'])\n",
    "            print (\"Type        :\", data_mentions[dm]['type'], '\\tLine:', dm)\n",
    "            print (\"Description :\\n\\t\", data_mentions[dm]['desc'])\n",
    "            print (\"data_url :\", data_mentions[dm]['data_url'])\n",
    "            print (\"*******************************************\")\n",
    "            decide_action = False\n",
    "            while not decide_action:\n",
    "                print('Action:')\n",
    "                print('\\tr) review: https://doi.org/'+data_mentions[dm]['doi'])\n",
    "                print('\\ta) add new row')\n",
    "                print('\\tn) next')\n",
    "                print('\\tt) terminate')\n",
    "                print('\\tSelect r, a, n, t:')\n",
    "                lts = input()\n",
    "                if lts == \"r\":\n",
    "                    data_mentions[dm]['action'] = 'reviewed'\n",
    "                    print ('https://doi.org/'+data_mentions[dm]['doi'])\n",
    "                    print ('link:',data_mentions[dm]['link'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['link'] = add_this\n",
    "                    print ('issue:',data_mentions[dm]['issue'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['issue'] = add_this\n",
    "                    print ('name:',data_mentions[dm]['name'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['name'] = add_this\n",
    "                    print ('file:',data_mentions[dm]['file'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['file'] = add_this\n",
    "                if lts == \"a\":\n",
    "                    #add a new row\n",
    "                    new_idx = len(data_mentions) + len(additional_rows) + 1\n",
    "                    additional_rows[new_idx] = {}\n",
    "                    additional_rows[new_idx]['id'] = data_mentions[dm]['id']\n",
    "                    additional_rows[new_idx]['doi'] = data_mentions[dm]['doi']\n",
    "                    additional_rows[new_idx]['type'] = data_mentions[dm]['type']\n",
    "                    additional_rows[new_idx]['desc'] = data_mentions[dm]['desc']\n",
    "                    additional_rows[new_idx]['action'] = 'reviewed'\n",
    "                    print ('link:')\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['link'] = add_this\n",
    "                    print ('issue:')\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['issue'] = add_this\n",
    "                    print ('name:')\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['name'] = add_this\n",
    "                    print ('file:')\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['file'] = add_this\n",
    "                elif lts == \"n\":\n",
    "                    if data_mentions[dm]['action'] != 'reviewed':\n",
    "                        data_mentions[dm]['action'] = 'none'\n",
    "                    decide_action = True\n",
    "                elif lts == 't':\n",
    "                    decide_action = True\n",
    "                    terminate = True\n",
    "        art__id = data_mentions[dm]['id']\n",
    "        if dm > 1700 or terminate:\n",
    "            break\n",
    "    if len(additional_rows)> 0 :\n",
    "        for nr in additional_rows:\n",
    "           data_mentions[nr] = additional_rows[nr]\n",
    "    if len(data_mentions) > 0:\n",
    "       csvh.write_csv_data(data_mentions, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# clear the output after each loop cycle\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def review_interactive(data_refs, work_dir):\n",
    "    out_name = data_refs + \"_int\"\n",
    "    out_file = Path(work_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked data refences see:\", out_file)\n",
    "        return out_name\n",
    "\n",
    "    # Open results file\n",
    "    data_mentions, dm_headers = csvh.get_csv_data(Path(work_dir,data_refs+ '.csv'))\n",
    "    print(dm_headers)\n",
    "    art_id = ''\n",
    "    for dm in data_mentions:\n",
    "        if not 'acion' in data_mentions[dm].keys() or data_mentions[dm]['action']=='':\n",
    "            clear_output()\n",
    "            print (\"*******************************************\")\n",
    "            print (\"Article id  :\", data_mentions[dm]['id'])\n",
    "            print (\"DOI         :\", data_mentions[dm]['doi'])\n",
    "            print (\"Type        :\", data_mentions[dm]['type'], '\\tLine:', dm)\n",
    "            print (\"Description :\\n\\t\", data_mentions[dm]['desc'])\n",
    "            print (\"data_url :\", data_mentions[dm]['data_url'])\n",
    "            print (\"*******************************************\")\n",
    "            decide_action = False\n",
    "            while not decide_action:\n",
    "                print('Action:')\n",
    "                print('\\ta) review')\n",
    "                print('\\tb) none')\n",
    "                print('\\tSelect a or b:')\n",
    "                lts = input()\n",
    "                if lts == \"a\":\n",
    "                    data_mentions[dm]['action'] = 'review'\n",
    "                    decide_action = True\n",
    "                elif lts == \"b\":\n",
    "                    data_mentions[dm]['action'] = 'none'\n",
    "                    decide_action = True\n",
    "        art__id = data_mentions[dm]['id']\n",
    "        if dm > 1700:\n",
    "            break\n",
    "    if len(data_mentions) > 0:\n",
    "       csvh.write_csv_data(data_mentions, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review references interactively\n",
    "\n",
    "Check each marked reference to determine if they should be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run next to get the ones which need to be reviewed online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def revise_online(revised_refs, db_name, work_dir):\n",
    "    print (revised_refs, db_name, work_dir)\n",
    "    out_name = 'html_'+db_name\n",
    "    out_file = Path(work_dir,out_name+'.csv')\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked refences online:\", out_file)\n",
    "        return out_name\n",
    "    in_file = Path(Path(work_dir),revised_refs+'.csv')\n",
    "    data_mentions, dm_headers = csvh.get_csv_data(in_file)\n",
    "    filter_mentions = {}\n",
    "    for dm in data_mentions:\n",
    "        if 'add' in data_mentions[dm].keys() and data_mentions[dm]['add'] == '1':\n",
    "            filter_mentions[dm]={}\n",
    "            for a_field in dm_headers:\n",
    "                filter_mentions[dm][a_field] = data_mentions[dm][a_field]\n",
    "    print('filtered mentions:', len(filter_mentions))\n",
    "\n",
    "    new_do_id_list =[]\n",
    "    for fm in filter_mentions:\n",
    "        art_id = int(filter_mentions[fm][\"id\"])\n",
    "        if not art_id in new_do_id_list:\n",
    "            new_do_id_list.append(art_id)\n",
    "\n",
    "    # currend app DB\n",
    "    ukchapp_db = \"db_files/\"+db_name+\".sqlite3\"\n",
    "\n",
    "    no_data_pubs = pr_fns.get_pub_app_no_data(ukchapp_db)\n",
    "\n",
    "    print(len(no_data_pubs))\n",
    "    print(new_do_id_list, len(new_do_id_list))\n",
    "    filter_mentions\n",
    "\n",
    "\n",
    "    int_idx = 0\n",
    "    revised_list = {}\n",
    "    if Path(\"./html_revised202111.csv\").is_file():\n",
    "        revised_list, rl_headers = csvh.get_csv_data('html_revised202111.csv')\n",
    "        int_idx = len(revised_list)\n",
    "\n",
    "    already_revised =[]\n",
    "    for fm in revised_list:\n",
    "        art_id = int(revised_list[fm][\"id\"])\n",
    "        if not art_id in already_revised:\n",
    "            already_revised.append(art_id)\n",
    "\n",
    "    for ndp in no_data_pubs:\n",
    "        if not ndp[0] in new_do_id_list and ndp[0] > 786 and not ndp[0] in already_revised:\n",
    "            int_idx += 1\n",
    "            pub_id = ndp[0]\n",
    "            pub_title = ndp[1]\n",
    "            pub_doi = ndp[2]\n",
    "            pub_url = ndp[3]\n",
    "            data_record = {'id':pub_id, 'doi':pub_doi, 'title':pub_title} \n",
    "            print ('id',pub_id, '\\n', pub_title)\n",
    "            decide_action = False\n",
    "            terminate = False\n",
    "            while not decide_action:\n",
    "                print('Action:')\n",
    "                print(pub_url)\n",
    "                print(\"https://doi.org/\"+pub_doi)\n",
    "                print('\\ts) skip (no data)' )\n",
    "                print('\\tr) review')\n",
    "                print('\\tn) next')\n",
    "                print('\\tt) terminate')\n",
    "                print('\\tSelect s, r, n, t:')\n",
    "                lts = input()\n",
    "                if lts == \"s\":\n",
    "                    data_record['action'] = 'no data'\n",
    "                    data_record['issue'] = \"no data availability or supplementary data mentioned in html or pdf versions or article\"\n",
    "                    revised_list[int_idx] = data_record\n",
    "                    decide_action = True\n",
    "                if lts == \"r\":\n",
    "                    data_record['action'] = 'review'\n",
    "                    if 'issue' in data_mentions[dm].keys():\n",
    "                        print ('issue:',data_mentions[dm]['issue'])\n",
    "                    add_this = input()\n",
    "                    data_record['issue'] = add_this\n",
    "                    revised_list[int_idx] = data_record\n",
    "                    decide_action = True\n",
    "                if lts == \"n\":\n",
    "                    decide_action = True\n",
    "                elif lts == 't':\n",
    "                    decide_action = True\n",
    "                    terminate = True\n",
    "            if terminate:\n",
    "                break\n",
    "\n",
    "    if len(revised_list) > 0:\n",
    "        csvh.write_csv_data(revised_list, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_file Alredy exists in  articles\n",
      "1. PDF file names copied to production\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1629d282aac74015aba135a05205a955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/645 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "1 Missing file for: pdf_files/Not available for 10.1002/9783527804085.ch10 64\n",
      "*************************\n",
      "2 Missing file for: pdf_files/b978-0-12-805324-9.09989-1 for 10.1016/b978-0-12-805324-9.09989-1 599\n",
      "*************************\n",
      "3 Missing file for: pdf_files/Not available for 10.1142/q0035 603\n",
      "*************************\n",
      "4 Missing file for: pdf_files/NA for 10.1142/q0354 925\n",
      "*************************\n",
      "5 Missing file for: pdf_files/s41467-023-40284-z.pdf for 10.1038/s41467-023-40284-z 940\n",
      "2. PDF file names copied to production\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2dffd650204cd1a11286db30b5936f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in DB: ChemSusChem-2016-Gill-DesignHighlySelectivePlatinumNanoparticle CatalystsAerobic OxidationKA‐Oil.pdf\n",
      "Not in DB: multiscale-simulations-identify-origins-of-differential-carbapenem-hydrolysis-by-the-oxa-48-b-lactamase.pdf\n"
     ]
    }
   ],
   "source": [
    "# (0) Add column for pdf_file names\n",
    "db_name = 'production'\n",
    "add_pdf_file_column(db_name, \"articles\", \"pdf_file\", \"varchar\")\n",
    "\n",
    "#(1) previously verified files:\n",
    "last_processed = 930\n",
    "prev_db_name = \"production202307\"\n",
    "\n",
    "db_name, names_added = add_pdf_file_names(prev_db_name, db_name)\n",
    "# working dir\n",
    "pdf_data_search_dir = \"./data_search_pdf\"\n",
    "\n",
    "if (names_added):\n",
    "    print (\"1. PDF file names copied to\", db_name);\n",
    "pdfs_ok = pdf_data_exists(db_name)\n",
    "if pdfs_ok:\n",
    "    print (\"2. PDF file names copied to\", db_name);\n",
    "    not_indexed = check_files_in_db(db_name)\n",
    "    if not_indexed:\n",
    "        print (\"3. All PDFs are indexed in\", db_name )\n",
    "else:\n",
    "    pdfs_ok = get_missing_pdfs(db_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************\n",
      "Article id  : 856\n",
      "DOI         : 10.1002/aenm.202201131\n",
      "Type        : supporting \tLine: 41\n",
      "Description :\n",
      "\t site and the nanoparticles have hemispherical shape as particleanalysis  data  obtained  from  SEM  image  analysis  indicates  inFigure 2a. For all three catalysts the corresponding nTOF values\n",
      "data_url : \n",
      "*******************************************\n",
      "Action:\n",
      "\ta) review\n",
      "\tb) none\n",
      "\tSelect a or b:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m pdf_mentions \u001b[38;5;241m=\u001b[39m get_data_refs(db_name, last_processed, pdf_data_search_dir)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4. PDFs data references stored in\u001b[39m\u001b[38;5;124m\"\u001b[39m, pdf_mentions)\n\u001b[1;32m----> 5\u001b[0m review_marked \u001b[38;5;241m=\u001b[39m \u001b[43mreview_interactive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_mentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf_data_search_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5. PDFs data references marked for review\u001b[39m\u001b[38;5;124m\"\u001b[39m, review_marked)\n\u001b[0;32m      7\u001b[0m revised_refs \u001b[38;5;241m=\u001b[39m review_interactive(pdf_mentions, pdf_data_search_dir)\n",
      "Cell \u001b[1;32mIn[10], line 31\u001b[0m, in \u001b[0;36mreview_interactive\u001b[1;34m(data_refs, work_dir)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mb) none\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mSelect a or b:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m lts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lts \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     33\u001b[0m     data_mentions[dm][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py:1182\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1180\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py:1225\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1223\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1225\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#review data references\n",
    "if pdfs_ok:\n",
    "    pdf_mentions = get_data_refs(db_name, last_processed, pdf_data_search_dir)\n",
    "    print (\"4. PDFs data references stored in\", pdf_mentions)\n",
    "    review_marked = review_interactive(pdf_mentions, pdf_data_search_dir)\n",
    "    print (\"5. PDFs data references marked for review\", review_marked)\n",
    "    revised_refs = review_interactive(pdf_mentions, pdf_data_search_dir)\n",
    "    print (\"6. PDFs data references reviewed\", revised_refs)\n",
    "    html_refs = revise_online(pdf_mentions, db_name, pdf_data_search_dir)\n",
    "    print (\"7. Online references reviewed\", html_refs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pdf_mentionsproduction'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_ok = get_missing_pdfs(db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(revised_list) > 0:\n",
    "    csvh.writre_csv_data(revised_list, 'html_revised202301.csv')\n",
    "revised_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"db_files/app_db2.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "ukchapp_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names and links for references in data mentions\n",
    "data_mentions, dm_fields = csvh.get_csv_data('pdf_mentions_filtered_02.csv', 'num')\n",
    "\n",
    "for dm in data_mentions:\n",
    "    print(\"https://doi.org/\" + data_mentions[dm]['doi'])\n",
    "    \n",
    "    ref_name = data_mentions[dm]['ref_name']\n",
    "    while ref_name == \"\":\n",
    "        print('Please enter the name of data object:')\n",
    "        ref_name = input()\n",
    "    ref_link = data_mentions[dm]['ref_link']\n",
    "    while ref_link == \"\":\n",
    "        print('Please enter the data object link:')\n",
    "        ref_link = input()\n",
    "    data_mentions[dm]['ref_name'] = ref_name\n",
    "    data_mentions[dm]['ref_link'] = ref_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getmembers, isfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pdfminer.high_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
