{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data for new articles and list of all articles not citing data\n",
    "A list of publications is obtainded from the app database. This list will contain a titles, IDs and DOIs which need to be explored to look for the corresponding pdf files. \n",
    "The steps of the process are: \n",
    " 1. get a Title, DOI, and URL for each publication\n",
    " 2. convert the DOI to a pdf file name and try to open de file\n",
    " 3. use pdfMiner and/or CDE to get the reference to data\n",
    " 4. add a new dataset entry each time a new data object is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "from tqdm import tqdm\n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n",
    "# library for handling http requests\n",
    "import requests\n",
    "# import custom functions (common to various notebooks)\n",
    "import processing_functions as pr_fns\n",
    "\n",
    "current_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pdf_column_populated(data_db):\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    \n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "    i_indx = 0\n",
    "    for a_pub in app_pubs:\n",
    "        if a_pub[4] != None:\n",
    "            i_indx += 1\n",
    "    #print (i_indx/len(app_pubs) > 0.9, i_indx/len(app_pubs))\n",
    "    return (i_indx/len(app_pubs) > 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pdf and HTML names into app DB\n",
    "\n",
    "0. Add fields to articles table for holding pdf file names\n",
    "1. Open the previously verified DB and get the publications list\n",
    "2. Open the current publication list from the appdb\n",
    "3. Get pdf and html file names from previous and put it in current\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0) Add fields\n",
    "# ALTER TABLE articles \n",
    "# ADD COLUMN pdf_file varchar;\n",
    "\n",
    "#(1) previously verified files:\n",
    "\n",
    "def add_pdf_file_names(prev_db, curr_db):\n",
    "    has_file_names = False\n",
    "    \n",
    "    prevapp_db = \"db_files/\"+prev_db +\".sqlite3\"\n",
    "\n",
    "    while not Path(prevapp_db).is_file():\n",
    "        print('Please enter the name of app db file:')\n",
    "        prevapp_db = input()\n",
    "\n",
    "    # get publication data from the db\n",
    "    prev_pubs = pr_fns.get_pub_data(prevapp_db)\n",
    "\n",
    "    #2 currend app DB\n",
    "    ukchapp_db = \"db_files/\" + curr_db + \".sqlite3\"\n",
    "    while not Path(ukchapp_db).is_file():\n",
    "        print('Please enter the name of app db file:')\n",
    "        curr_db = input()\n",
    "        ukchapp_db = \"db_files/\" + curr_db + \".sqlite3\"\n",
    "    \n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "    \n",
    "    # check if file names have been added\n",
    "    # return true if OK\n",
    "    if pdf_column_populated(curr_db):\n",
    "        has_file_names = True\n",
    "        return curr_db, has_file_names\n",
    "\n",
    "    # 3 get pdf file name from previous and put it in current\n",
    "    for a_pub in tqdm_notebook(prev_pubs):\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        match_found = False\n",
    "        for curr_pub in app_pubs:\n",
    "            if curr_pub[2] == pub_doi and pub_doi != None:\n",
    "                pr_fns.set_pdf_file_value(pub_pdf, curr_pub[0], ukchapp_db)\n",
    "                match_found = True\n",
    "                break\n",
    "            elif curr_pub[1] == pub_title:\n",
    "                pr_fns.set_pdf_file_value(pub_pdf, curr_pub[0], ukchapp_db)\n",
    "                match_found = True\n",
    "                break\n",
    "        if not match_found:\n",
    "            print(\"*************\\n\",a_pub)\n",
    "\n",
    "        has_file_names = True\n",
    "    return curr_db, has_file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that idexed pdf files exist \n",
    "\n",
    "Use the data on the articles table to verify if file are stored in the corresponding folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_data_exists(data_db):\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    \n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "    i_indx = 0\n",
    "    for a_pub in tqdm_notebook(app_pubs):\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        if pub_pdf == None:\n",
    "            print(\"*************************\")\n",
    "            i_indx +=1\n",
    "            print(i_indx, \"Missing PDF for:\", pub_doi, pub_id)\n",
    "            \n",
    "        else:\n",
    "            pdf_file = \"pdf_files/\" + pub_pdf\n",
    "            if not Path(pdf_file).is_file():\n",
    "                print(\"*************************\")\n",
    "                i_indx +=1\n",
    "                print(i_indx, \"Missing file for:\", pdf_file, \"for\", pub_doi, pub_id)\n",
    "                \n",
    "    #print(i_indx/len(app_pubs) )\n",
    "    # If less than 1% if missing that is OK\n",
    "    return (i_indx/len(app_pubs) < 0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that all PDF files are indexed \n",
    "\n",
    "Check that the files in the folder are all accounted for (have a corersponding record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_files_in_db(data_db):\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    \n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "    files_not_in_DB = 0\n",
    "    for infile in tqdm_notebook(Path(\"pdf_files\").glob('*.pdf')):\n",
    "        file_found = False\n",
    "        for a_pub in app_pubs:\n",
    "            if infile.name == a_pub[4]:\n",
    "                file_found = True\n",
    "                break\n",
    "        if not file_found:\n",
    "            print(\"Not in DB:\", infile.name)\n",
    "            files_not_in_DB += 1\n",
    "    return files_not_in_DB < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get missing pdfs\n",
    "If there are more than 1% missing try to get them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use regular expression to check if a given string\n",
    "# is a valid DOI, using pattern from CR\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    if cr_doi == None:\n",
    "        return False\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_missing_pdfs(data_db):\n",
    "    return_val = False\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    # get publication data from the ukch app\n",
    "    db_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "    for a_pub in tqdm_notebook(db_pubs):\n",
    "        if a_pub[0] > 0:\n",
    "            pub_id = a_pub[0]\n",
    "            pub_title = a_pub[1]\n",
    "            pub_doi = a_pub[2]\n",
    "            pub_url = a_pub[3]\n",
    "            pub_pdf = a_pub[4]\n",
    "            if pub_pdf == None:\n",
    "                not_in_url = True\n",
    "                print(\"ID: \", pub_id, \"Publication: \",pub_title,\n",
    "                      \"\\n\\tDOI: \", pub_doi, \" URL: \", pub_url)\n",
    "                if \"pdf\" in pub_url:\n",
    "                    print (\"\\tTry to get the pdf from URL: \", pub_url)\n",
    "                    try:\n",
    "                        response = requests.get(pub_url)\n",
    "                        content_type = response.headers['content-type']\n",
    "                        if not 'text' in content_type:\n",
    "                            #print(response.headers)\n",
    "                            cd= response.headers['content-disposition']\n",
    "                            #print(cd)\n",
    "                            fname = re.findall(\"filename=(.+)\", cd)[0]\n",
    "                            #print(fname)\n",
    "                            if not Path('pdf_files/' + pdf_file).is_file():\n",
    "                                with open('pdf_files/'+ fname +'.pdf', 'wb') as f:\n",
    "                                    f.write(response.content)\n",
    "                            else:\n",
    "                                set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            not_in_url = False\n",
    "                    except:\n",
    "                        print(\"ID: \", pub_id, \"\\nPublication: \",pub_title, \n",
    "                               \"\\nDOI: \", pub_doi, \"\\nDOI: \", pub_url) \n",
    "                if not_in_url:\n",
    "                    print(\"\\tTry to see if json file has link to pdf: \")\n",
    "                    if valid_doi(pub_doi):\n",
    "                        crjd, doi_file = pr_fns.get_cr_json_object(pub_doi)\n",
    "                        got_pdf = False\n",
    "                        if \"link\" in crjd.keys():\n",
    "                            for a_link in crjd[\"link\"]:\n",
    "                                if \"\\tURL\" in a_link.keys() and (\"pdf\" in a_link[\"URL\"] or \"pdf\" in a_link[\"content-type\"]):\n",
    "                                    cr_url = a_link[\"URL\"]\n",
    "                                    #print(\"URL: \", cr_url)\n",
    "                                    pdf_file = get_pdf_from_url(cr_url)\n",
    "                                    # if the name corresponds to a existing file, assign value to db_record\n",
    "                                    if Path('pdf_files/' + pdf_file).is_file():\n",
    "                                        print(\"\\tFile name:\", pdf_file)\n",
    "                                        set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                                        got_pdf = True\n",
    "                                    else:\n",
    "                                        print(\"\\tcould not get file from\", cr_url)\n",
    "                        else: \n",
    "                            print(\"\\tno links in json\", pub_doi)\n",
    "                    if not got_pdf and \"elsevier\" in pub_url:\n",
    "                        print(\"\\tTrying elsevier doi:\" )\n",
    "                        pdf_file = pr_fns.get_elsevier_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    elif not got_pdf and \"wiley\" in pub_url:\n",
    "                        print(\"\\tTrying wiley doi:\" )\n",
    "                        pdf_file = pr_fns.get_wiley_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    elif not got_pdf and \"pubs.acs\" in pub_url:\n",
    "                        print(\"\\tTrying acs doi:\" )\n",
    "                        pdf_file = pr_fns.get_acs_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    if not got_pdf:\n",
    "                        print(\"\\tTry doi:  https://doi.org/\" + pub_doi)\n",
    "    return return_val\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pdfminer to get metadata from pdf file\n",
    "\n",
    "Functions which use pdf miner to get data from pdf_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer import high_level as pdfmnr_hl\n",
    "\n",
    "# functions for PDFminer\n",
    "\n",
    "def get_pdf_text(pdf_file):\n",
    "    return pdfmnr_hl.extract_text(pdf_file)\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_ref_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if pr_fns.is_data_stmt(sentence.lower()):\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1,idx,idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_all_data_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if 'data' in sentence.lower() or 'inform' in sentence.lower():\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1, idx, idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        if not full_sentence in return_group:\n",
    "            return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the http strings from references to data\n",
    "def get_http_ref(sentence):\n",
    "    http_frag = \"\"\n",
    "    if 'http' in sentence.lower():\n",
    "        idx_http = sentence.lower().index('http')\n",
    "        http_frag = sentence[idx_http:]\n",
    "        space_in_ref = True\n",
    "        while \" \" in http_frag:\n",
    "            space_idx = http_frag.rfind(\" \")\n",
    "            http_frag = http_frag[:space_idx]\n",
    "        if(http_frag[-1:]==\".\"):\n",
    "            http_frag = http_frag[:-1]\n",
    "    return http_frag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data mentions from pdf files\n",
    "Write the results to a csv file to be checked to verify data mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_refs(data_db, last_processed, work_dir):\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    out_name =  'pdf_mentions' + db_name\n",
    "    out_file = Path(work_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked for data refences in:\", data_db)\n",
    "        return out_name\n",
    "\n",
    "    # get publication data from the ukch app\n",
    "    db_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "    # get the list of dois already mined for data \n",
    "    input_file = './data_load/pub_data_add202012.csv'\n",
    "    id_field = 'num'\n",
    "    processed, headings = csvh.get_csv_data(input_file, id_field)\n",
    "    for id_num in processed:\n",
    "        current_title = processed[id_num]['doi']\n",
    "    processed[1]['num']\n",
    "\n",
    "    processed_dois = []\n",
    "    for entry in processed:\n",
    "        if not processed[entry]['doi'] in processed_dois:\n",
    "            processed_dois.append( processed[entry]['doi'])\n",
    "\n",
    "    data_records = {}\n",
    "    data_mentions = {}\n",
    "    ref_count = mention_count = 0\n",
    "    for a_pub in tqdm_notebook(db_pubs):\n",
    "        data_refs = []\n",
    "        data_sents = []\n",
    "        if a_pub[0] > last_processed:\n",
    "            pub_id = a_pub[0]\n",
    "            pub_title = a_pub[1]\n",
    "            pub_doi = a_pub[2]\n",
    "            pub_url = a_pub[3]\n",
    "            pub_pdf = a_pub[4]\n",
    "            if pub_pdf == 'None':\n",
    "                print(\"*************************\")\n",
    "                print(\"Missing PDF for:\", pub_doi)\n",
    "                print(\"*************************\")\n",
    "            else:\n",
    "                pdf_file = \"pdf_files/\" + pub_pdf\n",
    "                if not Path(pdf_file).is_file():\n",
    "                    print(\"*************************\")\n",
    "                    print(\"Missing file for:\", pdf_file, \"for\", pub_doi)\n",
    "                    print(\"*************************\")\n",
    "                else: \n",
    "                    print(\"PDF filename\", pdf_file)\n",
    "                    pdf_text = get_pdf_text(pdf_file)\n",
    "                    ref_sentences = get_ref_sentences(pdf_text)\n",
    "                    data_sentences = get_all_data_sentences(pdf_text)\n",
    "                    for r_sentence in ref_sentences:\n",
    "                        dt_link = get_http_ref(r_sentence)\n",
    "                        if 'supplem' in r_sentence.lower():\n",
    "                            data_refs.append({'type':'supplementary',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                        else:\n",
    "                            data_refs.append({'type':'supporting',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                    for d_sentence in data_sentences:\n",
    "                        dt_link = get_http_ref(d_sentence)\n",
    "                        if 'supplem' in d_sentence.lower():\n",
    "                            data_sents.append({'type':'supplementary',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "                        else:\n",
    "                            data_sents.append({'type':'supporting',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "            if data_refs != []:\n",
    "                for data_ref in data_refs:\n",
    "                    data_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                    data_record.update(data_ref)\n",
    "                    data_records[ref_count] = data_record\n",
    "                    ref_count += 1\n",
    "            if data_sents != []:\n",
    "                for data_sent in data_sents:\n",
    "                    sentence_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                    sentence_record.update(data_sent)\n",
    "                    data_mentions[mention_count] = sentence_record\n",
    "                    mention_count += 1\n",
    "                    \n",
    "    # csvh.write_csv_data(data_records, 'pdf_data.csv')\n",
    "    if len(data_mentions) > 0:\n",
    "        csvh.write_csv_data(data_mentions, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark for review\n",
    "\n",
    "Verify if the mentions of data or information actually can be linked to data objects.\n",
    "\n",
    "Results need to be reviewed interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_interactivex(data_refs, work_dir):\n",
    "    in_name = data_refs + \"_int\"\n",
    "    out_name = data_refs + \"_rev\"\n",
    "    out_file = Path(work_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked data refences see:\", out_file)\n",
    "        return out_name\n",
    "        \n",
    "    print('Input File: ', in_name)\n",
    "    # Open results file\n",
    "    data_mentions, dm_headers = csvh.get_csv_data(Path(work_dir, in_name+'.csv'))\n",
    "    print(dm_headers)\n",
    "    art_id = ''\n",
    "    terminate = False\n",
    "    additional_rows = {}\n",
    "    for dm in data_mentions:\n",
    "        if data_mentions[dm]['action']=='review':\n",
    "            clear_output()\n",
    "            print (\"*******************************************\")\n",
    "            print (\"Article id  :\", data_mentions[dm]['id'])\n",
    "            print (\"DOI         :\", data_mentions[dm]['doi'])\n",
    "            print (\"Type        :\", data_mentions[dm]['type'], '\\tLine:', dm)\n",
    "            print (\"Description :\\n\\t\", data_mentions[dm]['desc'])\n",
    "            print (\"data_url :\", data_mentions[dm]['data_url'])\n",
    "            print (\"*******************************************\")\n",
    "            decide_action = False\n",
    "            while not decide_action:\n",
    "                print('Action:')\n",
    "                print('\\tr) review: https://doi.org/'+data_mentions[dm]['doi'])\n",
    "                print('\\ta) add new row')\n",
    "                print('\\tn) next')\n",
    "                print('\\tt) terminate')\n",
    "                print('\\tSelect r, a, n, t:')\n",
    "                lts = input()\n",
    "                if lts == \"r\":\n",
    "                    data_mentions[dm]['action'] = 'reviewed'\n",
    "                    print ('https://doi.org/'+data_mentions[dm]['doi'])\n",
    "                    print ('link:',data_mentions[dm]['link'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['link'] = add_this\n",
    "                    print ('issue:',data_mentions[dm]['issue'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['issue'] = add_this\n",
    "                    print ('name:',data_mentions[dm]['name'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['name'] = add_this\n",
    "                    print ('file:',data_mentions[dm]['file'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['file'] = add_this\n",
    "                if lts == \"a\":\n",
    "                    #add a new row\n",
    "                    new_idx = len(data_mentions) + len(additional_rows) + 1\n",
    "                    additional_rows[new_idx] = {}\n",
    "                    additional_rows[new_idx]['id'] = data_mentions[dm]['id']\n",
    "                    additional_rows[new_idx]['doi'] = data_mentions[dm]['doi']\n",
    "                    additional_rows[new_idx]['type'] = data_mentions[dm]['type']\n",
    "                    additional_rows[new_idx]['desc'] = data_mentions[dm]['desc']\n",
    "                    additional_rows[new_idx]['action'] = 'reviewed'\n",
    "                    print ('link:')\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['link'] = add_this\n",
    "                    print ('issue:')\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['issue'] = add_this\n",
    "                    print ('name:')\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['name'] = add_this\n",
    "                    print ('file:')\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['file'] = add_this\n",
    "                elif lts == \"n\":\n",
    "                    if data_mentions[dm]['action'] != 'reviewed':\n",
    "                        data_mentions[dm]['action'] = 'none'\n",
    "                    decide_action = True\n",
    "                elif lts == 't':\n",
    "                    decide_action = True\n",
    "                    terminate = True\n",
    "        art__id = data_mentions[dm]['id']\n",
    "        if dm > 1700 or terminate:\n",
    "            break\n",
    "    if len(additional_rows)> 0 :\n",
    "        for nr in additional_rows:\n",
    "           data_mentions[nr] = additional_rows[nr]\n",
    "    if len(data_mentions) > 0:\n",
    "       csvh.write_csv_data(data_mentions, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# clear the output after each loop cycle\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def review_interactive(data_refs, work_dir):\n",
    "    out_name = data_refs + \"_int\"\n",
    "    out_file = Path(work_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked data refences see:\", out_file)\n",
    "        return out_name\n",
    "\n",
    "    # Open results file\n",
    "    data_mentions, dm_headers = csvh.get_csv_data(Path(work_dir,data_refs+ '.csv'))\n",
    "    print(dm_headers)\n",
    "    art_id = ''\n",
    "    for dm in data_mentions:\n",
    "        if not 'acion' in data_mentions[dm].keys() or data_mentions[dm]['action']=='':\n",
    "            clear_output()\n",
    "            print (\"*******************************************\")\n",
    "            print (\"Article id  :\", data_mentions[dm]['id'])\n",
    "            print (\"DOI         :\", data_mentions[dm]['doi'])\n",
    "            print (\"Type        :\", data_mentions[dm]['type'], '\\tLine:', dm)\n",
    "            print (\"Description :\\n\\t\", data_mentions[dm]['desc'])\n",
    "            print (\"data_url :\", data_mentions[dm]['data_url'])\n",
    "            print (\"*******************************************\")\n",
    "            decide_action = False\n",
    "            while not decide_action:\n",
    "                print('Action:')\n",
    "                print('\\ta) review')\n",
    "                print('\\tb) none')\n",
    "                print('\\tSelect a or b:')\n",
    "                lts = input()\n",
    "                if lts == \"a\":\n",
    "                    data_mentions[dm]['action'] = 'review'\n",
    "                    decide_action = True\n",
    "                elif lts == \"b\":\n",
    "                    data_mentions[dm]['action'] = 'none'\n",
    "                    decide_action = True\n",
    "        art__id = data_mentions[dm]['id']\n",
    "        if dm > 1700:\n",
    "            break\n",
    "    if len(data_mentions) > 0:\n",
    "       csvh.write_csv_data(data_mentions, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review references interactively\n",
    "\n",
    "Check each marked reference to determine if they should be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run next to get the ones which need to be reviewed online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def revise_online(revised_refs, db_name, work_dir):\n",
    "    print (revised_refs, db_name, work_dir)\n",
    "    out_name = 'html_'+db_name\n",
    "    out_file = Path(work_dir,out_name+'.csv')\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked refences online:\", out_file)\n",
    "        return out_name\n",
    "    in_file = Path(Path(work_dir),revised_refs+'.csv')\n",
    "    data_mentions, dm_headers = csvh.get_csv_data(in_file)\n",
    "    filter_mentions = {}\n",
    "    for dm in data_mentions:\n",
    "        if 'add' in data_mentions[dm].keys() and data_mentions[dm]['add'] == '1':\n",
    "            filter_mentions[dm]={}\n",
    "            for a_field in dm_headers:\n",
    "                filter_mentions[dm][a_field] = data_mentions[dm][a_field]\n",
    "    print('filtered mentions:', len(filter_mentions))\n",
    "\n",
    "    new_do_id_list =[]\n",
    "    for fm in filter_mentions:\n",
    "        art_id = int(filter_mentions[fm][\"id\"])\n",
    "        if not art_id in new_do_id_list:\n",
    "            new_do_id_list.append(art_id)\n",
    "\n",
    "    # currend app DB\n",
    "    ukchapp_db = \"db_files/\"+db_name+\".sqlite3\"\n",
    "\n",
    "    no_data_pubs = pr_fns.get_pub_app_no_data(ukchapp_db)\n",
    "\n",
    "    print(len(no_data_pubs))\n",
    "    print(new_do_id_list, len(new_do_id_list))\n",
    "    filter_mentions\n",
    "\n",
    "\n",
    "    int_idx = 0\n",
    "    revised_list = {}\n",
    "    if Path(\"./html_revised202111.csv\").is_file():\n",
    "        revised_list, rl_headers = csvh.get_csv_data('html_revised202111.csv')\n",
    "        int_idx = len(revised_list)\n",
    "\n",
    "    already_revised =[]\n",
    "    for fm in revised_list:\n",
    "        art_id = int(revised_list[fm][\"id\"])\n",
    "        if not art_id in already_revised:\n",
    "            already_revised.append(art_id)\n",
    "\n",
    "    for ndp in no_data_pubs:\n",
    "        if not ndp[0] in new_do_id_list and ndp[0] > 786 and not ndp[0] in already_revised:\n",
    "            int_idx += 1\n",
    "            pub_id = ndp[0]\n",
    "            pub_title = ndp[1]\n",
    "            pub_doi = ndp[2]\n",
    "            pub_url = ndp[3]\n",
    "            data_record = {'id':pub_id, 'doi':pub_doi, 'title':pub_title} \n",
    "            print ('id',pub_id, '\\n', pub_title)\n",
    "            decide_action = False\n",
    "            terminate = False\n",
    "            while not decide_action:\n",
    "                print('Action:')\n",
    "                print(pub_url)\n",
    "                print(\"https://doi.org/\"+pub_doi)\n",
    "                print('\\ts) skip (no data)' )\n",
    "                print('\\tr) review')\n",
    "                print('\\tn) next')\n",
    "                print('\\tt) terminate')\n",
    "                print('\\tSelect s, r, n, t:')\n",
    "                lts = input()\n",
    "                if lts == \"s\":\n",
    "                    data_record['action'] = 'no data'\n",
    "                    data_record['issue'] = \"no data availability or supplementary data mentioned in html or pdf versions or article\"\n",
    "                    revised_list[int_idx] = data_record\n",
    "                    decide_action = True\n",
    "                if lts == \"r\":\n",
    "                    data_record['action'] = 'review'\n",
    "                    if 'issue' in data_mentions[dm].keys():\n",
    "                        print ('issue:',data_mentions[dm]['issue'])\n",
    "                    add_this = input()\n",
    "                    data_record['issue'] = add_this\n",
    "                    revised_list[int_idx] = data_record\n",
    "                    decide_action = True\n",
    "                if lts == \"n\":\n",
    "                    decide_action = True\n",
    "                elif lts == 't':\n",
    "                    decide_action = True\n",
    "                    terminate = True\n",
    "            if terminate:\n",
    "                break\n",
    "\n",
    "    if len(revised_list) > 0:\n",
    "        csvh.write_csv_data(revised_list, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. PDF file names copied to app_db20220527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scman1\\AppData\\Local\\Temp\\ipykernel_284\\547795858.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for a_pub in tqdm_notebook(app_pubs):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f9ae047cde4c35b88df7728946207e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "1 Missing file for: pdf_files/Not available for 10.1002/9783527804085.ch10 64\n",
      "*************************\n",
      "2 Missing file for: pdf_files/b978-0-12-805324-9.09989-1 for 10.1016/b978-0-12-805324-9.09989-1 599\n",
      "*************************\n",
      "3 Missing file for: pdf_files/Not available for 10.1142/q0035 603\n",
      "2. PDF file names copied to app_db20220527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scman1\\AppData\\Local\\Temp\\ipykernel_284\\3906074336.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for infile in tqdm_notebook(Path(\"pdf_files\").glob('*.pdf')):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eed787bed974a95911c7bcdb41a2c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. All PDFs are indexed in app_db20220527\n",
      "Already checked for data refences in: app_db20220527\n",
      "4. PDFs data references stored in pdf_mentionsapp_db20220527\n",
      "Already checked data refences see: data_search_pdf\\pdf_mentionsapp_db20220527_int.csv\n",
      "5. PDFs data references marked for review pdf_mentionsapp_db20220527_int\n",
      "Already checked data refences see: data_search_pdf\\pdf_mentionsapp_db20220527_int.csv\n",
      "6. PDFs data references reviewed pdf_mentionsapp_db20220527_int\n",
      "pdf_mentionsapp_db20220527 app_db20220527 ./data_search_pdf\n",
      "filtered mentions: 0\n",
      "133\n",
      "[] 0\n",
      "id 791 \n",
      " Direct air capture: process technology, techno-economic and socio-political challenges\n",
      "Action:\n",
      "[{\"URL\"=>\"http://pubs.rsc.org/en/content/articlepdf/2022/EE/D1EE03523A\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1039/d1ee03523a\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "s\n",
      "id 795 \n",
      " Advances in Upgrading Biomass to Biofuels and Oxygenated Fuel Additives Using Metal Oxide Catalysts\n",
      "Action:\n",
      "[{\"URL\"=>\"https://pubs.acs.org/doi/pdf/10.1021/acs.energyfuels.1c03346\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1021/acs.energyfuels.1c03346\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "s\n",
      "id 800 \n",
      " Co-Processing Lignocellulosic Biomass and Sewage Digestate by Hydrothermal Carbonisation: Influence of Blending on Product Quality\n",
      "Action:\n",
      "[{\"URL\"=>\"https://www.mdpi.com/1996-1073/15/4/1418/pdf\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.3390/en15041418\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "s\n",
      "id 811 \n",
      " Multiscale Simulations Identify Origins of Differential Carbapenem Hydrolysis by the OXA-48 β-Lactamase\n",
      "Action:\n",
      "[{\"URL\"=>\"https://pubs.acs.org/doi/pdf/10.1021/acscatal.1c05694\", \"content-type\"=>\"application/pdf\", \"content-version\"=>\"vor\", \"intended-application\"=>\"unspecified\"}, {\"URL\"=>\"https://pubs.acs.org/doi/pdf/10.1021/acscatal.1c05694\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1021/acscatal.1c05694\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "r\n",
      "https://pubs.acs.org/doi/suppl/10.1021/acscatal.1c05694/suppl_file/cs1c05694_si_001.pdf\n",
      "id 812 \n",
      " The Critical Role of βPdZn Alloy in Pd/ZnO Catalysts for the Hydrogenation of Carbon Dioxide to Methanol\n",
      "Action:\n",
      "[{\"URL\"=>\"https://pubs.acs.org/doi/pdf/10.1021/acscatal.2c00552\", \"content-type\"=>\"application/pdf\", \"content-version\"=>\"vor\", \"intended-application\"=>\"unspecified\"}, {\"URL\"=>\"https://pubs.acs.org/doi/pdf/10.1021/acscatal.2c00552\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1021/acscatal.2c00552\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "r\n",
      "https://pubs.acs.org/doi/suppl/10.1021/acscatal.2c00552/suppl_file/cs2c00552_si_001.pdf\n",
      "id 813 \n",
      " Integrated CO2 capture and methanation on Ru/CeO2-MgO combined materials: Morphology effect from CeO2 support\n",
      "Action:\n",
      "[{\"URL\"=>\"https://api.elsevier.com/content/article/PII:S0016236122002861?httpAccept=text/xml\", \"content-type\"=>\"text/xml\", \"content-version\"=>\"vor\", \"intended-application\"=>\"text-mining\"}, {\"URL\"=>\"https://api.elsevier.com/content/article/PII:S0016236122002861?httpAccept=text/plain\", \"content-type\"=>\"text/plain\", \"content-version\"=>\"vor\", \"intended-application\"=>\"text-mining\"}]\n",
      "https://doi.org/10.1016/j.fuel.2022.123420\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "s\n",
      "id 814 \n",
      " Zr(IV) Catalyst for the Ring-Opening Copolymerization of Anhydrides (A) with Epoxides (B), Oxetane (B), and Tetrahydrofurans (C) to Make ABB- and/or ABC-Poly(ester-<i>alt</i>-ethers)\n",
      "Action:\n",
      "[{\"URL\"=>\"https://pubs.acs.org/doi/pdf/10.1021/jacs.2c01225\", \"content-type\"=>\"application/pdf\", \"content-version\"=>\"vor\", \"intended-application\"=>\"unspecified\"}, {\"URL\"=>\"https://pubs.acs.org/doi/pdf/10.1021/jacs.2c01225\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1021/jacs.2c01225\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "r\n",
      "https://pubs.acs.org/doi/suppl/10.1021/jacs.2c01225/suppl_file/ja2c01225_si_001.pdf\n",
      "id 815 \n",
      " Electrochemical separation processes for future societal challenges\n",
      "Action:\n",
      "[{\"URL\"=>\"https://api.elsevier.com/content/article/PII:S2666386422001187?httpAccept=text/xml\", \"content-type\"=>\"text/xml\", \"content-version\"=>\"vor\", \"intended-application\"=>\"text-mining\"}, {\"URL\"=>\"https://api.elsevier.com/content/article/PII:S2666386422001187?httpAccept=text/plain\", \"content-type\"=>\"text/plain\", \"content-version\"=>\"vor\", \"intended-application\"=>\"text-mining\"}]\n",
      "https://doi.org/10.1016/j.xcrp.2022.100844\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "s\n",
      "id 816 \n",
      " Heterotrinuclear Ring Opening Copolymerization Catalysis: Structure–activity Relationships\n",
      "Action:\n",
      "[{\"URL\"=>\"https://pubs.acs.org/doi/pdf/10.1021/acscatal.1c04449\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1021/acscatal.1c04449\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "r\n",
      "https://pubs.acs.org/doi/suppl/10.1021/acscatal.1c04449/suppl_file/cs1c04449_si_001.pdf\n",
      "id 817 \n",
      " Silicon microfabricated reactor for <i>operando</i> XAS/DRIFTS studies of heterogeneous catalytic reactions\n",
      "Action:\n",
      "[{\"URL\"=>\"http://pubs.rsc.org/en/content/articlepdf/2020/CY/D0CY01608J\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1039/d0cy01608j\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "r\n",
      "https://www.rsc.org/suppdata/d0/cy/d0cy01608j/d0cy01608j1.pdf\n",
      "id 818 \n",
      " Enzyme evolution and the temperature dependence of enzyme catalysis\n",
      "Action:\n",
      "[{\"URL\"=>\"https://api.elsevier.com/content/article/PII:S0959440X20301007?httpAccept=text/xml\", \"content-type\"=>\"text/xml\", \"content-version\"=>\"vor\", \"intended-application\"=>\"text-mining\"}, {\"URL\"=>\"https://api.elsevier.com/content/article/PII:S0959440X20301007?httpAccept=text/plain\", \"content-type\"=>\"text/plain\", \"content-version\"=>\"vor\", \"intended-application\"=>\"text-mining\"}]\n",
      "https://doi.org/10.1016/j.sbi.2020.06.001\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "s\n",
      "id 819 \n",
      " Design of Highly Selective Platinum Nanoparticle Catalysts for the Aerobic Oxidation of KA-Oil using Continuous-Flow Chemistry\n",
      "Action:\n",
      "[{\"URL\"=>\"https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1002%2Fcssc.201600246\", \"content-type\"=>\"application/pdf\", \"content-version\"=>\"vor\", \"intended-application\"=>\"text-mining\"}, {\"URL\"=>\"https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1002%2Fcssc.201600246\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"text-mining\"}, {\"URL\"=>\"http://onlinelibrary.wiley.com/wol1/doi/10.1002/cssc.201600246/fullpdf\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1002/cssc.201600246\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "s\n",
      "id 820 \n",
      " Catalytic Reduction of Carbon Dioxide on the (001), (011), and (111) Surfaces of TiC and ZrC: A Computational Study\n",
      "Action:\n",
      "[{\"URL\"=>\"https://pubs.acs.org/doi/pdf/10.1021/acs.jpcc.1c10180\", \"content-type\"=>\"application/pdf\", \"content-version\"=>\"vor\", \"intended-application\"=>\"unspecified\"}, {\"URL\"=>\"https://pubs.acs.org/doi/pdf/10.1021/acs.jpcc.1c10180\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1021/acs.jpcc.1c10180\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "r\n",
      "https://pubs.acs.org/doi/suppl/10.1021/acs.jpcc.1c10180/suppl_file/jp1c10180_si_001.pdf\n",
      "id 821 \n",
      " Dry reforming of methane on bimetallic Pt–Ni@CeO<sub>2</sub> catalyst: a <i>in situ</i> DRIFTS-MS mechanistic study\n",
      "Action:\n",
      "[{\"URL\"=>\"http://pubs.rsc.org/en/content/articlepdf/2021/CY/D1CY00382H\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1039/d1cy00382h\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\n",
      "https://www.rsc.org/suppdata/d1/cy/d1cy00382h/d1cy00382h1.pdf\n",
      "id 822 \n",
      " Diffusion of Isobutane in Silicalite: A Neutron Spin–Echo and Molecular Dynamics Simulation Study\n",
      "Action:\n",
      "[{\"URL\"=>\"https://pubs.acs.org/doi/pdf/10.1021/acs.jpcc.5b08048\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1021/acs.jpcc.5b08048\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "s\n",
      "id 823 \n",
      " Tailoring the selectivity of glycerol oxidation by tuning the acid–base properties of Au catalysts\n",
      "Action:\n",
      "[{\"URL\"=>\"http://pubs.rsc.org/en/content/articlepdf/2015/CY/C4CY01246A\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1039/c4cy01246a\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "r\n",
      "https://www.rsc.org/suppdata/cy/c4/c4cy01246a/c4cy01246a1.pdf\n",
      "id 824 \n",
      " Real time chemical imaging of a working catalytic membrane reactor during oxidative coupling of methane\n",
      "Action:\n",
      "[{\"URL\"=>\"http://pubs.rsc.org/en/content/articlepdf/2015/CC/C5CC03208C\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1039/c5cc03208c\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "r\n",
      "https://www.rsc.org/suppdata/c5/cc/c5cc03208c/c5cc03208c1.pdf\n",
      "id 825 \n",
      " Recent advances in automotive catalysis for NO<sub>x</sub> emission control by small-pore microporous materials\n",
      "Action:\n",
      "[{\"URL\"=>\"http://pubs.rsc.org/en/content/articlepdf/2015/CS/C5CS00108K\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "https://doi.org/10.1039/c5cs00108k\n",
      "\ts) skip (no data)\n",
      "\tr) review\n",
      "\tn) next\n",
      "\tt) terminate\n",
      "\tSelect s, r, n, t:\n",
      "s\n",
      "7. Online references reviewed html_app_db20220527\n"
     ]
    }
   ],
   "source": [
    "# (0) Add fields\n",
    "# ALTER TABLE articles \n",
    "# ADD COLUMN pdf_file varchar;\n",
    "\n",
    "#(1) previously verified files:\n",
    "last_processed = 810\n",
    "prev_db_name = \"app_db202204\"\n",
    "db_name = 'app_db20220527'\n",
    "db_name, names_added = add_pdf_file_names(prev_db_name, db_name)\n",
    "# working dir\n",
    "pdf_data_search_dir = \"./data_search_pdf\"\n",
    "\n",
    "if (names_added):\n",
    "    print (\"1. PDF file names copied to\", db_name);\n",
    "pdfs_ok = pdf_data_exists(db_name)\n",
    "if pdfs_ok:\n",
    "    print (\"2. PDF file names copied to\", db_name);\n",
    "    not_indexed = check_files_in_db(db_name)\n",
    "    if not_indexed:\n",
    "        print (\"3. All PDFs are indexed in\", db_name )\n",
    "else:\n",
    "    pdfs_ok = get_missing_pdfs(db_name)\n",
    "if pdfs_ok:\n",
    "    pdf_mentions = get_data_refs(db_name, last_processed, pdf_data_search_dir)\n",
    "    print (\"4. PDFs data references stored in\", pdf_mentions)\n",
    "    review_marked = review_interactive(pdf_mentions, pdf_data_search_dir)\n",
    "    print (\"5. PDFs data references marked for review\", review_marked)\n",
    "    revised_refs = review_interactive(pdf_mentions, pdf_data_search_dir)\n",
    "    print (\"6. PDFs data references reviewed\", revised_refs)\n",
    "    html_refs = revise_online(pdf_mentions, db_name, pdf_data_search_dir)\n",
    "    print (\"7. Online references reviewed\", html_refs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_ok = get_missing_pdfs(db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfs_ok"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if len(revised_list) > 0:\n",
    "    csvh.writre_csv_data(revised_list, 'html_revised202204.csv')\n",
    "revised_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"db_files/app_db2.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "ukchapp_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names and links for references in data mentions\n",
    "data_mentions, dm_fields = csvh.get_csv_data('pdf_mentions_filtered_02.csv', 'num')\n",
    "\n",
    "for dm in data_mentions:\n",
    "    print(\"https://doi.org/\" + data_mentions[dm]['doi'])\n",
    "    ref_name = data_mentions[dm]['ref_name']\n",
    "    while ref_name == \"\":\n",
    "        print('Please enter the name of data object:')\n",
    "        ref_name = input()\n",
    "    ref_link = data_mentions[dm]['ref_link']\n",
    "    while ref_link == \"\":\n",
    "        print('Please enter the data object link:')\n",
    "        ref_link = input()\n",
    "    data_mentions[dm]['ref_name'] = ref_name\n",
    "    data_mentions[dm]['ref_link'] = ref_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getmembers, isfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pdfminer.high_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
