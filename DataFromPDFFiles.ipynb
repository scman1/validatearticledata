{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get PDF Files for publications in the UK Catalysis Hub app db\n",
    "A list of publications is obtainded from the app database. This list will contain a titles, IDs and DOIs which need to be explored to look for the corresponding pdf files. \n",
    "The steps of the process are: \n",
    " 1. get a Title, DOI, and URL for each publication\n",
    " 2. convert the DOI to a pdf file name and try to open de file\n",
    " 3. use pdfMiner and/or CDE to get the reference to data\n",
    " 4. add a new dataset entry each time a new data object is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n",
    "# library for handling http requests\n",
    "import requests\n",
    "\n",
    "# import custom functions (common to various notebooks)\n",
    "import processing_functions as pr_fns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for ChemDataExtractor\n",
    "# not used for mining data references (suplementary/raw) or to get pdf metadata\n",
    "from chemdataextractor import Document\n",
    "\n",
    "# A function for getting a list of files from the directory\n",
    "# This will be modified to get the list from a csv file\n",
    "def get_files_list (source_dir):\n",
    "    i_counter = 0\n",
    "    files_list = []\n",
    "    for filepath in sorted(source_dir.glob('*.pdf')):\n",
    "        i_counter += 1\n",
    "        files_list.append(filepath)\n",
    "    return files_list\n",
    "\n",
    "def cde_read_pdfs(a_file):\n",
    "    pdf_f = open(a_file, 'rb')\n",
    "    doc = Document.from_file(pdf_f)\n",
    "    return doc\n",
    "\n",
    "def find_doi(element_text):\n",
    "    cr_re_01 = '10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.search(cr_re_01, element_text, re.IGNORECASE)\n",
    "    if compare != None:\n",
    "        return compare.group()\n",
    "    return \"\"\n",
    "\n",
    "def get_db_id(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"id\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_db_title(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"title\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_close_dois(str_name, db_name = \"prev_search.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, doi, title, pdf_file\"\n",
    "    filter_str = \"doi like '%\"+str_name+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"db_files/app_db.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pdf files for publications\n",
    "\n",
    "Read database and try to recover pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get publication data from the ukch app\n",
    "db_pubs = pr_fns.get_pub_app_data(ukchapp_db)\n",
    "\n",
    "# get the list of dois already mined for data \n",
    "input_file = 'pub_data_all.csv'\n",
    "id_field = 'num'\n",
    "processed, headings = csvh.get_csv_data(input_file, id_field)\n",
    "for id_num in processed:\n",
    "    current_title = processed[id_num]['doi']\n",
    "processed[1]['num']\n",
    "\n",
    "processed_dois = []\n",
    "for entry in processed:\n",
    "    if not processed[entry]['doi'] in processed_dois:\n",
    "        processed_dois.append( processed[entry]['doi'])\n",
    "\n",
    "\n",
    "for a_pub in tqdm_notebook(db_pubs):\n",
    "    pub_id = a_pub[0]\n",
    "    pub_title = a_pub[1]\n",
    "    pub_doi = a_pub[2]\n",
    "    pub_url = a_pub[3]\n",
    "    pub_pdf = a_pub[4]\n",
    "    pub_html = a_pub[5]\n",
    "    if pub_pdf == 'None':\n",
    "        print(\"*************************\")\n",
    "        print(\"Missing PDF for:\", pub_doi)\n",
    "        print(\"*************************\")\n",
    "    else:\n",
    "        pdf_file = \"pdf_files/\" + pub_pdf\n",
    "        if not Path(pdf_file).is_file():\n",
    "            print(\"*************************\")\n",
    "            print(\"Missing file for:\", pdf_file, \"for\", pub_doi)\n",
    "            print(\"*************************\")\n",
    "        else: \n",
    "            print(\"PDF filename\", pub_pdf)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pdfminer to get metadata from pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"pdf_files/\" + 's41929-019-0334-3.pdf'\n",
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "text = extract_text(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ArticlesNature Catalysis\\x0cData availabilityInformation on the data supporting the results presented here, including how toaccess them, can be found in the Cardiff University data catalogue at https://doi.org/10.17035/d.2019.0079744472.', 'Additional informationSupplementary information is available for this paper at https://doi.org/10.1038/s41929-019-0334-3.']\n",
      "https://doi.org/10.17035/d.2019.0079744472\n",
      "https://doi.org/10.1038/s41929-019-0334-3\n"
     ]
    }
   ],
   "source": [
    "def get_ref_sentences(text):\n",
    "    sentences = text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if pr_fns.is_data_stmt(sentence.lower()):\n",
    "            idx = sentences.index(sentence)\n",
    "            #context = sentences[idx-1].strip() + sentences[idx].strip() + sentences[idx+1].strip()\n",
    "            groups.append([idx-1,idx,idx+1])\n",
    "           # print(sentences.index(sentence), \":\" , len(sentence), sentence )\n",
    "            #print(context)\n",
    "\n",
    "    #print(groups)\n",
    "\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                #print(set_bg.intersection(set_g), set_bg, set_g, set_bg.union(set_g))\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                #print(ordered_union)\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                #print(set_ag.intersection(set_g), set_ag, set_g)\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "            #print(reduced_groups)\n",
    "\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        return_group.append(full_sentence)\n",
    "\n",
    "    return return_group\n",
    "\n",
    "\n",
    "def get_http_ref(sentence):\n",
    "    http_frag = \"\"\n",
    "    if 'http' in sentence.lower():\n",
    "        idx_http = sentence.lower().index('http')\n",
    "        http_frag = sentence[idx_http:]\n",
    "        space_in_ref = True\n",
    "        while \" \" in http_frag:\n",
    "            space_idx = http_frag.rfind(\" \")\n",
    "            http_frag = http_frag[:space_idx]\n",
    "        if(http_frag[-1:]==\".\"):\n",
    "            http_frag = http_frag[:-1]\n",
    "    return http_frag\n",
    "    \n",
    "\n",
    "print(get_ref_sentences(text))\n",
    "ref_sentences = get_ref_sentences(text)\n",
    "for a_sentence in ref_sentences:\n",
    "    print(get_http_ref(a_sentence))\n",
    "\n",
    "#for sentence in sentences:\n",
    "#    if 'data' in sentence.lower():\n",
    "#        print (len(sentence), sentence, sentences.index(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentences.index(sentence), \":\" , len(sentence), sentence ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can be found in the cardiff university data catalogue at https://doi.org/10.17035/d.2019.0079744472. at c 1\n",
      "data\n",
      "found\n",
      "doi\n",
      "3\n",
      "46\n",
      "43\n",
      "-1\n",
      "https://doi.org/10.17035/d.2019.0079744472\n"
     ]
    }
   ],
   "source": [
    "sentence = \"can be found in the Cardiff University data catalogue at https://doi.org/10.17035/d.2019.0079744472. At C 1\"\n",
    "pr_fns.is_data_stmt(sentence.lower())\n",
    "\n",
    "support_keys = [\"data\", \"underpin\", \"support\", \"result\", \"found\", \"find\", \"obtain\", \"doi\",\"raw\", 'information',\n",
    "                    \"provide\", \"available\", \"online\", \"supplement\"]\n",
    "print (sentence.lower())\n",
    "count = 0\n",
    "for a_word in support_keys:\n",
    "    if a_word in sentence.lower():\n",
    "        print(a_word)\n",
    "        count += 1\n",
    "print (count)\n",
    "\n",
    "if 'http' in sentence.lower():\n",
    "    idx_http = sentence.lower().index('http')\n",
    "    http_frag = sentence[idx_http:]\n",
    "    space_in_ref = True\n",
    "    while \" \" in http_frag:\n",
    "        space_idx = http_frag.rfind(\" \")\n",
    "        http_frag = http_frag[:space_idx]\n",
    "        print(http_frag.rfind(\" \"))\n",
    "    if(http_frag[-1:]==\".\"):\n",
    "        http_frag = http_frag[:-1]\n",
    "    print(http_frag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File name match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if file name matches some part of a doi\n",
    "files_list = get_not_matched_files(ukchapp_db)\n",
    "\n",
    "not_assigned = []\n",
    "for a_file in tqdm_notebook(files_list):\n",
    "    search_this = a_file.name.replace(\".pdf\", \"\").lower()\n",
    "    print(a_file.name,\"\\t\",search_this)\n",
    "    close_dois = get_close_dois(search_this, ukchapp_db)\n",
    "    print(len(close_dois))\n",
    "    \n",
    "    if len(close_dois) == 1 :\n",
    "        doi_dat = close_dois[0]\n",
    "        selected = False\n",
    "        if doi_dat[3] == None:\n",
    "            while not selected:\n",
    "                print(\"Assign file: \", a_file.name, \" to:\\n\\t\", doi_dat[0],doi_dat[1],doi_dat[2], doi_dat[3])\n",
    "                print('***************************************************************')\n",
    "                print(\"Options:\\n\\ta) assign\\n\\tb)go to next\")\n",
    "                print(\"selection:\")\n",
    "                usr_select = input()\n",
    "                if usr_select == 'a':\n",
    "                    selected = True\n",
    "                    set_pdf_file_value(a_file.name, doi_dat[0], ukchapp_db)\n",
    "                    print(\"assing and go to next\")\n",
    "                elif usr_select == 'b':\n",
    "                    #working_file[art_num]['ignore']=3 # visual inspection\n",
    "                    selected = True\n",
    "                    print(\"going to next\")\n",
    "        else:\n",
    "            print(\"Assigned in db: \",  doi_dat[0],doi_dat[1],doi_dat[2], doi_dat[3])\n",
    "    else:\n",
    "        not_assigned.append(a_file)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = get_files_list(Path(\"pdf_files\"))\n",
    "db_pubs = get_pub_app_data(ukchapp_db)\n",
    "missing=[]\n",
    "# check which files are really missing linking\n",
    "for file in files_list:\n",
    "    found_in_db = False\n",
    "    for db_pub in db_pubs:\n",
    "        if file.name == db_pub[4]:\n",
    "            found_in_db = True\n",
    "            break\n",
    "    if not found_in_db:\n",
    "        missing.append(file)\n",
    "\n",
    "# check if all linked files are in the folder\n",
    "missing2=[]\n",
    "for db_pub in db_pubs:\n",
    "    found_in_system = False\n",
    "    for file in files_list:\n",
    "        if file.name == db_pub[4] or db_pub[4] == None:\n",
    "            found_in_system = True\n",
    "            break\n",
    "    if not found_in_system:\n",
    "        missing2.append(db_pub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ChemDataExtractor to read pdf and get DOIs in document\n",
    "for a_file in tqdm_notebook(not_assigned):\n",
    "    pdf_doc = cde_read_pdfs(a_file)\n",
    "    print(a_file.name)\n",
    "    dois_list = []\n",
    "    for element in pdf_doc.elements:\n",
    "        if 'doi' in str(element):\n",
    "            found_doi = find_doi(str(element))\n",
    "            if found_doi[-1:] == \".\":\n",
    "                found_doi = found_doi[:-1]\n",
    "            if not found_doi in dois_list:\n",
    "                dois_list.append(found_doi)       \n",
    "    \n",
    "    if dois_list != [] and len(dois_list) == 1:\n",
    "        for a_doi in dois_list:\n",
    "            close_dois = get_close_dois(a_doi, ukchapp_db)\n",
    "            selected = False\n",
    "            if len(close_dois) == 1:\n",
    "                doi_dat = close_dois[0]\n",
    "                if doi_dat[3] == None:\n",
    "                    while not selected:\n",
    "                        print(\"Assign file: \",a_file.name, \" to:\\n\\t\", doi_dat[0],doi_dat[1],doi_dat[2], doi_dat[3])\n",
    "                        print('***************************************************************')\n",
    "                        print(\"Options:\\n\\ta) assign\\n\\tb)go to next\")\n",
    "                        print(\"selection:\")\n",
    "                        usr_select = input()\n",
    "                        if usr_select == 'a':\n",
    "                            selected = True\n",
    "                            set_pdf_file_value(a_file.name, doi_dat[0], ukchapp_db)\n",
    "                            print(\"assing and go to next\")\n",
    "                        elif usr_select == 'b':\n",
    "                            #working_file[art_num]['ignore']=3 # visual inspection\n",
    "                            selected = True\n",
    "                            print(\"going to next\")\n",
    "                else: \n",
    "                    print(\"Already assingned to:\\n\\t\", doi_dat[0],doi_dat[1],doi_dat[2], doi_dat[3])\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
