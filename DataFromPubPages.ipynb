{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data references from html pages\n",
    "\n",
    "A list of publications is obtainded from the app database. This list will contain a titles, IDs and DOIs which need to be explored to look for asociated data (suplementary data, raw data, processed data).\n",
    "\n",
    "The steps of the process are:\n",
    "\n",
    "1. get a Title, DOI, and URL for each publication \n",
    "2. get the DOI landing page and see if it contains references to data \n",
    "3. add a new dataset entry each time a new ds is found \n",
    "4. link the dataset to the publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "\n",
    "# read and write csv files\n",
    "import lib.handle_csv as csv_rw\n",
    "\n",
    "# Parsing html \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# http requests \n",
    "import requests\n",
    "\n",
    "# url parser\n",
    "from urllib.parse import urlparse  # python 3.x\n",
    "\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "# library for using regular expressions\n",
    "import re\n",
    "\n",
    "# values for metadata class names to exclude\n",
    "exclude_metadata = {'nature':['viewport', 'msapplication-TileColor', 'msapplication-config', 'theme-color', \n",
    "                    'application-name', 'robots', 'access', 'WT.cg_s', 'WT.z_bandiera_abtest', 'WT.page_categorisation',\n",
    "                    'WT.template', 'WT.z_cg_type', 'WT.cg_n', 'dc.rights', 'prism.issn'],'springer':['viewport', 'msapplication-TileColor', 'msapplication-config',  'theme-color', \n",
    "                    'application-name', 'robots', 'access', 'WT.cg_s', 'WT.z_bandiera_abtest', 'WT.page_categorisation',\n",
    "                    'WT.template', 'WT.z_cg_type', 'WT.cg_n', 'dc.rights', 'prism.issn'],\"wiley\":[],'rsc':['viewport',\n",
    "                    'format-detection', 'msapplication-TileColor', 'theme-color', 'dc.domain', 'twitter:card',\n",
    "                    'twitter:site'], \"acs\":['pbContext','viewport','robots','twitter:description','pb-robots-disabled',\n",
    "                    'twitter:card','twitter:site','twitter:image','twitter:title','google-site-verification']}\n",
    "\n",
    "# values for section labels which may contain references to data\n",
    "section_labels = {'nature':{'aria-labelledby':'data-availability'},'springer':{'aria-labelledby':'data-availability'}}\n",
    "\n",
    "# values for div which may contain references to data\n",
    "div_filters = {'nature':{'class':'c-article-supplementary__item'}, 'springer':{'class':\"c-article-supplementary__item\"}}\n",
    "\n",
    "#  Custom functions to get references to datasets\n",
    "# returns beautifulsoup object from given url\n",
    "def get_content(url):\n",
    "    html_soup = None\n",
    "    try:\n",
    "        req_head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n",
    "                    (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'}\n",
    "        response = requests.get(url, headers = req_head)\n",
    "        redirected_to = response.url\n",
    "        html_soup = BeautifulSoup(response.text,'html.parser')       \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return html_soup, redirected_to\n",
    "\n",
    "# get metadata\n",
    "def get_metadata(soup, journal):\n",
    "    result=[]\n",
    "    try:\n",
    "        metadata = soup.find_all('meta')\n",
    "        ignore_these = []\n",
    "        if journal in exclude_metadata:\n",
    "            ignore_these = exclude_metadata[journal] \n",
    "        else:\n",
    "            print('new journal')\n",
    "        for md_item in metadata:\n",
    "            if md_item.has_attr(\"name\") and not md_item[\"name\"] in ignore_these :\n",
    "                result.append(md_item)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return result\n",
    "\n",
    "# get data ref from metadata\n",
    "def get_data_from_metadata(soup, journal = 'nature', data_refs = []):\n",
    "    data_refs = []\n",
    "    res = get_metadata(soup, journal)\n",
    "    # check if metadata references supporting data or supplementary data\n",
    "    for md_item in res:\n",
    "        if 'data' in str(md_item[\"name\"]).lower():\n",
    "            #print(md_item[\"name\"], md_item[\"content\"])\n",
    "            ret_data = md_item[\"content\"]\n",
    "            data_refs.append({'type':\"metadata\", \"name\":md_item[\"name\"], 'data_url':md_item[\"content\"]})\n",
    "    # get author(s) data from metadata\n",
    "    #for md_item in res:\n",
    "    #    if 'author' in str(md_item[\"name\"]).lower():\n",
    "    #        print(md_item[\"name\"], md_item[\"content\"])    \n",
    "    return data_refs\n",
    "\n",
    "def get_data_from_section(soup, journal = 'nature', data_refs = [], base_url=\"\"):\n",
    "    inspect_these = {}\n",
    "    if journal in section_labels:\n",
    "        inspect_these = section_labels[journal]\n",
    "    for sec_filter in inspect_these:\n",
    "        sections = soup.find_all('section', {sec_filter:inspect_these[sec_filter]})\n",
    "        for section in sections:\n",
    "            pars = section.find_all('p')\n",
    "            for par in pars:\n",
    "                references = par.find_all('a')\n",
    "                if len(references) == 0:\n",
    "                   data_refs.append({'type':inspect_these[sec_filter], \"name\":par.contents[0], 'data_url':None}) \n",
    "                for a_ref in references:\n",
    "                    content_text = a_ref.contents[0]\n",
    "                    data_url = a_ref['href']\n",
    "                    if data_url[0] == '/' and base_url != \"\":\n",
    "                        data_url = base_url + data_url\n",
    "                    data_refs.append({'type':inspect_these[sec_filter], \"name\":a_ref.contents[0], 'data_url':data_url})\n",
    "    return data_refs\n",
    "\n",
    "def get_data_from_divs(soup, journal = 'nature', data_refs = [], base_url=\"\"):\n",
    "    inspect_these = {}\n",
    "    if journal in div_filters:\n",
    "        inspect_these = div_filters[journal]\n",
    "    for div_filter in inspect_these:\n",
    "        divs = soup.find_all('div',{div_filter:inspect_these[div_filter]})\n",
    "        for div in divs:\n",
    "            a_ref =  div.find('a')\n",
    "            content_text = a_ref.contents[0]\n",
    "            data_url = a_ref['href']\n",
    "            if data_url[0] == '/' and base_url != \"\":\n",
    "                data_url = base_url + data_url\n",
    "            data_refs.append({'type':\"supplementary\", \"name\":a_ref.contents[0], 'data_url':data_url})\n",
    "    return data_refs\n",
    "\n",
    "\n",
    "# Wiley online stores supplementary in tables on the article page. \n",
    "table_filters={'wiley':{\"class\":\"support-info__table\"}}\n",
    "def get_data_from_tables(soup, journal = 'wiley', data_refs = [], base_url=\"\"):\n",
    "    inspect_these = {}\n",
    "    if journal in table_filters:\n",
    "        inspect_these = table_filters[journal]\n",
    "    for tbl_filter in inspect_these:\n",
    "        tables = soup.find_all('table',{tbl_filter:inspect_these[tbl_filter]})\n",
    "        for table in tables:\n",
    "            # find rows\n",
    "            trs = table.find_all('tr')\n",
    "            # get the type and link from each row\n",
    "            for tr in trs:\n",
    "                td_link = tr.find('td',{\"headers\":\"article-filename\"})\n",
    "                td_desc = tr.find('td',{\"headers\":\"article-description\"})\n",
    "                data_link = td_link.find('a')\n",
    "                data_url = data_link['href']\n",
    "                if data_url[0] == '/' and base_url != \"\":\n",
    "                    data_url = base_url + data_url\n",
    "                data_refs.append({'type':td_desc.contents[0], \"name\":data_link.contents[0], 'data_url':data_url})\n",
    "    return data_refs\n",
    "\n",
    "# extract from anchor in text publications\n",
    "a_filters={'rsc':{\"class\":\"list__item-link\"}, 'acs':{\"class\":\"suppl-anchor\"}}\n",
    "def get_data_from_anchor(soup, journal = 'rsc', data_refs = [], base_url=\"\"):\n",
    "    # find line for supplementary\n",
    "    if journal in a_filters:\n",
    "        inspect_these = a_filters[journal]\n",
    "    for a_filter in inspect_these:\n",
    "        supp_h2_line = -1\n",
    "        inspect_heads = soup.find_all(\"h2\")\n",
    "        for a_head in inspect_heads:\n",
    "            for content in a_head.contents:\n",
    "                if content != None and \"supplementary\" in str(content).lower() :\n",
    "                    supp_h2_line = a_head.sourceline\n",
    "\n",
    "        # Use the position of \"header line\" as offset to look for data links\n",
    "        links = soup.find_all(\"a\", {a_filter:inspect_these[a_filter]})\n",
    "        for link in links:\n",
    "            dt_type = dt_link = dt_name = \"\"\n",
    "            if link.sourceline > supp_h2_line:\n",
    "                #print(link, \"\\nLine: \", link.sourceline)\n",
    "                if journal == 'rsc':\n",
    "                    l_spans = link.find_all(\"span\",{\"class\":\"list__item-label\"})\n",
    "                    for a_span in l_spans:\n",
    "                        for contnt in a_span.contents:\n",
    "                            if 'supplementary' in str(contnt).lower():\n",
    "                                dt_link = link['href']\n",
    "                                dt_name = str(contnt).strip()\n",
    "                                #print('supplementary', link['href'], str(contnt).strip())\n",
    "                            if str(type(contnt)) == \"<class 'bs4.element.Tag'>\":\n",
    "                                #print(contnt.contents[0])\n",
    "                                dt_name += contnt.contents[0]\n",
    "                            #print(str(type(contnt)), str(contnt).strip())\n",
    "                elif journal == 'acs':\n",
    "                    dt_link = link['href']\n",
    "                    dt_name = str(link.contents[0]).strip()\n",
    "            if dt_link != \"\" and dt_name != \"\":\n",
    "                if dt_link[0] == '/' and base_url != \"\":\n",
    "                    dt_link = base_url + dt_link\n",
    "                data_refs.append({'type':'supplementary',\"name\":dt_name, 'data_url':dt_link})\n",
    "    return data_refs\n",
    "\n",
    "# get full doc from rsc landig page\n",
    "def get_full_html_doc(soup):\n",
    "    # check if full html text is available\n",
    "    more_soup = anoter_url = None\n",
    "    metadata = soup.find_all(\"meta\",{\"name\":\"citation_fulltext_html_url\"})\n",
    "    if len(metadata)> 0:\n",
    "        more_soup, anoter_url = get_content(metadata[0]['content'])\n",
    "    return more_soup, anoter_url\n",
    "\n",
    "# verify if statement refers to supporting data\n",
    "def is_data_stmt(statement=\"\"):\n",
    "    support_keys = [\"data\", \"underpin\", \"support\", \"result\", \"found\", \"find\", \"obtain\", \"doi\",\"raw\", \"information\"\n",
    "                    \"provide\", \"availabe\", \"online\"]\n",
    "    count = 0\n",
    "    for a_word in support_keys:\n",
    "        if a_word in statement:\n",
    "            count += 1\n",
    "    if count > 2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# get data references from full html doc\n",
    "def get_data_from_html_doc(soup, journal = 'rsc', data_refs = [], base_url=\"\"):\n",
    "    if journal == 'rsc':\n",
    "        # rsc lists the link to full html document in metadata\n",
    "        more_soup, another_url  = get_full_html_doc(soup)\n",
    "        if more_soup != None and another_url != None:\n",
    "            base_url = get_base_url(another_url)\n",
    "            soup = more_soup\n",
    "    tag_targets = ['p', 'span']\n",
    "    for tag_name in tag_targets:\n",
    "        paras = soup.find_all(tag_name)\n",
    "        for para in paras:\n",
    "            for cont_para in para.contents:\n",
    "                content = str(cont_para).lower()\n",
    "                if 'data' in content:\n",
    "                    intresting = \"\"\n",
    "                    if 'data' in content[content.rfind(\".\")+2:]:\n",
    "                        intresting = content[content.rfind(\".\")+2:]\n",
    "                    else:\n",
    "                        intresting = content[:content.rfind(\".\")]\n",
    "                    anchor_refs = para.find_all('a')\n",
    "                    if len(anchor_refs)>0 and is_data_stmt(intresting):\n",
    "                        for a_ref in anchor_refs:\n",
    "                            dt_link = a_ref['href']\n",
    "                            dt_name = str(a_ref.contents[0])\n",
    "                            if dt_link != \"\" and dt_name != \"\" and dt_link[0] != \"#\":\n",
    "                                if dt_link[0] == '/' and base_url != \"\":\n",
    "                                    dt_link = base_url + dt_link \n",
    "                                data_refs.append({'type':'supporting',\"name\":dt_name, 'data_url':dt_link})\n",
    "    return data_refs\n",
    "\n",
    "# get a list of ids, titles, dois, links, pdf_file and \n",
    "# html_file names from the app database\n",
    "def get_pub_app_data(db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi, link, pdf_file, html_file\"\n",
    "    filter_str = \"status = 'Added'\"\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "def get_base_url(response_url):\n",
    "    parsed_uri = urlparse(response_url)  # returns six components\n",
    "    base_url = parsed_uri.scheme + \"://\" + parsed_uri.netloc\n",
    "    return base_url\n",
    "\n",
    "# use regular expression to check if a given string\n",
    "# is a valid DOI, using pattern from CR\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    if cr_doi == None:\n",
    "        return False\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the publications list from the app database\n",
    "ukchapp_db = \"db_files/app_db2.sqlite3\"\n",
    "db_pubs = get_pub_app_data(ukchapp_db)\n",
    "\n",
    "# get the list of dois already mined for data \n",
    "input_file = 'pub_data_add202012.csv'\n",
    "id_field = 'num'\n",
    "processed, headings = csv_rw.get_csv_data(input_file, id_field)\n",
    "processed_dois = []\n",
    "for entry in processed:\n",
    "    if not processed[entry]['doi'] in processed_dois:\n",
    "        processed_dois.append( processed[entry]['doi'])\n",
    "\n",
    "data_records = {}\n",
    "ref_count = 1\n",
    "for a_pub in tqdm_notebook(db_pubs):\n",
    "    if a_pub[0] > 616: # only check new publications added after 616\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        pub_html = a_pub[5]\n",
    "        publishers = ['acs', \"wiley\", \"springer\", \"rsc\", 'nature','elsevier']\n",
    "        if not pub_doi in processed_dois and valid_doi(pub_doi):\n",
    "            # use doi reference to get landing page\n",
    "            url = \"http://dx.doi.org/\" + pub_doi\n",
    "            doc_content, response_url = get_content(url)\n",
    "            base_url = get_base_url(response_url)\n",
    "            publisher = 'another_pub'\n",
    "            for pb_name in publishers:\n",
    "                if pb_name in base_url:\n",
    "                    publisher = pb_name\n",
    "            print(pub_id, \"Title: \", pub_title, \" look up: \", base_url, \" publisher:\", publisher)\n",
    "            res = []\n",
    "            if publisher in ['springer', 'nature']:\n",
    "                res = get_data_from_metadata(doc_content, publisher, res)\n",
    "                res = get_data_from_section(doc_content, publisher, res, base_url)\n",
    "                res = get_data_from_divs(doc_content, publisher, res, base_url)\n",
    "            if publisher in ['wiley']:\n",
    "                res = get_data_from_tables(doc_content, publisher, res, base_url)\n",
    "            if publisher in ['rsc','acs']:\n",
    "                res = get_data_from_anchor(doc_content, publisher, res, base_url)\n",
    "                res = get_data_from_html_doc(doc_content, publisher, res, base_url)\n",
    "            if res != []:\n",
    "                for data_ref in res:\n",
    "                    #print(data_ref)\n",
    "                    data_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                    data_record.update(data_ref)\n",
    "                    data_records[ref_count] = data_record\n",
    "                    ref_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(processed_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data_records) > 0:\n",
    "    csv_rw.write_csv_data(data_records, 'pub_data_add202012.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_pubs = []\n",
    "for a_pub in db_pubs:\n",
    "    if a_pub[0] > 616: # only check new publications added after 616\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        pub_html = a_pub[5]\n",
    "        if \"acs\" in str(pub_url).lower():\n",
    "            data_found = False\n",
    "            for dr in data_records:\n",
    "                if pub_id == data_records[dr]['id']:\n",
    "                    data_found = True\n",
    "                    break\n",
    "            if not data_found:\n",
    "                url = \"http://dx.doi.org/\" + pub_doi\n",
    "                print(\"missing: \", pub_id, pub_doi, url)\n",
    "                missing_pubs.append(a_pub)\n",
    "    len(missing_pubs)\n",
    "print(missing_pubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://doi.org/10.1016/j.apcata.2018.10.010'\n",
    "req_head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'}\n",
    "response = requests.get(url, headers = req_head)\n",
    "\n",
    "response = requests.get(response.url, headers = req_head)\n",
    "print(response)\n",
    "print(response.url)\n",
    "redirected_to = response.url\n",
    "parsed_uri = urlparse(redirected_to)  # returns six components\n",
    "print(parsed_uri)\n",
    "domain = parsed_uri.netloc\n",
    "result = domain.replace('www.', '')  # as per your case\n",
    "print(domain)\n",
    "base_url = parsed_uri.scheme + \"://\" + parsed_uri.netloc\n",
    "\n",
    "print(base_url)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "# check if full html text is available\n",
    "metadata = soup.find_all(\"meta\")#,{\"name\":\"citation_fulltext_html_url\"})\n",
    "for meta in metadata:\n",
    "  print(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exclude_metadata = {'nature':['viewport', 'msapplication-TileColor', 'msapplication-config',  'theme-color', \n",
    "                    'application-name', 'robots', 'access', 'WT.cg_s', 'WT.z_bandiera_abtest', 'WT.page_categorisation',\n",
    "                    'WT.template', 'WT.z_cg_type', 'WT.cg_n', 'dc.rights', 'prism.issn'],'springer':['viewport', \n",
    "                    'msapplication-TileColor', 'msapplication-config',  'theme-color', \n",
    "                    'application-name', 'robots', 'access', 'WT.cg_s', 'WT.z_bandiera_abtest', 'WT.page_categorisation',\n",
    "                    'WT.template', 'WT.z_cg_type', 'WT.cg_n', 'dc.rights', 'prism.issn'],\"wiley\":[], 'rsc':['viewport',\n",
    "                    'format-detection', 'msapplication-TileColor', 'theme-color', 'dc.domain','twitter:card',\n",
    "                    'twitter:site'],\"acs\":['pbContext','viewport','robots','twitter:description','pb-robots-disabled',\n",
    "                    'twitter:card','twitter:site','twitter:image','twitter:title','google-site-verification'],'elsevier':\n",
    "                    []}\n",
    "metadata = soup.find_all(\"meta\",{\"name\":True})\n",
    "\n",
    "publisher = 'elsevier'\n",
    "for md in metadata:\n",
    "    if not md['name'] in exclude_metadata[publisher]:\n",
    "        print(\"X:\", md['name'])\n",
    "        print(md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
