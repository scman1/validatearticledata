{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "\n",
    "# Parsing html \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# http requests \n",
    "import requests\n",
    "\n",
    "# values for metadata class names to exclude\n",
    "exclude_metadata = {'nature catalysis':['viewport', 'msapplication-TileColor', 'msapplication-config',  'theme-color', \n",
    "                    'application-name', 'robots', 'access', 'WT.cg_s', 'WT.z_bandiera_abtest', 'WT.page_categorisation',\n",
    "                    'WT.template', 'WT.z_cg_type', 'WT.cg_n', 'dc.rights', 'prism.issn']}\n",
    "\n",
    "# values for section labels which may contain references to data\n",
    "section_labels = {'nature catalysis':{'aria-labelledby':'data-availability'}}\n",
    "\n",
    "#  Custom functions to get references to datasets\n",
    "# returns beautifulsoup object from given url\n",
    "def get_content(url):\n",
    "    html_soup = None\n",
    "    try:\n",
    "        req_head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'}\n",
    "        response = requests.get(url, headers = req_head)\n",
    "        html_soup = BeautifulSoup(response.text,'html.parser')       \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return html_soup\n",
    "\n",
    "# get metadata\n",
    "def get_metadata(soup, journal):\n",
    "    result=[]\n",
    "    try:\n",
    "        metadata = soup.find_all('meta')\n",
    "        ignore_these = []\n",
    "        if journal in exclude_metadata:\n",
    "            ignore_these = exclude_metadata[journal] \n",
    "        else:\n",
    "            print('new journal')\n",
    "        for md_item in metadata:\n",
    "            if md_item.has_attr(\"name\") and not md_item[\"name\"] in ignore_these :\n",
    "                result.append(md_item)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return result\n",
    "\n",
    "# get data ref from metadata\n",
    "def get_data_from_metadata(soup, journal = 'nature catalysis'):\n",
    "    ret_data = \"\"\n",
    "    res = get_metadata(soup, 'nature catalysis')\n",
    "    # check if metadata references supporting data or supplementary data\n",
    "    for md_item in res:\n",
    "        if 'data' in str(md_item[\"name\"]).lower():\n",
    "            #print(md_item[\"name\"], md_item[\"content\"])\n",
    "            ret_data = md_item[\"content\"]\n",
    "    # get author(s) data from metadata\n",
    "    #for md_item in res:\n",
    "    #    if 'author' in str(md_item[\"name\"]).lower():\n",
    "    #        print(md_item[\"name\"], md_item[\"content\"])    \n",
    "    return ret_data\n",
    "\n",
    "def get_data_from_section(soup, journal = 'nature catalysis'):\n",
    "    data_link = \"\"\n",
    "    sections = soup.find_all('section')\n",
    "    for section in sections:\n",
    "        inspect_this = section_labels[journal]\n",
    "        for sect_attr in inspect_this:\n",
    "            if section.has_attr(sect_attr) and section[sect_attr] == 'data-availability':\n",
    "                #print(section[\"aria-labelledby\"])\n",
    "                pars = section.find_all('p')\n",
    "                for par in pars:\n",
    "                    #print(par)\n",
    "                    references = par.find_all('a')\n",
    "                    for reference in references:\n",
    "                        for content in reference.contents:\n",
    "                            #print(content)\n",
    "                            data_link = content\n",
    "    return data_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found in metadata\n",
      "Found as section:  https://github.com/AlexanderHoffman/supporting-info\n"
     ]
    }
   ],
   "source": [
    "url = \"http://dx.doi.org/10.1038/s41563-020-0800-y\"\n",
    "\n",
    "doc_content = get_content(url)\n",
    "res = get_data_from_metadata(doc_content, journal = 'nature catalysis')\n",
    "if res != \"\":\n",
    "    print(res)\n",
    "else:\n",
    "    print(\"Not found in metadata\")\n",
    "    \n",
    "    \n",
    "res = get_data_from_section(doc_content, 'nature catalysis')\n",
    "\n",
    "if res != \"\":\n",
    "    print(\"Found as section: \", res)\n",
    "else:\n",
    "    print(\"Not found in section\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://github.com/AlexanderHoffman/supporting-info'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'}\n",
    "response = requests.get(url, headers = req_head)\n",
    "soup = BeautifulSoup(response.text,'html.parser')  \n",
    "sections = soup.find_all('section')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
