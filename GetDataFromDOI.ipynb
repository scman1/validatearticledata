{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data references from html pages\n",
    "\n",
    "A list of publications is obtainded from the app database. This list will contain a titles, IDs and DOIs which need to be explored to look for asociated data (suplementary data, raw data, processed data).\n",
    "\n",
    "The steps of the process are:\n",
    "\n",
    "1. get a Title, DOI, and URL for each publication \n",
    "2. get the DOI landing page and see if it contains references to data \n",
    "3. add a new dataset entry each time a new ds is found \n",
    "4. link the dataset to the publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "\n",
    "# read and write csv files\n",
    "import lib.handle_csv as csv_rw\n",
    "\n",
    "# Parsing html \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# http requests \n",
    "import requests\n",
    "\n",
    "# url parser\n",
    "from urllib.parse import urlparse  # python 3.x\n",
    "\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "# library for using regular expressions\n",
    "import re\n",
    "\n",
    "\n",
    "# values for metadata class names to exclude\n",
    "exclude_metadata = {'nature':['viewport', 'msapplication-TileColor', 'msapplication-config', 'theme-color', \n",
    "                    'application-name', 'robots', 'access', 'WT.cg_s', 'WT.z_bandiera_abtest', 'WT.page_categorisation',\n",
    "                    'WT.template', 'WT.z_cg_type', 'WT.cg_n', 'dc.rights', 'prism.issn'],'springer':['viewport', 'msapplication-TileColor', 'msapplication-config',  'theme-color', \n",
    "                    'application-name', 'robots', 'access', 'WT.cg_s', 'WT.z_bandiera_abtest', 'WT.page_categorisation',\n",
    "                    'WT.template', 'WT.z_cg_type', 'WT.cg_n', 'dc.rights', 'prism.issn'],\"wiley\":[],'rsc':['viewport',\n",
    "                    'format-detection', 'msapplication-TileColor', 'theme-color', 'dc.domain', 'twitter:card',\n",
    "                    'twitter:site'], \"acs\":['pbContext','viewport','robots','twitter:description','pb-robots-disabled',\n",
    "                    'twitter:card','twitter:site','twitter:image','twitter:title','google-site-verification']}\n",
    "\n",
    "# values for section labels which may contain references to data\n",
    "section_labels = {'nature':{'aria-labelledby':'data-availability'},'springer':{'aria-labelledby':'data-availability'}}\n",
    "\n",
    "# values for div which may contain references to data\n",
    "div_filters = {'nature':{'class':'c-article-supplementary__item'}, 'springer':{'class':\"c-article-supplementary__item\"}}\n",
    "\n",
    "#  Custom functions to get references to datasets\n",
    "# returns beautifulsoup object from given url\n",
    "def get_content(url):\n",
    "    html_soup = None\n",
    "    try:\n",
    "        req_head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n",
    "                    (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'}\n",
    "        response = requests.get(url, headers = req_head)\n",
    "        redirected_to = response.url\n",
    "        html_soup = BeautifulSoup(response.text,'html.parser')       \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return html_soup, redirected_to\n",
    "\n",
    "# get metadata\n",
    "def get_metadata(soup, journal):\n",
    "    result=[]\n",
    "    try:\n",
    "        metadata = soup.find_all('meta')\n",
    "        ignore_these = []\n",
    "        if journal in exclude_metadata:\n",
    "            ignore_these = exclude_metadata[journal] \n",
    "        else:\n",
    "            print('new journal')\n",
    "        for md_item in metadata:\n",
    "            if md_item.has_attr(\"name\") and not md_item[\"name\"] in ignore_these :\n",
    "                result.append(md_item)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return result\n",
    "\n",
    "# get data ref from metadata\n",
    "def get_data_from_metadata(soup, journal = 'nature', data_refs = []):\n",
    "    data_refs = []\n",
    "    res = get_metadata(soup, journal)\n",
    "    # check if metadata references supporting data or supplementary data\n",
    "    for md_item in res:\n",
    "        if 'data' in str(md_item[\"name\"]).lower():\n",
    "            #print(md_item[\"name\"], md_item[\"content\"])\n",
    "            ret_data = md_item[\"content\"]\n",
    "            data_refs.append({'type':\"metadata\", \"name\":md_item[\"name\"], 'data_url':md_item[\"content\"]})\n",
    "    # get author(s) data from metadata\n",
    "    #for md_item in res:\n",
    "    #    if 'author' in str(md_item[\"name\"]).lower():\n",
    "    #        print(md_item[\"name\"], md_item[\"content\"])    \n",
    "    return data_refs\n",
    "\n",
    "def get_data_from_section(soup, journal = 'nature', data_refs = [], base_url=\"\"):\n",
    "    inspect_these = {}\n",
    "    if journal in section_labels:\n",
    "        inspect_these = section_labels[journal]\n",
    "    for sec_filter in inspect_these:\n",
    "        sections = soup.find_all('section', {sec_filter:inspect_these[sec_filter]})\n",
    "        for section in sections:\n",
    "            pars = section.find_all('p')\n",
    "            for par in pars:\n",
    "                references = par.find_all('a')\n",
    "                if len(references) == 0:\n",
    "                   data_refs.append({'type':inspect_these[sec_filter], \"name\":par.contents[0], 'data_url':None}) \n",
    "                for a_ref in references:\n",
    "                    content_text = a_ref.contents[0]\n",
    "                    data_url = a_ref['href']\n",
    "                    if data_url[0] == '/' and base_url != \"\":\n",
    "                        data_url = base_url + data_url\n",
    "                    data_refs.append({'type':inspect_these[sec_filter], \"name\":a_ref.contents[0], 'data_url':data_url})\n",
    "    return data_refs\n",
    "\n",
    "def get_data_from_divs(soup, journal = 'nature', data_refs = [], base_url=\"\"):\n",
    "    inspect_these = {}\n",
    "    if journal in div_filters:\n",
    "        inspect_these = div_filters[journal]\n",
    "    for div_filter in inspect_these:\n",
    "        divs = soup.find_all('div',{div_filter:inspect_these[div_filter]})\n",
    "        for div in divs:\n",
    "            a_ref =  div.find('a')\n",
    "            content_text = a_ref.contents[0]\n",
    "            data_url = a_ref['href']\n",
    "            if data_url[0] == '/' and base_url != \"\":\n",
    "                data_url = base_url + data_url\n",
    "            data_refs.append({'type':\"supplementary\", \"name\":a_ref.contents[0], 'data_url':data_url})\n",
    "    return data_refs\n",
    "\n",
    "\n",
    "# Wiley online stores supplementary in tables on the article page. \n",
    "table_filters={'wiley':{\"class\":\"support-info__table\"}}\n",
    "def get_data_from_tables(soup, journal = 'wiley', data_refs = [], base_url=\"\"):\n",
    "    inspect_these = {}\n",
    "    if journal in table_filters:\n",
    "        inspect_these = table_filters[journal]\n",
    "    for tbl_filter in inspect_these:\n",
    "        tables = soup.find_all('table',{tbl_filter:inspect_these[tbl_filter]})\n",
    "        for table in tables:\n",
    "            # find rows\n",
    "            trs = table.find_all('tr')\n",
    "            # get the type and link from each row\n",
    "            for tr in trs:\n",
    "                td_link = tr.find('td',{\"headers\":\"article-filename\"})\n",
    "                td_desc = tr.find('td',{\"headers\":\"article-description\"})\n",
    "                data_link = td_link.find('a')\n",
    "                data_url = data_link['href']\n",
    "                if data_url[0] == '/' and base_url != \"\":\n",
    "                    data_url = base_url + data_url\n",
    "                data_refs.append({'type':td_desc.contents[0], \"name\":data_link.contents[0], 'data_url':data_url})\n",
    "    return data_refs\n",
    "\n",
    "# extract from anchor in text publications\n",
    "a_filters={'rsc':{\"class\":\"list__item-link\"}, 'acs':{\"class\":\"suppl-anchor\"}}\n",
    "def get_data_from_anchor(soup, journal = 'rsc', data_refs = [], base_url=\"\"):\n",
    "    # find line for supplementary\n",
    "    if journal in a_filters:\n",
    "        inspect_these = a_filters[journal]\n",
    "    for a_filter in inspect_these:\n",
    "        supp_h2_line = -1\n",
    "        inspect_heads = soup.find_all(\"h2\")\n",
    "        for a_head in inspect_heads:\n",
    "            for content in a_head.contents:\n",
    "                if content != None and \"supplementary\" in str(content).lower() :\n",
    "                    supp_h2_line = a_head.sourceline\n",
    "\n",
    "        # Use the position of \"header line\" as offset to look for data links\n",
    "        links = soup.find_all(\"a\", {a_filter:inspect_these[a_filter]})\n",
    "        for link in links:\n",
    "            dt_type = dt_link = dt_name = \"\"\n",
    "            if link.sourceline > supp_h2_line:\n",
    "                #print(link, \"\\nLine: \", link.sourceline)\n",
    "                if journal == 'rsc':\n",
    "                    l_spans = link.find_all(\"span\",{\"class\":\"list__item-label\"})\n",
    "                    for a_span in l_spans:\n",
    "                        for contnt in a_span.contents:\n",
    "                            if 'supplementary' in str(contnt).lower():\n",
    "                                dt_link = link['href']\n",
    "                                dt_name = str(contnt).strip()\n",
    "                                #print('supplementary', link['href'], str(contnt).strip())\n",
    "                            if str(type(contnt)) == \"<class 'bs4.element.Tag'>\":\n",
    "                                #print(contnt.contents[0])\n",
    "                                dt_name += contnt.contents[0]\n",
    "                            #print(str(type(contnt)), str(contnt).strip())\n",
    "                elif journal == 'acs':\n",
    "                    dt_link = link['href']\n",
    "                    dt_name = str(link.contents[0]).strip()\n",
    "            if dt_link != \"\" and dt_name != \"\":\n",
    "                if dt_link[0] == '/' and base_url != \"\":\n",
    "                    dt_link = base_url + dt_link\n",
    "                data_refs.append({'type':'supplementary',\"name\":dt_name, 'data_url':dt_link})\n",
    "    return data_refs\n",
    "\n",
    "# get full doc from rsc landig page\n",
    "def get_full_html_doc(soup):\n",
    "    # check if full html text is available\n",
    "    more_soup = anoter_url = None\n",
    "    metadata = soup.find_all(\"meta\",{\"name\":\"citation_fulltext_html_url\"})\n",
    "    if len(metadata)> 0:\n",
    "        more_soup, anoter_url = get_content(metadata[0]['content'])\n",
    "    return more_soup, anoter_url\n",
    "\n",
    "# verify if statement refers to supporting data\n",
    "def is_data_stmt(statement=\"\"):\n",
    "    support_keys = [\"data\", \"underpin\", \"support\", \"result\", \"found\", \"find\", \"obtain\", \"doi\",\"raw\", \"information\"\n",
    "                    \"provide\", \"availabe\", \"online\"]\n",
    "    count = 0\n",
    "    for a_word in support_keys:\n",
    "        if a_word in statement:\n",
    "            count += 1\n",
    "    if count > 2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# get data references from full html doc\n",
    "def get_data_from_html_doc(soup, journal = 'rsc', data_refs = [], base_url=\"\"):\n",
    "    if journal == 'rsc':\n",
    "        # rsc lists the link to full html document in metadata\n",
    "        more_soup, another_url  = get_full_html_doc(soup)\n",
    "        if more_soup != None and another_url != None:\n",
    "            base_url = get_base_url(another_url)\n",
    "            soup = more_soup\n",
    "    tag_targets = ['p', 'span']\n",
    "    for tag_name in tag_targets:\n",
    "        paras = soup.find_all(tag_name)\n",
    "        for para in paras:\n",
    "            for cont_para in para.contents:\n",
    "                content = str(cont_para).lower()\n",
    "                if 'data' in content:\n",
    "                    intresting = \"\"\n",
    "                    if 'data' in content[content.rfind(\".\")+2:]:\n",
    "                        intresting = content[content.rfind(\".\")+2:]\n",
    "                    else:\n",
    "                        intresting = content[:content.rfind(\".\")]\n",
    "                    anchor_refs = para.find_all('a')\n",
    "                    if len(anchor_refs)>0 and is_data_stmt(intresting):\n",
    "                        for a_ref in anchor_refs:\n",
    "                            dt_link = a_ref['href']\n",
    "                            dt_name = str(a_ref.contents[0])\n",
    "                            if dt_link != \"\" and dt_name != \"\" and dt_link[0] != \"#\":\n",
    "                                if dt_link[0] == '/' and base_url != \"\":\n",
    "                                    dt_link = base_url + dt_link \n",
    "                                data_refs.append({'type':'supporting',\"name\":dt_name, 'data_url':dt_link})\n",
    "    return data_refs\n",
    "\n",
    "# get a list of ids, titles, dois, links, pdf_file and \n",
    "# html_file names from the data database\n",
    "def get_pub_data(db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi, link, pdf_file, html_file\"\n",
    "    filter_str = \"status = 'Added'\"\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "# get a list of ids, titles, dois, links, pdf_file and \n",
    "# html_file names from the data database\n",
    "def get_pub_data(db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi, link, pdf_file, html_file\"\n",
    "    filter_str = \"status = 'Added'\"\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "\n",
    "def get_base_url(response_url):\n",
    "    parsed_uri = urlparse(response_url)  # returns six components\n",
    "    base_url = parsed_uri.scheme + \"://\" + parsed_uri.netloc\n",
    "    return base_url\n",
    "\n",
    "# use regular expression to check if a given string\n",
    "# is a valid DOI, using pattern from CR\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    if cr_doi == None:\n",
    "        return False\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_html_parsed(doi='10.5286/ISIS.E.24067306'):\n",
    "    url = 'https://dx.doi.org/'+doi\n",
    "    req_head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'}\n",
    "    response = requests.get(url, headers = req_head)\n",
    "\n",
    "    response = requests.get(response.url, headers = req_head)\n",
    "    #print(response)\n",
    "    #print(response.url)\n",
    "    redirected_to = response.url\n",
    "    parsed_uri = urlparse(redirected_to)  # returns six components\n",
    "    #print(parsed_uri)\n",
    "    domain = parsed_uri.netloc\n",
    "    result = domain.replace('www.', '')  # as per your case\n",
    "    #print(domain)\n",
    "    base_url = parsed_uri.scheme + \"://\" + parsed_uri.netloc\n",
    "\n",
    "    #print(base_url)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_metadata(soup):\n",
    "    metadata = soup.find_all(\"meta\")\n",
    "    return metadata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exclude_metadata = {'nature':['viewport', 'msapplication-TileColor', 'msapplication-config',  'theme-color', \n",
    "                    'application-name', 'robots', 'access', 'WT.cg_s', 'WT.z_bandiera_abtest', 'WT.page_categorisation',\n",
    "                    'WT.template', 'WT.z_cg_type', 'WT.cg_n', 'dc.rights', 'prism.issn'],'springer':['viewport', \n",
    "                    'msapplication-TileColor', 'msapplication-config',  'theme-color', \n",
    "                    'application-name', 'robots', 'access', 'WT.cg_s', 'WT.z_bandiera_abtest', 'WT.page_categorisation',\n",
    "                    'WT.template', 'WT.z_cg_type', 'WT.cg_n', 'dc.rights', 'prism.issn'],\"wiley\":[], 'rsc':['viewport',\n",
    "                    'format-detection', 'msapplication-TileColor', 'theme-color', 'dc.domain','twitter:card',\n",
    "                    'twitter:site'],\"acs\":['pbContext','viewport','robots','twitter:description','pb-robots-disabled',\n",
    "                    'twitter:card','twitter:site','twitter:image','twitter:title','google-site-verification'],'elsevier':\n",
    "                    []}\n",
    "metadata = soup.find_all(\"meta\",{\"name\":True})\n",
    "\n",
    "publisher = 'elsevier'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_ds_details(soup):\n",
    "    details = soup.find_all(\"p\",{'class':\"details\"} )#,{\"name\":\"citation_fulltext_html_url\"})\n",
    "    rec = {}\n",
    "    for counter, detail in enumerate(details):\n",
    "        if 'Abstract' in  detail.text.strip():\n",
    "            rec['abstract'] = detail.text.replace('Abstract:', \"\").strip()\n",
    "        if 'Public release date' in  detail.text.strip():\n",
    "            date_time_str = detail.text.replace('Public release date:', \"\").strip()\n",
    "            date_time_obj = datetime.strptime(date_time_str, '%d %B %Y')\n",
    "            rec['release_date'] = str(date_time_obj)\n",
    "        if 'Date of Experiment' in  detail.text.strip():\n",
    "            date_time_str = detail.text.replace('Date of Experiment:', \"\").strip()\n",
    "            date_time_obj = datetime.strptime(date_time_str, '%d %B %Y')\n",
    "            rec['experiment_date'] = str(date_time_obj)\n",
    "        if 'Principal Investigator' in  detail.text.strip():\n",
    "            rec['experiment_users'] = {}\n",
    "            people_list = detail.text.strip().split(\"\\n\")\n",
    "            for u_num, person in enumerate(people_list):\n",
    "                rec['experiment_users'][person.split(':')[0].replace(' ','_')+\"_\" + str(u_num)] = person.split(':')[1]\n",
    "        if 'Publisher' in  detail.text.strip():\n",
    "            rec['publisher'] = detail.text.replace('Publisher:', \"\").strip()\n",
    "        if 'Data format' in  detail.text.strip():\n",
    "            rec['data_format'] = detail.text.replace('Data format:', \"\").strip().split('\\n')[0]\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'ISISDataRepository.csv'\n",
    "id_field = 'num'\n",
    "ds_start, headings = csv_rw.get_csv_data(input_file, id_field)\n",
    "ds_end={}\n",
    "for entry in processed:\n",
    "    ds_end[entry] = ds_start[entry]\n",
    "    if processed[entry]['DOI'].strip() != \"\":\n",
    "        #print (entry, processed[entry]['DOI'])\n",
    "        html_parsed = get_html_parsed(processed[entry]['DOI'])\n",
    "        #metadata = get_metadata(html_parsed)\n",
    "        #for md in metadata:\n",
    "        #    print(md)\n",
    "        ds_detail = get_ds_details(html_parsed)\n",
    "        #print(ds_detail)\n",
    "        ds_end[entry] = ds_start[entry]|ds_detail\n",
    "#print( ds_end)\n",
    "\n",
    "if len(ds_end) > 0:\n",
    "    csv_rw.write_csv_data(ds_end, 'ISISDataRepositoryDetails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'num': '1',\n",
       "  'Title': 'Empty RI catalysis cell in RAL F3',\n",
       "  'Experiment': '3229 - POLARIS',\n",
       "  'RB Number': '0',\n",
       "  'DOI': ' ',\n",
       "  'Size': '2.35 MB',\n",
       "  'Instrument': 'POLARIS',\n",
       "  'Start Date': '1998-04-06',\n",
       "  'End Date': '2003-07-10',\n",
       "  'type': '',\n",
       "  'object': 'investigation',\n",
       "  'created': '',\n",
       "  'modified': '',\n",
       "  'location': ''},\n",
       " 2: {'num': '2',\n",
       "  'Title': 'Water structure around surfactant micelles at low pH',\n",
       "  'Experiment': '1 - SANDALS',\n",
       "  'RB Number': '720060',\n",
       "  'DOI': ' ',\n",
       "  'Size': '0 B',\n",
       "  'Instrument': 'SANDALS',\n",
       "  'Start Date': '2006-04-21',\n",
       "  'End Date': '',\n",
       "  'type': '',\n",
       "  'object': 'investigation',\n",
       "  'created': '',\n",
       "  'modified': '',\n",
       "  'location': ''},\n",
       " 3: {'num': '3',\n",
       "  'Title': 'Effect of Aging Temperature on Inorganic Films Templated by Polymer-Surfactant Complexes',\n",
       "  'Experiment': '1 - SURF',\n",
       "  'RB Number': '910125',\n",
       "  'DOI': '10.5286/ISIS.E.24067306',\n",
       "  'Size': '6.32 MB',\n",
       "  'Instrument': 'SURF',\n",
       "  'Start Date': '2009-03-21',\n",
       "  'End Date': '2009-07-28',\n",
       "  'type': '',\n",
       "  'object': 'investigation',\n",
       "  'created': '',\n",
       "  'modified': '',\n",
       "  'location': '',\n",
       "  'abstract': 'Recently we have been investigating the mineralization of polymer-surfactant films to create thicker and more robust self-supporting membranes. Our previous experiments used the cationic surfactant-polyethylenimine system, and also mixtures of cationic and anionic surfactants with water soluble polymers. During SAXS studies on dried films we noticed that aging the polymer-surfactant template solutions at different temperatures has a marked effect on the size and type of nanostructure formed in the films. We therefore wish to investigate the structures in these films at the air-solution interface and to use selective deuteration of the surfactants to determine whether the thermal history affects the molar ratio of the two species in the films. We will study both silica and titania films since these have a wider range of potential applications in catalysis and photoelectrochemical devices.',\n",
       "  'release_date': '2012-07-28 00:00:00',\n",
       "  'experiment_users': {'Principal_Investigator_0': ' Professor Karen Edler',\n",
       "   'Experimenter_1': ' Dr Bin Yang',\n",
       "   'Experimenter_2': ' Mr Matthew Wasbrough',\n",
       "   'Experimenter_3': ' Mr Robben Jaber',\n",
       "   'Experimenter_4': ' Mr Jim Holdaway'},\n",
       "  'experiment_date': '2009-03-21 00:00:00',\n",
       "  'publisher': 'STFC ISIS Neutron and Muon Source',\n",
       "  'data_format': 'RAW/Nexus'}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
