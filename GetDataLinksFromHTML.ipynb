{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data references from html pages\n",
    "\n",
    "A list of publications is obtainded from the app database. This list will contain a titles, IDs and DOIs which need to be explored to look for asociated data (suplementary data, raw data, processed data).\n",
    "\n",
    "The steps of the process are:\n",
    "\n",
    "1. get a Title, DOI, and URL for each publication \n",
    "2. get the DOI landing page and see if it contains references to data \n",
    "3. add a new dataset entry each time a new ds is found \n",
    "4. link the dataset to the publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "\n",
    "# read and write csv files\n",
    "import lib.handle_csv as csv_rw\n",
    "\n",
    "# Parsing html \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# http requests \n",
    "import requests\n",
    "\n",
    "# url parser\n",
    "from urllib.parse import urlparse  # python 3.x\n",
    "\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "# library for using regular expressions\n",
    "import re\n",
    "\n",
    "\n",
    "# values for metadata class names to exclude\n",
    "exclude_metadata = {'nature':['viewport', 'msapplication-TileColor', 'msapplication-config', 'theme-color', \n",
    "                    'application-name', 'robots', 'access', 'WT.cg_s', 'WT.z_bandiera_abtest', 'WT.page_categorisation',\n",
    "                    'WT.template', 'WT.z_cg_type', 'WT.cg_n', 'dc.rights', 'prism.issn'],'springer':['viewport', 'msapplication-TileColor', 'msapplication-config',  'theme-color', \n",
    "                    'application-name', 'robots', 'access', 'WT.cg_s', 'WT.z_bandiera_abtest', 'WT.page_categorisation',\n",
    "                    'WT.template', 'WT.z_cg_type', 'WT.cg_n', 'dc.rights', 'prism.issn'],\"wiley\":[],'rsc':['viewport',\n",
    "                    'format-detection', 'msapplication-TileColor', 'theme-color', 'dc.domain', 'twitter:card',\n",
    "                    'twitter:site'], \"acs\":['pbContext','viewport','robots','twitter:description','pb-robots-disabled',\n",
    "                    'twitter:card','twitter:site','twitter:image','twitter:title','google-site-verification']}\n",
    "\n",
    "# values for section labels which may contain references to data\n",
    "section_labels = {'nature':{'aria-labelledby':'data-availability'},'springer':{'aria-labelledby':'data-availability'}}\n",
    "\n",
    "# values for div which may contain references to data\n",
    "div_filters = {'nature':{'class':'c-article-supplementary__item'}, 'springer':{'class':\"c-article-supplementary__item\"}}\n",
    "\n",
    "#  Custom functions to get references to datasets\n",
    "# returns beautifulsoup object from given url\n",
    "def get_content(url):\n",
    "    html_soup = None\n",
    "    try:\n",
    "        req_head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n",
    "                    (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'}\n",
    "        response = requests.get(url, headers = req_head)\n",
    "        redirected_to = response.url\n",
    "        html_soup = BeautifulSoup(response.text,'html.parser')       \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return html_soup, redirected_to\n",
    "\n",
    "# get metadata\n",
    "def get_metadata(soup, journal):\n",
    "    result=[]\n",
    "    try:\n",
    "        metadata = soup.find_all('meta')\n",
    "        ignore_these = []\n",
    "        if journal in exclude_metadata:\n",
    "            ignore_these = exclude_metadata[journal] \n",
    "        else:\n",
    "            print('new journal')\n",
    "        for md_item in metadata:\n",
    "            if md_item.has_attr(\"name\") and not md_item[\"name\"] in ignore_these :\n",
    "                result.append(md_item)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return result\n",
    "\n",
    "# get data ref from metadata\n",
    "def get_data_from_metadata(soup, journal = 'nature', data_refs = []):\n",
    "    data_refs = []\n",
    "    res = get_metadata(soup, journal)\n",
    "    # check if metadata references supporting data or supplementary data\n",
    "    for md_item in res:\n",
    "        if 'data' in str(md_item[\"name\"]).lower():\n",
    "            #print(md_item[\"name\"], md_item[\"content\"])\n",
    "            ret_data = md_item[\"content\"]\n",
    "            data_refs.append({'type':\"metadata\", \"name\":md_item[\"name\"], 'data_url':md_item[\"content\"]})\n",
    "    # get author(s) data from metadata\n",
    "    #for md_item in res:\n",
    "    #    if 'author' in str(md_item[\"name\"]).lower():\n",
    "    #        print(md_item[\"name\"], md_item[\"content\"])    \n",
    "    return data_refs\n",
    "\n",
    "def get_data_from_section(soup, journal = 'nature', data_refs = [], base_url=\"\"):\n",
    "    inspect_these = {}\n",
    "    if journal in section_labels:\n",
    "        inspect_these = section_labels[journal]\n",
    "    for sec_filter in inspect_these:\n",
    "        sections = soup.find_all('section', {sec_filter:inspect_these[sec_filter]})\n",
    "        for section in sections:\n",
    "            pars = section.find_all('p')\n",
    "            for par in pars:\n",
    "                references = par.find_all('a')\n",
    "                if len(references) == 0:\n",
    "                   data_refs.append({'type':inspect_these[sec_filter], \"name\":par.contents[0], 'data_url':None}) \n",
    "                for a_ref in references:\n",
    "                    content_text = a_ref.contents[0]\n",
    "                    data_url = a_ref['href']\n",
    "                    if data_url[0] == '/' and base_url != \"\":\n",
    "                        data_url = base_url + data_url\n",
    "                    data_refs.append({'type':inspect_these[sec_filter], \"name\":a_ref.contents[0], 'data_url':data_url})\n",
    "    return data_refs\n",
    "\n",
    "def get_data_from_divs(soup, journal = 'nature', data_refs = [], base_url=\"\"):\n",
    "    inspect_these = {}\n",
    "    if journal in div_filters:\n",
    "        inspect_these = div_filters[journal]\n",
    "    for div_filter in inspect_these:\n",
    "        divs = soup.find_all('div',{div_filter:inspect_these[div_filter]})\n",
    "        for div in divs:\n",
    "            a_ref =  div.find('a')\n",
    "            content_text = a_ref.contents[0]\n",
    "            data_url = a_ref['href']\n",
    "            if data_url[0] == '/' and base_url != \"\":\n",
    "                data_url = base_url + data_url\n",
    "            data_refs.append({'type':\"supplementary\", \"name\":a_ref.contents[0], 'data_url':data_url})\n",
    "    return data_refs\n",
    "\n",
    "\n",
    "# Wiley online stores supplementary in tables on the article page. \n",
    "table_filters={'wiley':{\"class\":\"support-info__table\"}}\n",
    "def get_data_from_tables(soup, journal = 'wiley', data_refs = [], base_url=\"\"):\n",
    "    inspect_these = {}\n",
    "    if journal in table_filters:\n",
    "        inspect_these = table_filters[journal]\n",
    "    for tbl_filter in inspect_these:\n",
    "        tables = soup.find_all('table',{tbl_filter:inspect_these[tbl_filter]})\n",
    "        for table in tables:\n",
    "            # find rows\n",
    "            trs = table.find_all('tr')\n",
    "            # get the type and link from each row\n",
    "            for tr in trs:\n",
    "                td_link = tr.find('td',{\"headers\":\"article-filename\"})\n",
    "                td_desc = tr.find('td',{\"headers\":\"article-description\"})\n",
    "                data_link = td_link.find('a')\n",
    "                data_url = data_link['href']\n",
    "                if data_url[0] == '/' and base_url != \"\":\n",
    "                    data_url = base_url + data_url\n",
    "                data_refs.append({'type':td_desc.contents[0], \"name\":data_link.contents[0], 'data_url':data_url})\n",
    "    return data_refs\n",
    "\n",
    "# extract from anchor in text publications\n",
    "a_filters={'rsc':{\"class\":\"list__item-link\"}, 'acs':{\"class\":\"suppl-anchor\"}}\n",
    "def get_data_from_anchor(soup, journal = 'rsc', data_refs = [], base_url=\"\"):\n",
    "    # find line for supplementary\n",
    "    if journal in a_filters:\n",
    "        inspect_these = a_filters[journal]\n",
    "    for a_filter in inspect_these:\n",
    "        supp_h2_line = -1\n",
    "        inspect_heads = soup.find_all(\"h2\")\n",
    "        for a_head in inspect_heads:\n",
    "            for content in a_head.contents:\n",
    "                if content != None and \"supplementary\" in str(content).lower() :\n",
    "                    supp_h2_line = a_head.sourceline\n",
    "\n",
    "        # Use the position of \"header line\" as offset to look for data links\n",
    "        links = soup.find_all(\"a\", {a_filter:inspect_these[a_filter]})\n",
    "        for link in links:\n",
    "            dt_type = dt_link = dt_name = \"\"\n",
    "            if link.sourceline > supp_h2_line:\n",
    "                #print(link, \"\\nLine: \", link.sourceline)\n",
    "                if journal == 'rsc':\n",
    "                    l_spans = link.find_all(\"span\",{\"class\":\"list__item-label\"})\n",
    "                    for a_span in l_spans:\n",
    "                        for contnt in a_span.contents:\n",
    "                            if 'supplementary' in str(contnt).lower():\n",
    "                                dt_link = link['href']\n",
    "                                dt_name = str(contnt).strip()\n",
    "                                #print('supplementary', link['href'], str(contnt).strip())\n",
    "                            if str(type(contnt)) == \"<class 'bs4.element.Tag'>\":\n",
    "                                #print(contnt.contents[0])\n",
    "                                dt_name += contnt.contents[0]\n",
    "                            #print(str(type(contnt)), str(contnt).strip())\n",
    "                elif journal == 'acs':\n",
    "                    dt_link = link['href']\n",
    "                    dt_name = str(link.contents[0]).strip()\n",
    "            if dt_link != \"\" and dt_name != \"\":\n",
    "                if dt_link[0] == '/' and base_url != \"\":\n",
    "                    dt_link = base_url + dt_link\n",
    "                data_refs.append({'type':'supplementary',\"name\":dt_name, 'data_url':dt_link})\n",
    "    return data_refs\n",
    "\n",
    "# get full doc from rsc landig page\n",
    "def get_full_html_doc(soup):\n",
    "    # check if full html text is available\n",
    "    more_soup = anoter_url = None\n",
    "    metadata = soup.find_all(\"meta\",{\"name\":\"citation_fulltext_html_url\"})\n",
    "    if len(metadata)> 0:\n",
    "        more_soup, anoter_url = get_content(metadata[0]['content'])\n",
    "    return more_soup, anoter_url\n",
    "\n",
    "# verify if statement refers to supporting data\n",
    "def is_data_stmt(statement=\"\"):\n",
    "    support_keys = [\"data\", \"underpin\", \"support\", \"result\", \"found\", \"find\", \"obtain\", \"doi\",\"raw\", \"information\"\n",
    "                    \"provide\", \"availabe\", \"online\"]\n",
    "    count = 0\n",
    "    for a_word in support_keys:\n",
    "        if a_word in statement:\n",
    "            count += 1\n",
    "    if count > 2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# get data references from full html doc\n",
    "def get_data_from_html_doc(soup, journal = 'rsc', data_refs = [], base_url=\"\"):\n",
    "    if journal == 'rsc':\n",
    "        # rsc lists the link to full html document in metadata\n",
    "        more_soup, another_url  = get_full_html_doc(soup)\n",
    "        if more_soup != None and another_url != None:\n",
    "            base_url = get_base_url(another_url)\n",
    "            soup = more_soup\n",
    "    tag_targets = ['p', 'span']\n",
    "    for tag_name in tag_targets:\n",
    "        paras = soup.find_all(tag_name)\n",
    "        for para in paras:\n",
    "            for cont_para in para.contents:\n",
    "                content = str(cont_para).lower()\n",
    "                if 'data' in content:\n",
    "                    intresting = \"\"\n",
    "                    if 'data' in content[content.rfind(\".\")+2:]:\n",
    "                        intresting = content[content.rfind(\".\")+2:]\n",
    "                    else:\n",
    "                        intresting = content[:content.rfind(\".\")]\n",
    "                    anchor_refs = para.find_all('a')\n",
    "                    if len(anchor_refs)>0 and is_data_stmt(intresting):\n",
    "                        for a_ref in anchor_refs:\n",
    "                            dt_link = a_ref['href']\n",
    "                            dt_name = str(a_ref.contents[0])\n",
    "                            if dt_link != \"\" and dt_name != \"\" and dt_link[0] != \"#\":\n",
    "                                if dt_link[0] == '/' and base_url != \"\":\n",
    "                                    dt_link = base_url + dt_link \n",
    "                                data_refs.append({'type':'supporting',\"name\":dt_name, 'data_url':dt_link})\n",
    "    return data_refs\n",
    "\n",
    "# get a list of ids, titles, dois, links, pdf_file and \n",
    "# html_file names from the data database\n",
    "def get_pub_data(db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi, link, pdf_file, html_file\"\n",
    "    filter_str = \"status = 'Added'\"\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "# get a list of ids, titles, dois, links, pdf_file and \n",
    "# html_file names from the data database\n",
    "def get_pub_data(db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi, link, pdf_file, html_file\"\n",
    "    filter_str = \"status = 'Added'\"\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "\n",
    "def get_base_url(response_url):\n",
    "    parsed_uri = urlparse(response_url)  # returns six components\n",
    "    base_url = parsed_uri.scheme + \"://\" + parsed_uri.netloc\n",
    "    return base_url\n",
    "\n",
    "# use regular expression to check if a given string\n",
    "# is a valid DOI, using pattern from CR\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    if cr_doi == None:\n",
    "        return False\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scman1\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182b8f536de74cf686cf86af86f657c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=374.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "MISSING:  623 Asymmetric synthesis of primary amines catalyzed by thermotolerant fungal reductive aminases 10.1039/d0sc02253e [{\"URL\"=>\"http://pubs.rsc.org/en/content/articlepdf/2020/SC/D0SC02253E\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]\n",
      "(623, '10.1039/d0sc02253e', 'Asymmetric synthesis of primary amines catalyzed by thermotolerant fungal reductive aminases', 2020, 'article-journal', 'Royal Society of Chemistry (RSC)', 'Chemical Science', '11', '19', '5052-5057', 2020, 5, 20, 2020, None, None, '[{\"URL\"=>\"http://creativecommons.org/licenses/by-nc/3.0/\", \"start\"=>{\"date-parts\"=>[[2020, 5, 5]], \"date-time\"=>\"2020-05-05T00:00:00Z\", \"timestamp\"=>1588636800000}, \"delay-in-days\"=>125, \"content-version\"=>\"vor\"}]', 8, '[{\"URL\"=>\"http://pubs.rsc.org/en/content/articlepdf/2020/SC/D0SC02253E\", \"content-type\"=>\"unspecified\", \"content-version\"=>\"vor\", \"intended-application\"=>\"similarity-checking\"}]', 'http://dx.doi.org/10.1039/d0sc02253e', '<p>Fungal reductive aminases as effective biocatalysts for the preparation of chiral primary amines.</p>', 'Added', '', '2020-11-30 16:00:19.063940', '2021-06-01 16:10:29.479347', 37, '19')\n",
      "***************\n",
      "MISSING:  649 Combined computational and neutron scattering studies of hydrocarbons confined in mesoporous materials  https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.808495\n",
      "(649, '', 'Combined computational and neutron scattering studies of hydrocarbons confined in mesoporous materials', 2020, 'phd-thesis', \"Queen's University Belfast\", 'Doctoral Thesis', '', '', '', 2020, None, None, 2020, None, None, '', None, 'https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.808495', 'https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.808495', 'Heterogeneous catalyst materials, in particular supported metal nanoparticles, are hugely important in industrial processes such as emissions control technology. In order to design new catalyst materials with improved catalytic performance, the catalytic properties and mechanistic pathways must be well understood. The work presented in this thesis makes advances to operando spectroscopy methods for the characterisation of industrial heterogeneous catalyst materials operating in realistic reaction conditions. A multi-technique approach, employing XAFS and DRIFTS simultaneously, has been used to probe the electronic and structural properties of metal nanoparticles as well as the molecular vibrations of reactants and intermediates at the catalyst surface. In this work, the preparation of supported metal nanoparticle catalysts have been investigated using time-resolved XAFS and DRIFTS spectroscopy approach. Structure-activity relationships of the supported Pd nanoparticle catalysts have been identified during their operation for an important reaction used in diesel after-treatment technology; the selective catalytic oxidation of NH3. The selectivity of the reaction towards the different reaction products (N2, N2O and NO), within the relevant temperature window for industrial application, has been linked to the different structural phases of Pd, including a previously unidentified PdNx species. The major challenges for operando measurements, concerning improved time resolution and spatial resolution, have been addressed with the development of a new reactor for combined XAFS and DRIFTS of a catalyst bed operating in plug-flow reaction conditions. The advantage of this reactor design for operando characterisation has been demonstrated in the investigation of a supported Pd nanoparticle catalyst (Pd/γ-Al2O3) during oscillating CO oxidation. This reactor improves on previous designs by allowing spectroscopic measurements to be performed with spatial resolution along the axial length of a catalyst bed. In this way, there is scope for elucidating the structure-function relationships of many other heterogeneous catalysts.', 'Added', 'uk.bl.ethos.808495', '2020-12-02 13:50:56.545192', '2020-12-18 11:36:00.490182', None, None)\n",
      "***************\n",
      "MISSING:  650 Improving Synchrotron Methods for Advanced Characterisation of Heterogeneous Catalysts  https://discovery.ucl.ac.uk/id/eprint/10072765/\n",
      "(650, '', 'Improving Synchrotron Methods for Advanced Characterisation of Heterogeneous Catalysts', 2019, 'phd-thesis', 'University College London', 'Doctoral Thesis', '', '', '', 2019, None, None, 2019, None, None, '', None, 'https://discovery.ucl.ac.uk/id/eprint/10072765/', 'https://discovery.ucl.ac.uk/id/eprint/10072765/', 'Heterogeneous catalyst materials, in particular supported metal nanoparticles, are hugely important in industrial processes such as emissions control technology. In order to design new catalyst materials with improved catalytic performance, the catalytic properties and mechanistic pathways must be well understood. The work presented in this thesis makes advances to operando spectroscopy methods for the characterisation of industrial heterogeneous catalyst materials operating in realistic reaction conditions. A multi-technique approach, employing XAFS and DRIFTS simultaneously, has been used to probe the electronic and structural properties of metal nanoparticles as well as the molecular vibrations of reactants and intermediates at the catalyst surface. In this work, the preparation of supported metal nanoparticle catalysts have been investigated using time-resolved XAFS and DRIFTS spectroscopy approach. Structure-activity relationships of the supported Pd nanoparticle catalysts have been identified during their operation for an important reaction used in diesel after-treatment technology; the selective catalytic oxidation of NH3. The selectivity of the reaction towards the different reaction products (N2, N2O and NO), within the relevant temperature window for industrial application, has been linked to the different structural phases of Pd, including a previously unidentified PdNx species. The major challenges for operando measurements, concerning improved time resolution and spatial resolution, have been addressed with the development of a new reactor for combined XAFS and DRIFTS of a catalyst bed operating in plug-flow reaction conditions. The advantage of this reactor design for operando characterisation has been demonstrated in the investigation of a supported Pd nanoparticle catalyst (Pd/γ-Al2O3) during oscillating CO oxidation. This reactor improves on previous designs by allowing spectroscopic measurements to be performed with spatial resolution along the axial length of a catalyst bed. In this way, there is scope for elucidating the structure-function relationships of many other heterogeneous catalysts.', 'Added', 'https://discovery.ucl.ac.uk/id/eprint/10072765', '2020-12-02 14:06:53.530318', '2020-12-18 11:36:29.343449', None, None)\n",
      "***************\n",
      "MISSING:  651 Metal oxide preparation for heterogeneous catalysis   http://orca.cf.ac.uk/129474/\n",
      "(651, '', 'Metal oxide preparation for heterogeneous catalysis ', 2019, 'phd-thesis', ' Cardiff University', 'Doctoral Thesis', '', '', '', 2019, None, None, 2019, None, None, '', None, 'http://orca.cf.ac.uk/129474/', 'http://orca.cf.ac.uk/129474/', 'The preparation method of a heterogeneous catalyst is one of the most fundamental aspects that can determine its morphology, surface area, phases present, elemental mixing and of course ultimately its catalytic activity. Currently there are a large number of different ways of preparing metal oxide catalysts such as co-precipitation or sol gel but over the last 20-30 years there has been a large number of solvent systems that have been used to develop alternative synthesis techniques such as supercritical solvents, ionic liquids, deep eutectic solvents and switchable solvents. These systems contain interesting properties that are not found in conventional solvent systems which could be utilised to synthesise metal oxide or metal oxide precursors that have unique properties that gives them an advantage over the metal oxide catalysts prepared by conventional methods. The aim of this thesis was to investigate the potential for these novel systems and adapt them for the application of metal oxide catalyst preparation and to see how these techniques compare with more established methods. In order to assess these systems three catalytic reactions were chosen to use as a model to see how metal oxide catalyst prepared by these methods compare with methods such as co-precipitation or supercritical anti-solvent. 1) the use of Co3O4, Mn2O3 and Fe2O3 for the total oxidation of propane to CO2. 2) the use of copper-manganese oxide (hopcalite) for low temperature carbon monoxide oxidation and 3) Cu/ZnO catalyst for methanol synthesis from CO2 and H2. Cobalt oxalate, manganese oxalate and iron oxalate were prepared using choline chloride-oxalic acid based deep eutectic solvent with a water or water-alcohol anti-solvent. It was found the for cobalt and iron precursors a rod shape morphology could be achieved and this morphology was retained after calcination although the precipitated manganese did not form rods. Varying the anti-solvent mixture changed morphology and surface area of the cobalt oxide and iron oxide catalysts. Mixed cobalt manganese oxide and ii iron manganese oxide prepared using deep eutectic solvents were also shown to form rod like morphologies similar to the single cobalt oxide and iron oxide catalysts. These catalysts that were tested for propane total oxidation method and did show some variations in activity between the different preparation methods but there was no significant improvement over the reference catalysts. The use of hydrothermal synthesis to make a crednerite phase CuMnO2 as a precursor to the spinel phase copper-manganese oxide was found to produce spinel copper-manganese oxide catalysts with properties that differed from co-precipitated equivalents. These catalysts demonstrated lower deactivation during the first 30 minutes of CO oxidation despite having generally having lower a surface area, although these catalysts showed deactivation after temperature ramp to 50 °C. Characterisation on the crednerite derived spinel showed that they differed from the regular co-precipitated hopcalite with XPS showing a higher Cu+:Cu2+ at lower temperature heat treatment which may indicate greater Cu-Mn integration. The use of a switchable solvent system was demonstrated for the preparation of carbonate precursors to copper manganese oxides CO oxidation catalysts which were shown to have high surface areas and excellent CO conversion comparable to copper-manganese oxide catalysts prepared by supercritical anti-solvent methods, presenting a less energy intensive method of making metal oxide catalysts to supercritical anti-solvent precipitation. The use of choline chloride-urea deep eutectic solvents to prepare copper-zinc oxide methanol synthesis catalysts was shown to be an ineffective method, with the MP-AES showing loss of zinc at higher copper loadings and XPS showing large amounts of surface chlorine present after calcination resulting in inactive catalysts. An initial study using switchable solvents to prepare Cu/ZnO catalysts was shown to produce catalysts that were active for methanol synthesis and presents a promising potential for future development.', 'Added', 'http://orca.cf.ac.uk/id/eprint/129474', '2020-12-02 13:50:56.545192', '2020-12-18 11:36:46.500098', None, None)\n",
      "\n",
      "[(0, 'id', 'integer', 1, None, 1), (1, 'doi', 'varchar', 0, 'NULL', 0), (2, 'title', 'varchar', 0, 'NULL', 0), (3, 'pub_year', 'integer', 0, 'NULL', 0), (4, 'pub_type', 'varchar', 0, 'NULL', 0), (5, 'publisher', 'varchar', 0, 'NULL', 0), (6, 'container_title', 'varchar', 0, 'NULL', 0), (7, 'volume', 'varchar', 0, 'NULL', 0), (8, 'issue', 'varchar', 0, 'NULL', 0), (9, 'page', 'varchar', 0, 'NULL', 0), (10, 'pub_print_year', 'integer', 0, 'NULL', 0), (11, 'pub_print_month', 'integer', 0, 'NULL', 0), (12, 'pub_print_day', 'integer', 0, 'NULL', 0), (13, 'pub_ol_year', 'integer', 0, 'NULL', 0), (14, 'pub_ol_month', 'integer', 0, 'NULL', 0), (15, 'pub_ol_day', 'integer', 0, 'NULL', 0), (16, 'license', 'varchar', 0, 'NULL', 0), (17, 'referenced_by_count', 'integer', 0, 'NULL', 0), (18, 'link', 'varchar', 0, 'NULL', 0), (19, 'url', 'varchar', 0, 'NULL', 0), (20, 'abstract', 'varchar', 0, 'NULL', 0), (21, 'status', 'varchar', 0, 'NULL', 0), (22, 'comment', 'varchar', 0, 'NULL', 0), (23, 'created_at', 'datetime', 1, None, 0), (24, 'updated_at', 'datetime', 1, None, 0), (25, 'references_count', 'integer', 0, 'NULL', 0), (26, 'journal_issue', 'varchar', 0, 'NULL', 0)]\n",
      "['id', 'doi', 'title', 'pub_year', 'pub_type', 'publisher', 'container_title', 'volume', 'issue', 'page', 'pub_print_year', 'pub_print_month', 'pub_print_day', 'pub_ol_year', 'pub_ol_month', 'pub_ol_day', 'license', 'referenced_by_count', 'link', 'url', 'abstract', 'status', 'comment', 'created_at', 'updated_at', 'references_count', 'journal_issue']\n"
     ]
    }
   ],
   "source": [
    "# import custom functions (common to various notebooks)\n",
    "import processing_functions as pr_fns\n",
    "\n",
    "\n",
    "# get the publications list from the app database\n",
    "ukchapp_db = \"db_files/app_db20210601.sqlite3\"\n",
    "ukchdata_db = \"db_files/data_pubs.sqlite3\"\n",
    "db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "data_conn = dbh.DataBaseAdapter(ukchdata_db)\n",
    "pubs_data = pr_fns.get_pub_data(ukchdata_db)\n",
    "pub_fields = db_conn.get_table_info(\"articles\")\n",
    "pub_fieldnames = [el[1] for el in pub_fields]\n",
    "pubs_metadata = pr_fns.get_pub_app_data(ukchapp_db)\n",
    "for a_pub in tqdm_notebook(pubs_metadata):\n",
    "    pub_id = a_pub[0]\n",
    "    pub_title = a_pub[1]\n",
    "    pub_doi = a_pub[2]\n",
    "    pub_url = a_pub[3]\n",
    "    data_id = 0\n",
    "    for a_data in pubs_data:\n",
    "        if a_data[0] == pub_id: \n",
    "            data_id = a_data[0]\n",
    "            break \n",
    "    if data_id == 0:\n",
    "        print(\"***************\\nMISSING: \",pub_id,pub_title,pub_doi,pub_url)\n",
    "        an_article = db_conn.get_row( \"articles\", pub_id)[0]\n",
    "        print(an_article)\n",
    "    \n",
    "        #data_conn.put_values_table('articles',pub_fieldnames[1:],an_article[1:])\n",
    "    \n",
    "print(pub_fields)\n",
    "print(pub_fieldnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such column: pdf_file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-31c7b385896b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mukchapp_db\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"db_files/app_db20210601.sqlite3\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mukchdata_db\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"db_files/data_pubs.sqlite3\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdb_pubs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pub_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mukchapp_db\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# get the list of dois already mined for data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-f89ce7df2607>\u001b[0m in \u001b[0;36mget_pub_data\u001b[1;34m(db_name)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0mfields_required\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"id, title, doi, link, pdf_file, html_file\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[0mfilter_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"status = 'Added'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m     \u001b[0mdb_titles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields_required\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m     \u001b[0mdb_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdb_titles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Harwell\\examples\\validatearticledata\\lib\\handle_db.py\u001b[0m in \u001b[0;36mget_values\u001b[1;34m(self, table, field, filter_str)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mstr_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"SELECT %s FROM %s WHERE %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mvalue_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_query\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalue_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such column: pdf_file"
     ]
    }
   ],
   "source": [
    "# get the publications list from the app database\n",
    "ukchapp_db = \"db_files/app_db20210601.sqlite3\"\n",
    "ukchdata_db = \"db_files/data_pubs.sqlite3\"\n",
    "db_pubs = get_pub_data(ukchapp_db)\n",
    "\n",
    "# get the list of dois already mined for data \n",
    "input_file = 'pub_data_add202105.csv'\n",
    "id_field = 'num'\n",
    "processed, headings = csv_rw.get_csv_data(input_file, id_field)\n",
    "processed_dois = []\n",
    "for entry in processed:\n",
    "    if not processed[entry]['doi'] in processed_dois:\n",
    "        processed_dois.append( processed[entry]['doi'])\n",
    "\n",
    "data_records = {}\n",
    "ref_count = 1\n",
    "for a_pub in tqdm_notebook(db_pubs):\n",
    "    if a_pub[0] > 616: # only check new publications added after 616\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        pub_html = a_pub[5]\n",
    "        publishers = ['acs', \"wiley\", \"springer\", \"rsc\", 'nature','elsevier']\n",
    "        if not pub_doi in processed_dois and valid_doi(pub_doi):\n",
    "            # use doi reference to get landing page\n",
    "            url = \"http://dx.doi.org/\" + pub_doi\n",
    "            doc_content, response_url = get_content(url)\n",
    "            base_url = get_base_url(response_url)\n",
    "            publisher = 'another_pub'\n",
    "            for pb_name in publishers:\n",
    "                if pb_name in base_url:\n",
    "                    publisher = pb_name\n",
    "            print(pub_id, \"Title: \", pub_title, \" look up: \", base_url, \" publisher:\", publisher)\n",
    "            res = []\n",
    "            if publisher in ['springer', 'nature']:\n",
    "                res = get_data_from_metadata(doc_content, publisher, res)\n",
    "                res = get_data_from_section(doc_content, publisher, res, base_url)\n",
    "                res = get_data_from_divs(doc_content, publisher, res, base_url)\n",
    "            if publisher in ['wiley']:\n",
    "                res = get_data_from_tables(doc_content, publisher, res, base_url)\n",
    "            if publisher in ['rsc','acs']:\n",
    "                res = get_data_from_anchor(doc_content, publisher, res, base_url)\n",
    "                res = get_data_from_html_doc(doc_content, publisher, res, base_url)\n",
    "            if res != []:\n",
    "                for data_ref in res:\n",
    "                    #print(data_ref)\n",
    "                    data_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                    data_record.update(data_ref)\n",
    "                    data_records[ref_count] = data_record\n",
    "                    ref_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(processed_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data_records) > 0:\n",
    "    csv_rw.write_csv_data(data_records, 'pub_data_add202012.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_pubs = []\n",
    "for a_pub in db_pubs:\n",
    "    if a_pub[0] > 616: # only check new publications added after 616\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        pub_html = a_pub[5]\n",
    "        if \"acs\" in str(pub_url).lower():\n",
    "            data_found = False\n",
    "            for dr in data_records:\n",
    "                if pub_id == data_records[dr]['id']:\n",
    "                    data_found = True\n",
    "                    break\n",
    "            if not data_found:\n",
    "                url = \"http://dx.doi.org/\" + pub_doi\n",
    "                print(\"missing: \", pub_id, pub_doi, url)\n",
    "                missing_pubs.append(a_pub)\n",
    "    len(missing_pubs)\n",
    "print(missing_pubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://doi.org/10.1016/j.apcata.2018.10.010'\n",
    "req_head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'}\n",
    "response = requests.get(url, headers = req_head)\n",
    "\n",
    "response = requests.get(response.url, headers = req_head)\n",
    "print(response)\n",
    "print(response.url)\n",
    "redirected_to = response.url\n",
    "parsed_uri = urlparse(redirected_to)  # returns six components\n",
    "print(parsed_uri)\n",
    "domain = parsed_uri.netloc\n",
    "result = domain.replace('www.', '')  # as per your case\n",
    "print(domain)\n",
    "base_url = parsed_uri.scheme + \"://\" + parsed_uri.netloc\n",
    "\n",
    "print(base_url)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "# check if full html text is available\n",
    "metadata = soup.find_all(\"meta\")#,{\"name\":\"citation_fulltext_html_url\"})\n",
    "for meta in metadata:\n",
    "  print(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exclude_metadata = {'nature':['viewport', 'msapplication-TileColor', 'msapplication-config',  'theme-color', \n",
    "                    'application-name', 'robots', 'access', 'WT.cg_s', 'WT.z_bandiera_abtest', 'WT.page_categorisation',\n",
    "                    'WT.template', 'WT.z_cg_type', 'WT.cg_n', 'dc.rights', 'prism.issn'],'springer':['viewport', \n",
    "                    'msapplication-TileColor', 'msapplication-config',  'theme-color', \n",
    "                    'application-name', 'robots', 'access', 'WT.cg_s', 'WT.z_bandiera_abtest', 'WT.page_categorisation',\n",
    "                    'WT.template', 'WT.z_cg_type', 'WT.cg_n', 'dc.rights', 'prism.issn'],\"wiley\":[], 'rsc':['viewport',\n",
    "                    'format-detection', 'msapplication-TileColor', 'theme-color', 'dc.domain','twitter:card',\n",
    "                    'twitter:site'],\"acs\":['pbContext','viewport','robots','twitter:description','pb-robots-disabled',\n",
    "                    'twitter:card','twitter:site','twitter:image','twitter:title','google-site-verification'],'elsevier':\n",
    "                    []}\n",
    "metadata = soup.find_all(\"meta\",{\"name\":True})\n",
    "\n",
    "publisher = 'elsevier'\n",
    "for md in metadata:\n",
    "    if not md['name'] in exclude_metadata[publisher]:\n",
    "        print(\"X:\", md['name'])\n",
    "        print(md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
