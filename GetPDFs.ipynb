{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get PDF Files for publications in the UK Catalysis Hub app db\n",
    "A list of publications is obtainded from the app database. This list will contain a titles, IDs and DOIs which need to be explored to look for the corresponding pdf files. \n",
    "The steps of the process are: \n",
    " 1. get a Title, DOI, and URL for each publication\n",
    " 2. get the DOI landing page and see if it contains a link to the pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n",
    "# library for handling http requests\n",
    "import requests\n",
    "\n",
    "# to iterate on lists\n",
    "import itertools as itools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Functions\n",
    "# ** will migrate to lib if needed for more than one notebook\n",
    "\n",
    "# get the crossreference json page from doi\n",
    "def get_cr_json_object(cr_doi):\n",
    "  crjd = None\n",
    "  doi_file = 'json_files/' + cr_doi.replace('/','_').lower() + '.json'\n",
    "  if not Path(doi_file).is_file():\n",
    "    crjd = cr_api.getBibData(cr_doi)\n",
    "    with open(doi_file, 'w', encoding='utf-8-sig', errors='ignore') as f:\n",
    "                json.dump(crjd, f, ensure_ascii=False, indent=4)\n",
    "  else:\n",
    "    with open(doi_file, 'r', encoding='utf-8-sig') as jf:\n",
    "        crjd = json.load(jf)\n",
    "  # return the content and the file name \n",
    "  return crjd, doi_file\n",
    "\n",
    "# get the landing page for the publication from uri\n",
    "def get_pub_html_doi(cr_doi):\n",
    "    html_file = 'html_files/' + cr_doi.replace('/','_').lower() + '.html'\n",
    "    if not Path(html_file).is_file():\n",
    "        page_content = urlh.getPageFromDOI(doi_text)\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(page_content.decode(\"utf-8\") )\n",
    "    else:\n",
    "        f = open(html_file, \"r\")\n",
    "        page_content = f.read()\n",
    "    return page_content, html_file\n",
    "\n",
    "# get a list of titles from the previous searches database\n",
    "def get_titles(str_pub_title, db_name = \"prev_search.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'prev_pop_searches'\n",
    "    fields_required = \"Num, Title\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "# get a list of ids, titles, and dois from the app database\n",
    "def get_titles_and_dois(str_pub_title, db_name = \"app_db.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "# get a list of ids, titles, dois, links, pdf_file and \n",
    "# html_file names from the app database\n",
    "def get_pub_app_data(db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi, link\"\n",
    "    filter_str = \"status = 'Added'\"\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    #print(db_titles)\n",
    "    search_in = 'pdf_files'\n",
    "    filter_str = \"id >= 1\"\n",
    "    fields_required = \"id, doi, pdf_file\"\n",
    "    db_pdfs = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    #print(db_pdfs)\n",
    "    return_list = []\n",
    "    zip_obj = itools.zip_longest(db_titles,db_pdfs)\n",
    "    for an_art, a_pdf in zip_obj:\n",
    "        if(a_pdf != None and an_art[0] == a_pdf[0]):\n",
    "            return_list.append(tuple(an_art + (a_pdf[2], None)))\n",
    "        else:\n",
    "            return_list.append(tuple(an_art + (None, None)))\n",
    "        \n",
    "    db_conn.close()\n",
    "    return return_list\n",
    "\n",
    "# get the current csv working file\n",
    "def get_working_file(nr_wf):\n",
    "    working_file = wf_fields = None\n",
    "    current_pass = 0\n",
    "    if Path(nr_wf).is_file():\n",
    "        working_file, wf_fields = csvh.get_csv_data(nr_wf,'Num')\n",
    "        for art_num in tqdm_notebook(working_file):\n",
    "            if 'ignore' in working_file[art_num].keys():\n",
    "                if current_pass < int(working_file[art_num]['ignore']):\n",
    "                    current_pass = int(working_file[art_num]['ignore'])\n",
    "            else:\n",
    "                break\n",
    "    print(\"Current pass:\", current_pass)\n",
    "    return working_file, wf_fields, current_pass\n",
    "\n",
    "# get an htm file saved locally in the html_file folder \n",
    "def get_pub_html_url(text_url, entry_id):\n",
    "    html_file = 'html_files/' +  entry_id + '.html'\n",
    "    if not Path(html_file).is_file():\n",
    "        print(\"\")\n",
    "        page_content = urlh.getPageFromURL(text_url)\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(page_content)\n",
    "    else:\n",
    "        f = open(html_file, \"r\")\n",
    "        page_content = f.read()\n",
    "    return page_content, html_file\n",
    "\n",
    "# use regular expression to check if a given string\n",
    "# is a valid DOI, using pattern from CR\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    if cr_doi == None:\n",
    "        return False\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# get a semicolon separated list of authors from CR json data\n",
    "def get_cr_author_list(article_data):\n",
    "    authors = []\n",
    "    if 'author' in article_data.keys():\n",
    "        for author in article_data['author']:\n",
    "            new_author=\"\"\n",
    "            new_author = author['family']\n",
    "            if 'given' in author.keys():\n",
    "                new_author += \", \" + author['given']\n",
    "            authors.append(new_author)\n",
    "    return (\"; \").join(authors)\n",
    "\n",
    "# get the publication year from CR json data\n",
    "def get_cr_year_published(article_data):\n",
    "    year_print = 0\n",
    "    if 'published-print' in article_data.keys() \\\n",
    "        and article_data['published-print'] != None \\\n",
    "        and article_data['published-print']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['published-print']['date-parts'][0][0])    \n",
    "    elif 'journal-issue' in article_data.keys() \\\n",
    "        and article_data['journal-issue'] != None \\\n",
    "        and 'published-print' in article_data['journal-issue'].keys() \\\n",
    "        and article_data['journal-issue']['published-print'] != None \\\n",
    "        and article_data['journal-issue']['published-print']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['journal-issue']['published-print']['date-parts'][0][0])\n",
    "    year_online = 0\n",
    "    if 'published-online' in article_data.keys() \\\n",
    "        and article_data['published-online'] != None \\\n",
    "        and article_data['published-online']['date-parts'][0] != None:\n",
    "        year_online = int(article_data['published-online']['date-parts'][0][0])    \n",
    "    elif 'journal-issue' in article_data.keys() \\\n",
    "        and article_data['journal-issue'] != None \\\n",
    "        and 'published-online' in article_data['journal-issue'].keys() \\\n",
    "        and article_data['journal-issue']['published-online'] != None \\\n",
    "        and article_data['journal-issue']['published-online']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['journal-issue']['published-online']['date-parts'][0][0])\n",
    "    if year_print != 0 and year_online != 0:\n",
    "        return year_print if year_print < year_online else year_online\n",
    "    else:\n",
    "        return year_print if year_online == 0 else year_online\n",
    "    return 0\n",
    "\n",
    "# try to download a pdf from a given url\n",
    "def get_pdf_from_url(pdf_url):\n",
    "    fname = \"\"\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        content_type = response.headers['content-type']\n",
    "        if not 'text' in content_type:\n",
    "            #print(response.headers)\n",
    "            cd= response.headers['content-disposition']\n",
    "            #print(cd)\n",
    "            fname = re.findall(\"filename=(.+)\", cd)[0]\n",
    "            #print(fname)\n",
    "            with open('pdf_files/'+ fname +'.pdf', 'wb') as f:\n",
    "                f.write(response.content)\n",
    "    except:\n",
    "        print(\"Error getting file from: \", pdf_url)\n",
    "    finally:\n",
    "        return fname\n",
    "# add name of the pdf file for a publication record in the app database     \n",
    "def set_pdf_file_value(file_name, pub_id, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    done = db_conn.set_value_table(table, pub_id, \"pdf_file\", file_name)\n",
    "    db_conn.close()\n",
    "    return done\n",
    "\n",
    "# try to get a pdf from elsevier\n",
    "def get_elsevier_pdf(doi):\n",
    "    pdf_url = f'http://api.elsevier.com/content/article/doi:{doi}?view=FULL'\n",
    "    print(\"\\t\", pdf_url) \n",
    "    return get_pdf_from_url(pdf_url)\n",
    "\n",
    "# try to get a pdf from wiley\n",
    "def get_wiley_pdf(doi):\n",
    "    pdf_url = f'https://onlinelibrary.wiley.com/doi/pdf/{doi}'\n",
    "    print(\"\\t\", pdf_url) \n",
    "    return get_pdf_from_url(pdf_url)\n",
    "\n",
    "def get_not_matched_files(db_name):\n",
    "    files_list = get_files_list(Path(\"pdf_files\"))\n",
    "    db_pubs = get_pub_app_data(db_name)\n",
    "    missing=[]\n",
    "    # check which files are really missing linking\n",
    "    for file in files_list:\n",
    "        found_in_db = False\n",
    "        for db_pub in db_pubs:\n",
    "            if file.name == db_pub[4]:\n",
    "                found_in_db = True\n",
    "                break\n",
    "        if not found_in_db:\n",
    "           missing.append(file) \n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for ChemDataExtractor\n",
    "# not used for mining data references (suplementary/raw) or to get pdf metadata\n",
    "from chemdataextractor import Document\n",
    "\n",
    "# A function for getting a list of files from the directory\n",
    "# This will be modified to get the list from a csv file\n",
    "def get_files_list (source_dir):\n",
    "    i_counter = 0\n",
    "    files_list = []\n",
    "    for filepath in sorted(source_dir.glob('*.pdf')):\n",
    "        i_counter += 1\n",
    "        files_list.append(filepath)\n",
    "    return files_list\n",
    "\n",
    "def cde_read_pdfs(a_file):\n",
    "    pdf_f = open(a_file, 'rb')\n",
    "    doc = Document.from_file(pdf_f)\n",
    "    return doc\n",
    "\n",
    "def find_doi(element_text):\n",
    "    cr_re_01 = '10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.search(cr_re_01, element_text, re.IGNORECASE)\n",
    "    if compare != None:\n",
    "        return compare.group()\n",
    "    return \"\"\n",
    "\n",
    "def get_db_id(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"id\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_db_title(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"title\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_close_dois(str_name, db_name = \"prev_search.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, doi, title, pdf_file\"\n",
    "    filter_str = \"doi like '%\"+str_name+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"db_files/app_db20211005.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pdf files for publications\n",
    "\n",
    "Read database and try to recover pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get publication data from the ukch app\n",
    "db_pubs = get_pub_app_data(ukchapp_db)\n",
    "for a_pub in tqdm_notebook(db_pubs):\n",
    "    if a_pub[0] > 688:\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        pub_html = a_pub[5]\n",
    "        if pub_pdf == None:\n",
    "            not_in_url = True\n",
    "            print(\"ID: \", pub_id, \"Publication: \",pub_title,\n",
    "                  \"\\n\\tDOI: \", pub_doi, \" URL: \", pub_url)\n",
    "            if \"pdf\" in pub_url:\n",
    "                print (\"\\tTry to get the pdf from URL: \", pub_url)\n",
    "                try:\n",
    "                    response = requests.get(pub_url)\n",
    "                    content_type = response.headers['content-type']\n",
    "                    if not 'text' in content_type:\n",
    "                        #print(response.headers)\n",
    "                        cd= response.headers['content-disposition']\n",
    "                        #print(cd)\n",
    "                        fname = re.findall(\"filename=(.+)\", cd)[0]\n",
    "                        #print(fname)\n",
    "                        if not Path('pdf_files/' + pdf_file).is_file():\n",
    "                            with open('pdf_files/'+ fname +'.pdf', 'wb') as f:\n",
    "                                f.write(response.content)\n",
    "                        else:\n",
    "                            set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                        not_in_url = False\n",
    "                except:\n",
    "                    print(\"ID: \", pub_id, \"\\nPublication: \",pub_title, \n",
    "                           \"\\nDOI: \", pub_doi, \"\\nDOI: \", pub_url) \n",
    "            if not_in_url:\n",
    "                print(\"\\tTry to see if json file has link to pdf: \")\n",
    "                if valid_doi(pub_doi):\n",
    "                    crjd, doi_file = get_cr_json_object(pub_doi)\n",
    "                    got_pdf = False\n",
    "                    if \"link\" in crjd.keys():\n",
    "                        for a_link in crjd[\"link\"]:\n",
    "                            if \"\\tURL\" in a_link.keys() and (\"pdf\" in a_link[\"URL\"] or \"pdf\" in a_link[\"content-type\"]):\n",
    "                                cr_url = a_link[\"URL\"]\n",
    "                                #print(\"URL: \", cr_url)\n",
    "                                pdf_file = get_pdf_from_url(cr_url)\n",
    "                                # if the name corresponds to a existing file, assign value to db_record\n",
    "                                if Path('pdf_files/' + pdf_file).is_file():\n",
    "                                    print(\"\\tFile name:\", pdf_file)\n",
    "                                    set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                                    got_pdf = True\n",
    "                                else:\n",
    "                                    print(\"\\tcould not get file from\", cr_url)\n",
    "                    else: \n",
    "                        print(\"\\tno links in json\", pub_doi)\n",
    "                if not got_pdf and \"elsevier\" in pub_url:\n",
    "                    print(\"\\tTrying elsevier doi:\" )\n",
    "                    pdf_file = get_elsevier_pdf(pub_doi)\n",
    "                    if Path('pdf_files/' + pdf_file).is_file():\n",
    "                        print(\"\\tFile name:\", pdf_file)\n",
    "                        set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                        got_pdf = True\n",
    "                elif not got_pdf and \"wiley\" in pub_url:\n",
    "                    print(\"\\tTrying elsevier doi:\" )\n",
    "                    pdf_file = get_wiley_pdf(pub_doi)\n",
    "                    if Path('pdf_files/' + pdf_file).is_file():\n",
    "                        print(\"\\tFile name:\", pdf_file)\n",
    "                        set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                        got_pdf = True\n",
    "\n",
    "                if not got_pdf:\n",
    "                    print(\"\\tTry doi:  https://doi.org/\" + pub_doi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File name match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if file name matches some part of a doi\n",
    "files_list = get_not_matched_files(ukchapp_db)\n",
    "\n",
    "not_assigned = []\n",
    "for a_file in tqdm_notebook(files_list):\n",
    "    search_this = a_file.name.replace(\".pdf\", \"\").lower()\n",
    "    print(a_file.name,\"\\t\",search_this)\n",
    "    close_dois = get_close_dois(search_this, ukchapp_db)\n",
    "    print(len(close_dois))\n",
    "    \n",
    "    if len(close_dois) == 1 :\n",
    "        doi_dat = close_dois[0]\n",
    "        selected = False\n",
    "        if doi_dat[3] == None:\n",
    "            while not selected:\n",
    "                print(\"Assign file: \", a_file.name, \" to:\\n\\t\", doi_dat[0],doi_dat[1],doi_dat[2], doi_dat[3])\n",
    "                print('***************************************************************')\n",
    "                print(\"Options:\\n\\ta) assign\\n\\tb)go to next\")\n",
    "                print(\"selection:\")\n",
    "                usr_select = input()\n",
    "                if usr_select == 'a':\n",
    "                    selected = True\n",
    "                    set_pdf_file_value(a_file.name, doi_dat[0], ukchapp_db)\n",
    "                    print(\"assing and go to next\")\n",
    "                elif usr_select == 'b':\n",
    "                    #working_file[art_num]['ignore']=3 # visual inspection\n",
    "                    selected = True\n",
    "                    print(\"going to next\")\n",
    "        else:\n",
    "            print(\"Assigned in db: \",  doi_dat[0],doi_dat[1],doi_dat[2], doi_dat[3])\n",
    "    else:\n",
    "        not_assigned.append(a_file)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pdfminer to get metadata from pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = get_files_list(Path(\"pdf_files\"))\n",
    "db_pubs = get_pub_app_data(ukchapp_db)\n",
    "missing=[]\n",
    "# check which files are really missing linking\n",
    "for file in files_list:\n",
    "    found_in_db = False\n",
    "    for db_pub in db_pubs:\n",
    "        if file.name == db_pub[4]:\n",
    "            found_in_db = True\n",
    "            break\n",
    "    if not found_in_db:\n",
    "        missing.append(file)\n",
    "\n",
    "# check if all linked files are in the folder\n",
    "missing2=[]\n",
    "for db_pub in db_pubs:\n",
    "    found_in_system = False\n",
    "    for file in files_list:\n",
    "        if file.name == db_pub[4] or db_pub[4] == None:\n",
    "            found_in_system = True\n",
    "            break\n",
    "    if not found_in_system:\n",
    "        missing2.append(db_pub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ChemDataExtractor to read pdf and get DOIs in document\n",
    "for a_file in tqdm_notebook(not_assigned):\n",
    "    pdf_doc = cde_read_pdfs(a_file)\n",
    "    print(a_file.name)\n",
    "    dois_list = []\n",
    "    for element in pdf_doc.elements:\n",
    "        if 'doi' in str(element):\n",
    "            found_doi = find_doi(str(element))\n",
    "            if found_doi[-1:] == \".\":\n",
    "                found_doi = found_doi[:-1]\n",
    "            if not found_doi in dois_list:\n",
    "                dois_list.append(found_doi)       \n",
    "    \n",
    "    if dois_list != [] and len(dois_list) == 1:\n",
    "        for a_doi in dois_list:\n",
    "            close_dois = get_close_dois(a_doi, ukchapp_db)\n",
    "            selected = False\n",
    "            if len(close_dois) == 1:\n",
    "                doi_dat = close_dois[0]\n",
    "                if doi_dat[3] == None:\n",
    "                    while not selected:\n",
    "                        print(\"Assign file: \",a_file.name, \" to:\\n\\t\", doi_dat[0],doi_dat[1],doi_dat[2], doi_dat[3])\n",
    "                        print('***************************************************************')\n",
    "                        print(\"Options:\\n\\ta) assign\\n\\tb)go to next\")\n",
    "                        print(\"selection:\")\n",
    "                        usr_select = input()\n",
    "                        if usr_select == 'a':\n",
    "                            selected = True\n",
    "                            set_pdf_file_value(a_file.name, doi_dat[0], ukchapp_db)\n",
    "                            print(\"assing and go to next\")\n",
    "                        elif usr_select == 'b':\n",
    "                            #working_file[art_num]['ignore']=3 # visual inspection\n",
    "                            selected = True\n",
    "                            print(\"going to next\")\n",
    "                else: \n",
    "                    print(\"Already assingned to:\\n\\t\", doi_dat[0],doi_dat[1],doi_dat[2], doi_dat[3])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch to update most recent version of app DB\n",
    "ukchapp_db = \"db_files/app_db2.sqlite3\"\n",
    "ukchapp_db_prev = \"db_files/app_db.sqlite3\"\n",
    "db_pubs = get_pub_app_data(ukchapp_db)\n",
    "db_pubs_prev = get_pub_app_data(ukchapp_db_prev)\n",
    "\n",
    "for a_pub in tqdm_notebook(db_pubs):\n",
    "    for old_pub in db_pubs_prev:\n",
    "        if old_pub[0] == a_pub[0]:\n",
    "            if a_pub[4] != None:\n",
    "                print (\"************** Assigned:\", a_pub[4], old_pub[4])\n",
    "            elif a_pub[4] == None:\n",
    "                print(\"Assign file: \", old_pub[4], \" to:\\n\\t\",  a_pub[0], a_pub[1], a_pub[2]  )\n",
    "                set_pdf_file_value(old_pub[4], a_pub[0], ukchapp_db)\n",
    "                \n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
