{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data for new articles and list of all articles not citing data\n",
    "A list of publications is obtainded from the app database. This list will contain a titles, IDs and DOIs which need to be explored to look for the corresponding pdf files. \n",
    "The steps of the process are: \n",
    " 1. get a Title, DOI, and URL for each publication\n",
    " 2. convert the DOI to a pdf file name and try to open de file\n",
    " 3. use pdfMiner and/or CDE to get the reference to data\n",
    " 4. add a new dataset entry each time a new data object is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import notebook \n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n",
    "# library for handling http requests\n",
    "import requests\n",
    "# import custom functions (common to various notebooks)\n",
    "import processing_functions as pr_fns\n",
    "\n",
    "current_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pdf_column_populated(data_db):\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    \n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "    i_indx = 0\n",
    "    for a_pub in app_pubs:\n",
    "        if a_pub[4] != None:\n",
    "            i_indx += 1\n",
    "    #print (i_indx/len(app_pubs) > 0.9, i_indx/len(app_pubs))\n",
    "    return (i_indx/len(app_pubs) > 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pdf and HTML names into app DB\n",
    "\n",
    "0. Add fields to articles table for holding pdf file names\n",
    "1. Open the previously verified DB and get the publications list\n",
    "2. Open the current publication list from the appdb\n",
    "3. Get pdf and html file names from previous and put it in current\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pdf_file_column(db_name, table_name, column_name, column_type):\n",
    "    if not column_exists(db_name, table_name, column_name):\n",
    "        ukchapp_db = \"db_files/\" + db_name + \".sqlite3\"\n",
    "        db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "        db_conn.add_column(table_name, column_name, column_type)\n",
    "    else:\n",
    "        print (column_name, \"Alredy exists in \", table_name)\n",
    "        \n",
    "def column_exists(db_name, table_name, column_name):\n",
    "    ukchapp_db = \"db_files/\" + db_name + \".sqlite3\"\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    ti=db_conn.get_table_info('articles')\n",
    "    for a_col in ti:\n",
    "        if a_col[1] == column_name:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "    \n",
    "def add_pdf_file_names(prev_db, curr_db):\n",
    "    has_file_names = False\n",
    "    \n",
    "    prevapp_db = \"db_files/\"+prev_db +\".sqlite3\"\n",
    "\n",
    "    while not Path(prevapp_db).is_file():\n",
    "        print('Please enter the name of app db file:')\n",
    "        prevapp_db = input()\n",
    "\n",
    "    # get publication data from the db\n",
    "    prev_pubs = pr_fns.get_pub_data(prevapp_db)\n",
    "\n",
    "    #2 currend app DB\n",
    "    ukchapp_db = \"db_files/\" + curr_db + \".sqlite3\"\n",
    "    while not Path(ukchapp_db).is_file():\n",
    "        print('Please enter the name of app db file:')\n",
    "        curr_db = input()\n",
    "        ukchapp_db = \"db_files/\" + curr_db + \".sqlite3\"\n",
    "    \n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "    \n",
    "    # check if file names have been added\n",
    "    # return true if OK\n",
    "    if pdf_column_populated(curr_db):\n",
    "        has_file_names = True\n",
    "        return curr_db, has_file_names\n",
    "\n",
    "    # 3 get pdf file name from previous and put it in current\n",
    "    for a_pub in notebook.tqdm(prev_pubs):\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        match_found = False\n",
    "        for curr_pub in app_pubs:\n",
    "            if curr_pub[2] == pub_doi and pub_doi != None:\n",
    "                pr_fns.set_pdf_file_value(pub_pdf, curr_pub[0], ukchapp_db)\n",
    "                match_found = True\n",
    "                break\n",
    "            elif curr_pub[1] == pub_title:\n",
    "                pr_fns.set_pdf_file_value(pub_pdf, curr_pub[0], ukchapp_db)\n",
    "                match_found = True\n",
    "                break\n",
    "        if not match_found:\n",
    "            print(\"*************\\n\",a_pub)\n",
    "\n",
    "        has_file_names = True\n",
    "    return curr_db, has_file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that idexed pdf files exist \n",
    "\n",
    "Use the data on the articles table to verify if file are stored in the corresponding folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_data_exists(data_db):\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    \n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "    i_indx = 0\n",
    "    for a_pub in notebook.tqdm(app_pubs):\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        pub_pdf = a_pub[4]\n",
    "        if pub_pdf == None:\n",
    "            print(\"*************************\")\n",
    "            i_indx +=1\n",
    "            print(i_indx, \"Missing PDF for:\", pub_doi, pub_id)\n",
    "            \n",
    "        else:\n",
    "            pdf_file = \"pdf_files/\" + pub_pdf\n",
    "            if not Path(pdf_file).is_file():\n",
    "                print(\"*************************\")\n",
    "                i_indx +=1\n",
    "                print(i_indx, \"Missing file for:\", pdf_file, \"for\", pub_doi, pub_id)\n",
    "                \n",
    "    #print(i_indx/len(app_pubs) )\n",
    "    # If less than 1% if missing that is OK\n",
    "    return (i_indx/len(app_pubs) < 0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that all PDF files are indexed \n",
    "\n",
    "Check that the files in the folder are all accounted for (have a corersponding record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_files_in_db(data_db):\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    \n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "    files_not_in_DB = 0\n",
    "    for infile in notebook.tqdm(Path(\"pdf_files\").glob('*.pdf')):\n",
    "        file_found = False\n",
    "        for a_pub in app_pubs:\n",
    "            if infile.name == a_pub[4]:\n",
    "                file_found = True\n",
    "                break\n",
    "        if not file_found:\n",
    "            print(\"Not in DB:\", infile.name)\n",
    "            files_not_in_DB += 1\n",
    "    return files_not_in_DB < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get missing pdfs\n",
    "If there are more than 1% missing try to get them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use regular expression to check if a given string\n",
    "# is a valid DOI, using pattern from CR\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    if cr_doi == None:\n",
    "        return False\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_missing_pdfs(data_db):\n",
    "    return_val = False\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    # get publication data from the ukch app\n",
    "    db_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "    for a_pub in notebook.tqdm(db_pubs):\n",
    "        if a_pub[0] > 0:\n",
    "            pub_id = a_pub[0]\n",
    "            pub_title = a_pub[1]\n",
    "            pub_doi = a_pub[2]\n",
    "            pub_url = a_pub[3]\n",
    "            pub_pdf = a_pub[4]\n",
    "            if pub_pdf == None:\n",
    "                not_in_url = True\n",
    "                print(\"ID: \", pub_id, \"Publication: \",pub_title,\n",
    "                      \"\\n\\tDOI: \", pub_doi, \" URL: \", pub_url)\n",
    "                if \"pdf\" in pub_url:\n",
    "                    print (\"\\tTry to get the pdf from URL: \", pub_url)\n",
    "                    try:\n",
    "                        response = requests.get(pub_url)\n",
    "                        content_type = response.headers['content-type']\n",
    "                        if not 'text' in content_type:\n",
    "                            #print(response.headers)\n",
    "                            cd= response.headers['content-disposition']\n",
    "                            #print(cd)\n",
    "                            fname = re.findall(\"filename=(.+)\", cd)[0]\n",
    "                            #print(fname)\n",
    "                            if not Path('pdf_files/' + pdf_file).is_file():\n",
    "                                with open('pdf_files/'+ fname +'.pdf', 'wb') as f:\n",
    "                                    f.write(response.content)\n",
    "                            else:\n",
    "                                set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            not_in_url = False\n",
    "                    except:\n",
    "                        print(\"ID: \", pub_id, \"\\nPublication: \",pub_title, \n",
    "                               \"\\nDOI: \", pub_doi, \"\\nDOI: \", pub_url) \n",
    "                if not_in_url:\n",
    "                    print(\"\\tTry to see if json file has link to pdf: \")\n",
    "                    if valid_doi(pub_doi):\n",
    "                        crjd, doi_file = pr_fns.get_cr_json_object(pub_doi)\n",
    "                        got_pdf = False\n",
    "                        if \"link\" in crjd.keys():\n",
    "                            for a_link in crjd[\"link\"]:\n",
    "                                if \"\\tURL\" in a_link.keys() and (\"pdf\" in a_link[\"URL\"] or \"pdf\" in a_link[\"content-type\"]):\n",
    "                                    cr_url = a_link[\"URL\"]\n",
    "                                    #print(\"URL: \", cr_url)\n",
    "                                    pdf_file = get_pdf_from_url(cr_url)\n",
    "                                    # if the name corresponds to a existing file, assign value to db_record\n",
    "                                    if Path('pdf_files/' + pdf_file).is_file():\n",
    "                                        print(\"\\tFile name:\", pdf_file)\n",
    "                                        set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                                        got_pdf = True\n",
    "                                    else:\n",
    "                                        print(\"\\tcould not get file from\", cr_url)\n",
    "                        else: \n",
    "                            print(\"\\tno links in json\", pub_doi)\n",
    "                    if not got_pdf and \"elsevier\" in pub_url:\n",
    "                        print(\"\\tTrying elsevier doi:\" )\n",
    "                        pdf_file = pr_fns.get_elsevier_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    elif not got_pdf and \"wiley\" in pub_url:\n",
    "                        print(\"\\tTrying wiley doi:\" )\n",
    "                        pdf_file = pr_fns.get_wiley_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    elif not got_pdf and \"pubs.acs\" in pub_url:\n",
    "                        print(\"\\tTrying acs doi:\" )\n",
    "                        pdf_file = pr_fns.get_acs_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    if not got_pdf:\n",
    "                        print(\"\\tTry doi:  https://doi.org/\" + pub_doi)\n",
    "    return return_val\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pdfminer to get metadata from pdf file\n",
    "\n",
    "Functions which use pdf miner to get data from pdf_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer import high_level as pdfmnr_hl\n",
    "\n",
    "# functions for PDFminer\n",
    "\n",
    "def get_pdf_text(pdf_file):\n",
    "    return pdfmnr_hl.extract_text(pdf_file)\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_ref_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if pr_fns.is_data_stmt(sentence.lower()):\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1, idx, idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            print (single_sentence)\n",
    "            full_sentence += \" \" + sentences[single_sentence].strip()\n",
    "        return_group.append(full_sentence)\n",
    "        print (full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_all_data_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if 'data' in sentence.lower() or 'inform' in sentence.lower():\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1, idx, idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        if not full_sentence in return_group:\n",
    "            return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the http strings from references to data\n",
    "def get_http_ref(sentence):\n",
    "    http_frag = \"\"\n",
    "    if 'http' in sentence.lower():\n",
    "        idx_http = sentence.lower().index('http')\n",
    "        http_frag = sentence[idx_http:]\n",
    "        space_in_ref = True\n",
    "        while \" \" in http_frag:\n",
    "            space_idx = http_frag.rfind(\" \")\n",
    "            http_frag = http_frag[:space_idx]\n",
    "        if(http_frag[-1:]==\".\"):\n",
    "            http_frag = http_frag[:-1]\n",
    "    return http_frag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data mentions from pdf files\n",
    "Write the results to a csv file to be checked to verify data mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_refs(data_db, start_processing, stop_processing, work_dir):\n",
    "    ukchapp_db = \"db_files/\" + data_db + \".sqlite3\"\n",
    "    out_name =  'pdf_mentions' + \"_\" + str(start_processing).zfill(4)+ \"_\" + str(stop_processing).zfill(4)\n",
    "    out_file = Path(work_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked for data refences in:\", data_db, \"saved as\", out_file)\n",
    "        return out_name\n",
    "\n",
    "    # get publication data from the ukch app\n",
    "    db_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "    # get the list of dois already mined for data \n",
    "    input_file = './data_load/pub_data_add202012.csv'\n",
    "    id_field = 'num'\n",
    "    processed, headings = csvh.get_csv_data(input_file, id_field)\n",
    "    for id_num in processed:\n",
    "        current_title = processed[id_num]['doi']\n",
    "    processed[1]['num']\n",
    "\n",
    "    processed_dois = []\n",
    "    for entry in processed:\n",
    "        if not processed[entry]['doi'] in processed_dois:\n",
    "            processed_dois.append( processed[entry]['doi'])\n",
    "\n",
    "    data_records = {}\n",
    "    data_mentions = {}\n",
    "    ref_count = mention_count = 0\n",
    "    for a_pub in notebook.tqdm(db_pubs):\n",
    "        data_refs = []\n",
    "        data_sents = []\n",
    "        if a_pub[0] >= start_processing:\n",
    "            pub_id = a_pub[0]\n",
    "            pub_title = a_pub[1]\n",
    "            pub_doi = a_pub[2]\n",
    "            pub_url = a_pub[3]\n",
    "            pub_pdf = a_pub[4]\n",
    "            if pub_pdf == 'None':\n",
    "                print(\"*************************\")\n",
    "                print(\"Missing PDF for:\", pub_doi)\n",
    "                print(\"*************************\")\n",
    "            else:\n",
    "                pdf_file = \"pdf_files/\" + pub_pdf\n",
    "                if not Path(pdf_file).is_file():\n",
    "                    print(\"*************************\")\n",
    "                    print(\"Missing file for:\", pdf_file, \"for\", pub_doi)\n",
    "                    print(\"*************************\")\n",
    "                else: \n",
    "                    print(\"PDF filename\", pdf_file)\n",
    "                    pdf_text = get_pdf_text(pdf_file) # gets the whole PDF text\n",
    "                    ref_sentences = get_ref_sentences(pdf_text) # filter only references which mention data or information\n",
    "                    data_sentences = get_all_data_sentences(pdf_text)\n",
    "                    for r_sentence in ref_sentences:\n",
    "                        dt_link = get_http_ref(r_sentence)\n",
    "                        if 'supplem' in r_sentence.lower():\n",
    "                            data_refs.append({'type':'supplementary',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                        else:\n",
    "                            data_refs.append({'type':'supporting',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                    for d_sentence in data_sentences:\n",
    "                        dt_link = get_http_ref(d_sentence)\n",
    "                        if 'supplem' in d_sentence.lower():\n",
    "                            data_sents.append({'type':'supplementary',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "                        else:\n",
    "                            data_sents.append({'type':'supporting',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "            if data_refs != []:\n",
    "                for data_ref in data_refs:\n",
    "                    data_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                    data_record.update(data_ref)\n",
    "                    data_records[ref_count] = data_record\n",
    "                    ref_count += 1\n",
    "            if data_sents != []:\n",
    "                for data_sent in data_sents:\n",
    "                    sentence_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                    sentence_record.update(data_sent)\n",
    "                    data_mentions[mention_count] = sentence_record\n",
    "                    mention_count += 1\n",
    "        if a_pub[0] >= stop_processing :\n",
    "            break # for debugging           \n",
    "    # csvh.write_csv_data(data_records, 'pdf_data.csv')\n",
    "    if len(data_mentions) > 0:\n",
    "        csvh.write_csv_data(data_mentions, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark for review\n",
    "\n",
    "Verify if the mentions of data or information actually can be linked to data objects.\n",
    "\n",
    "Results need to be reviewed interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_interactivex(data_refs, work_dir):\n",
    "    in_name = data_refs + \"_int\"\n",
    "    out_name = data_refs + \"_rev\"\n",
    "    out_file = Path(work_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked data refences see:\", out_file)\n",
    "        return out_name\n",
    "        \n",
    "    print('Input File: ', in_name)\n",
    "    # Open results file\n",
    "    data_mentions, dm_headers = csvh.get_csv_data(Path(work_dir, in_name+'.csv'))\n",
    "    print(dm_headers)\n",
    "    art_id = ''\n",
    "    terminate = False\n",
    "    additional_rows = {}\n",
    "    for dm in data_mentions:\n",
    "        if data_mentions[dm]['action']=='review':\n",
    "            clear_output()\n",
    "            print (\"*******************************************\")\n",
    "            print (\"Article id  :\", data_mentions[dm]['id'])\n",
    "            print (\"DOI         :\", data_mentions[dm]['doi'])\n",
    "            print (\"Type        :\", data_mentions[dm]['type'], '\\tLine:', dm)\n",
    "            print (\"Description :\\n\\t\", data_mentions[dm]['desc'])\n",
    "            print (\"data_url :\", data_mentions[dm]['data_url'])\n",
    "            print (\"*******************************************\")\n",
    "            decide_action = False\n",
    "            while not decide_action:\n",
    "                print('Action:')\n",
    "                print('\\tr) review: https://doi.org/'+data_mentions[dm]['doi'])\n",
    "                print('\\ta) add new row')\n",
    "                print('\\tn) next')\n",
    "                print('\\tt) terminate')\n",
    "                print('\\tSelect r, a, n, t:')\n",
    "                lts = input()\n",
    "                if lts == \"r\":\n",
    "                    data_mentions[dm]['action'] = 'reviewed'\n",
    "                    print ('https://doi.org/'+data_mentions[dm]['doi'])\n",
    "                    print ('link:',data_mentions[dm]['link'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['link'] = add_this\n",
    "                    print ('issue:',data_mentions[dm]['issue'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['issue'] = add_this\n",
    "                    print ('name:',data_mentions[dm]['name'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['name'] = add_this\n",
    "                    print ('file:',data_mentions[dm]['file'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['file'] = add_this\n",
    "                if lts == \"a\":\n",
    "                    #add a new row\n",
    "                    new_idx = len(data_mentions) + len(additional_rows) + 1\n",
    "                    additional_rows[new_idx] = {}\n",
    "                    additional_rows[new_idx]['id'] = data_mentions[dm]['id']\n",
    "                    additional_rows[new_idx]['doi'] = data_mentions[dm]['doi']\n",
    "                    additional_rows[new_idx]['type'] = data_mentions[dm]['type']\n",
    "                    additional_rows[new_idx]['desc'] = data_mentions[dm]['desc']\n",
    "                    additional_rows[new_idx]['action'] = 'reviewed'\n",
    "                    print ('link:')\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['link'] = add_this\n",
    "                    print ('issue:')\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['issue'] = add_this\n",
    "                    print ('name:')\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['name'] = add_this\n",
    "                    print ('file:')\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['file'] = add_this\n",
    "                elif lts == \"n\":\n",
    "                    if data_mentions[dm]['action'] != 'reviewed':\n",
    "                        data_mentions[dm]['action'] = 'none'\n",
    "                    decide_action = True\n",
    "                elif lts == 't':\n",
    "                    decide_action = True\n",
    "                    terminate = True\n",
    "        art__id = data_mentions[dm]['id']\n",
    "        if dm > 1700 or terminate:\n",
    "            break\n",
    "    if len(additional_rows)> 0 :\n",
    "        for nr in additional_rows:\n",
    "           data_mentions[nr] = additional_rows[nr]\n",
    "    if len(data_mentions) > 0:\n",
    "       csvh.write_csv_data(data_mentions, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# clear the output after each loop cycle\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def review_interactive(data_refs, work_dir):\n",
    "    out_name = data_refs + \"_int\"\n",
    "    out_file = Path(work_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked data refences see:\", out_file)\n",
    "        return out_name\n",
    "\n",
    "    # Open results file\n",
    "    data_mentions, dm_headers = csvh.get_csv_data(Path(work_dir,data_refs+ '.csv'))\n",
    "    art_id = ''\n",
    "    for dm in data_mentions:\n",
    "        # only review if data statement is true\n",
    "        if data_mentions[dm]['DataStatement'] == \"1\":\n",
    "            clear_output()\n",
    "            print (\"*******************************************\")\n",
    "            print (\"Article id  :\", data_mentions[dm]['id'])\n",
    "            print (\"DOI         :\", data_mentions[dm]['doi'])\n",
    "            print (\"Type        :\", data_mentions[dm]['type'], '\\tLine:', dm)\n",
    "            print (\"Description :\\n\\t\", data_mentions[dm]['desc'])\n",
    "            print (\"data_url :\", data_mentions[dm]['data_url'])\n",
    "            print (\"*******************************************\")\n",
    "            decide_action = False\n",
    "            while not decide_action:\n",
    "                print('Action:')\n",
    "                print('\\ta) review')\n",
    "                print('\\tb) none')\n",
    "                print('\\tSelect a or b:')\n",
    "                lts = input()\n",
    "                if lts == \"a\":\n",
    "                    data_mentions[dm]['action'] = 'review'\n",
    "                    decide_action = True\n",
    "                elif lts == \"b\":\n",
    "                    data_mentions[dm]['action'] = 'none'\n",
    "                    decide_action = True\n",
    "        else:\n",
    "              data_mentions[dm]['action'] = 'none'\n",
    "        art_id = data_mentions[dm]['id']\n",
    "    if len(data_mentions) > 0:\n",
    "        csvh.write_csv_data(data_mentions, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review references interactively\n",
    "\n",
    "Check each marked reference to determine if they should be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run next to get the ones which need to be reviewed online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def revise_online(revised_refs, db_name, work_dir):\n",
    "    print (revised_refs, db_name, work_dir)\n",
    "    out_name = 'html_'+db_name\n",
    "    out_file = Path(work_dir,out_name+'.csv')\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked refences online:\", out_file)\n",
    "        return out_name\n",
    "    in_file = Path(Path(work_dir),revised_refs+'.csv')\n",
    "    data_mentions, dm_headers = csvh.get_csv_data(in_file)\n",
    "    filter_mentions = {}\n",
    "    for dm in data_mentions:\n",
    "        if 'add' in data_mentions[dm].keys() and data_mentions[dm]['add'] == '1':\n",
    "            filter_mentions[dm]={}\n",
    "            for a_field in dm_headers:\n",
    "                filter_mentions[dm][a_field] = data_mentions[dm][a_field]\n",
    "    print('filtered mentions:', len(filter_mentions))\n",
    "\n",
    "    new_do_id_list =[]\n",
    "    for fm in filter_mentions:\n",
    "        art_id = int(filter_mentions[fm][\"id\"])\n",
    "        if not art_id in new_do_id_list:\n",
    "            new_do_id_list.append(art_id)\n",
    "\n",
    "    # currend app DB\n",
    "    ukchapp_db = \"db_files/\"+db_name+\".sqlite3\"\n",
    "\n",
    "    no_data_pubs = pr_fns.get_pub_app_no_data(ukchapp_db)\n",
    "\n",
    "    print(len(no_data_pubs))\n",
    "    print(new_do_id_list, len(new_do_id_list))\n",
    "    filter_mentions\n",
    "\n",
    "\n",
    "    int_idx = 0\n",
    "    revised_list = {}\n",
    "    if Path(\"./html_revised202111.csv\").is_file():\n",
    "        revised_list, rl_headers = csvh.get_csv_data('html_revised202111.csv')\n",
    "        int_idx = len(revised_list)\n",
    "\n",
    "    already_revised =[]\n",
    "    for fm in revised_list:\n",
    "        art_id = int(revised_list[fm][\"id\"])\n",
    "        if not art_id in already_revised:\n",
    "            already_revised.append(art_id)\n",
    "\n",
    "    for ndp in no_data_pubs:\n",
    "        if not ndp[0] in new_do_id_list and ndp[0] > 786 and not ndp[0] in already_revised:\n",
    "            int_idx += 1\n",
    "            pub_id = ndp[0]\n",
    "            pub_title = ndp[1]\n",
    "            pub_doi = ndp[2]\n",
    "            pub_url = ndp[3]\n",
    "            data_record = {'id':pub_id, 'doi':pub_doi, 'title':pub_title} \n",
    "            print ('id',pub_id, '\\n', pub_title)\n",
    "            decide_action = False\n",
    "            terminate = False\n",
    "            while not decide_action:\n",
    "                print('Action:')\n",
    "                print(pub_url)\n",
    "                print(\"https://doi.org/\"+pub_doi)\n",
    "                print('\\ts) skip (no data)' )\n",
    "                print('\\tr) review')\n",
    "                print('\\tn) next')\n",
    "                print('\\tt) terminate')\n",
    "                print('\\tSelect s, r, n, t:')\n",
    "                lts = input()\n",
    "                if lts == \"s\":\n",
    "                    data_record['action'] = 'no data'\n",
    "                    data_record['issue'] = \"no data availability or supplementary data mentioned in html or pdf versions or article\"\n",
    "                    revised_list[int_idx] = data_record\n",
    "                    decide_action = True\n",
    "                if lts == \"r\":\n",
    "                    data_record['action'] = 'review'\n",
    "                    if 'issue' in data_mentions[dm].keys():\n",
    "                        print ('issue:',data_mentions[dm]['issue'])\n",
    "                    add_this = input()\n",
    "                    data_record['issue'] = add_this\n",
    "                    revised_list[int_idx] = data_record\n",
    "                    decide_action = True\n",
    "                if lts == \"n\":\n",
    "                    decide_action = True\n",
    "                elif lts == 't':\n",
    "                    decide_action = True\n",
    "                    terminate = True\n",
    "            if terminate:\n",
    "                break\n",
    "\n",
    "    if len(revised_list) > 0:\n",
    "        csvh.write_csv_data(revised_list, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_file Alredy exists in  articles\n",
      "1. PDF file names copied to production202412\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4be363148f47ac954fe702728c3e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/774 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "1 Missing file for: pdf_files/Not available for 10.1002/9783527804085.ch10 64\n",
      "*************************\n",
      "2 Missing file for: pdf_files/NA for 10.1016/b978-0-12-805324-9.09989-1 599\n",
      "*************************\n",
      "3 Missing file for: pdf_files/Not available for 10.1142/q0035 603\n",
      "*************************\n",
      "4 Missing file for: pdf_files/NA for 10.1142/q0354 925\n",
      "*************************\n",
      "5 Missing file for: pdf_files/NA for None 1029\n",
      "2. PDF file names copied to production202412\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af1b355d4594b98873927a04e37e08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in DB: 10.1002_cplu.202300413.pdf\n",
      "Not in DB: ChemBioChem-2023-Wahart-Harnessing_a_Biocatalyst_to_Bioremediate.pdf\n",
      "Not in DB: ChemPlusChem - 2023 - Price - Impact of Porous Silica Nanosphere Architectures on the Catalytic Performance of Supported.pdf\n",
      "Not in DB: ChemPlusChem-2023-Aljohani-Enhancing_Hydrogen_Production_from_the_Photoreforming _of_Lignin.pdf\n",
      "Not in DB: ChemPlusChem-2023-Peng-A_Facile_Synthesis_Route_to_AuPd_Alloys.pdf\n",
      "Not in DB: ChemSusChem-2023-Al_Sobhi-A_Comparison_of_the_Reactivity_of_the_Lattice_Nitrogen.pdf\n",
      "Not in DB: dorota_matras_phd.PDF\n",
      "Not in DB: Synology_RS816_Data_Sheet_enu.pdf\n"
     ]
    }
   ],
   "source": [
    "# (0) Add column for pdf_file names\n",
    "db_name = 'production202412'\n",
    "add_pdf_file_column(db_name, \"articles\", \"pdf_file\", \"varchar\")\n",
    "\n",
    "#(1) previously verified files:\n",
    "last_processed = 800\n",
    "prev_db_name = \"production202410\"\n",
    "\n",
    "db_name, names_added = add_pdf_file_names(prev_db_name, db_name)\n",
    "# working dir\n",
    "pdf_data_search_dir = \"./data_search_pdf_b\"\n",
    "\n",
    "if (names_added):\n",
    "    print (\"1. PDF file names copied to\", db_name);\n",
    "pdfs_ok = pdf_data_exists(db_name)\n",
    "if pdfs_ok:\n",
    "    print (\"2. PDF file names copied to\", db_name);\n",
    "    not_indexed = check_files_in_db(db_name)\n",
    "    if not_indexed:\n",
    "        print (\"3. All PDFs are indexed in\", db_name )\n",
    "else:\n",
    "    pdfs_ok = get_missing_pdfs(db_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1854412cd384eda9cb8ff8535dd64f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/774 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF filename pdf_files/d4ey00044g.pdf\n",
      "PDF filename pdf_files/s41929-024-01181-w.pdf\n",
      "PDF filename pdf_files/wang-et-al-2024-amphiphilic-janus-particles-for-aerobic-alcohol-oxidation-in-oil-foams.pdf\n",
      "PDF filename pdf_files/farooq-et-al-2024-chemical-imaging-of-carbide-formation-and-its-effect-on-alcohol-selectivity-in-fischer-tropsch.pdf\n",
      "PDF filename pdf_files/hardy-et-al-2024-probing-ferryl-reactivity-in-a-nonheme-iron-oxygenase-using-an-expanded-genetic-code.pdf\n",
      "PDF filename pdf_files/catalysts-14-00463.pdf\n",
      "PDF filename pdf_files/AdvancedScience-2024-ulHaq-DielectricBarrierPlasmaDischargeExsolutionNanoparticlesRoomTemperature.pdf\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      " repeated, are provided in the corresponding sections of this paper. Ad- ditional results and raw data underlying this work are available in the Supporting Information or on request following instructions provided at https://doi.org/10.15129/e2e11901-92c4-4b2e-a83e-ﬀ25052e972a.\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      " repeated, are provided in the corresponding sections of this paper. Ad- ditional results and raw data underlying this work are available in the Supporting Information or on request following instructions provided at https://doi.org/10.15129/e2e11901-92c4-4b2e-a83e-ﬀ25052e972a.\n",
      "PDF filename pdf_files/D4SC02051K.pdf\n",
      "PDF filename pdf_files/smart-et-al-2024-a-resorcin-4-arene-based-phosphite-phosphine-ligand-for-the-branched-selective-hydroformylation-of.pdf\n",
      "PDF filename pdf_files/ChemSusChem-2016-Gill-DesignHighlySelectivePlatinumNanoparticleCatalystsAerobicOxidationKA‐Oil.pdf\n",
      "PDF filename pdf_files/zhu-et-al-2015-selective-polymerization-catalysis-controlling-the-metal-chain-end-group-to-prepare-block-copolyesters.pdf\n",
      "PDF filename pdf_files/s41929-024-01234-0.pdf\n",
      "PDF filename pdf_files/s41467-024-51925-2.pdf\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      " Data availability Raw data that further supports and underpins the manuscript can be found in the supplementary information, with all raw data available via the online digital\n",
      "1728\n",
      "1729\n",
      "1730\n",
      " Additional information Supplementary information The online version contains supplementary material available at\n",
      "PDF filename pdf_files/1-s2.0-S0021951724004913-main.pdf\n",
      "PDF filename pdf_files/1-s2.0-S0039602824001717-main.pdf\n",
      "PDF filename pdf_files/1-s2.0-S0926337325000128-main.pdf\n",
      "PDF filename pdf_files/d4cp02949f.pdf\n",
      "PDF filename pdf_files/ChemCatChem - 2024 - Doherty - Gold Nanoparticle‐Catalyzed Solvent Switchable Selective Partial Reduction of Nitrobenzene.pdf\n",
      "PDF filename pdf_files/d3cs00468f.pdf\n",
      "PDF filename pdf_files/eisenhardt-et-al-2024-understanding-the-effect-of-m(iii)-choice-in-heterodinuclear-polymerization-catalysts.pdf\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      " *sı Supporting Information The Supporting Information is available free of charge at . (PDF) The Supporting Information is available free of charge at https://pubs.acs.org/doi/10.1021/acs.inorgchem.4c04430.\n",
      "PDF filename pdf_files/d4su00547c.pdf\n",
      "PDF filename pdf_files/d4re00449c.pdf\n",
      "PDF filename pdf_files/an-et-al-2024-activation-and-catalysis-of-methane-over-metal-organic-framework-materials.pdf\n",
      "PDF filename pdf_files/1-s2.0-S2095927324009873-main.pdf\n",
      "PDF filename pdf_files/d4cp03761h.pdf\n",
      "4. PDFs data references stored in pdf_mentions_1047_1071\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'DataStatement'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4. PDFs data references stored in\u001b[39m\u001b[38;5;124m\"\u001b[39m, pdf_mentions)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# use perceptron to filter data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m######\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# review marked as datasentences\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m review_marked \u001b[38;5;241m=\u001b[39m \u001b[43mreview_interactive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_mentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf_data_search_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5. PDFs data references marked for review\u001b[39m\u001b[38;5;124m\"\u001b[39m, review_marked)\n\u001b[0;32m     13\u001b[0m html_refs \u001b[38;5;241m=\u001b[39m revise_online(pdf_mentions, db_name, pdf_data_search_dir)\n",
      "Cell \u001b[1;32mIn[24], line 16\u001b[0m, in \u001b[0;36mreview_interactive\u001b[1;34m(data_refs, work_dir)\u001b[0m\n\u001b[0;32m     13\u001b[0m art_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dm \u001b[38;5;129;01min\u001b[39;00m data_mentions:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# only review if data statement is true\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdata_mentions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDataStatement\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     17\u001b[0m         clear_output()\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*******************************************\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'DataStatement'"
     ]
    }
   ],
   "source": [
    "#review data references\n",
    "start_from = 1047\n",
    "stop_at = 1071\n",
    "pdf_mentions = \"pdf_mentionsproduction202402\"\n",
    "if pdfs_ok:\n",
    "    pdf_mentions = get_data_refs(db_name, start_from, stop_at, pdf_data_search_dir)\n",
    "    print (\"4. PDFs data references stored in\", pdf_mentions)\n",
    "    # use perceptron to filter data\n",
    "    ######\n",
    "    # review marked as datasentences\n",
    "    review_marked = review_interactive(pdf_mentions, pdf_data_search_dir)\n",
    "    print (\"5. PDFs data references marked for review\", review_marked)\n",
    "    html_refs = revise_online(pdf_mentions, db_name, pdf_data_search_dir)\n",
    "    #print (\"6. Online references reviewed\", html_refs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(revised_list) > 0:\n",
    "    csvh.writre_csv_data(revised_list, 'html_revised202301.csv')\n",
    "revised_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"db_files/app_db2.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "ukchapp_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names and links for references in data mentions\n",
    "data_mentions, dm_fields = csvh.get_csv_data('pdf_mentions_filtered_02.csv', 'num')\n",
    "\n",
    "for dm in data_mentions:\n",
    "    print(\"https://doi.org/\" + data_mentions[dm]['doi'])\n",
    "    \n",
    "    ref_name = data_mentions[dm]['ref_name']\n",
    "    while ref_name == \"\":\n",
    "        print('Please enter the name of data object:')\n",
    "        ref_name = input()\n",
    "    ref_link = data_mentions[dm]['ref_link']\n",
    "    while ref_link == \"\":\n",
    "        print('Please enter the data object link:')\n",
    "        ref_link = input()\n",
    "    data_mentions[dm]['ref_name'] = ref_name\n",
    "    data_mentions[dm]['ref_link'] = ref_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getmembers, isfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pdfminer.high_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
