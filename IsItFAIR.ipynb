{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are Data Objects referenced from publications FAIR\n",
    "\n",
    "A list of data objects referenced from a set of publications is checked to determine their FAIRness.\n",
    "\n",
    "Instead of only referencing to data, these tests are applied to **data objects**, which are any data which is published to complement the publication, this includes raw data, supplementary data, processing data, tables, images, movies, and compilations containing one or more of such resources.\n",
    "\n",
    "The tests to be performed are aimed at finding out if the data objects are: \n",
    " - **F**indable:  can the object be found easily?\n",
    " - **A**ccessible: can the object be retrieved?\n",
    " - **I**nteroperable: can the object be accesed programatically to extract data and metadata?\n",
    " - **R**eusable: can the object be readily used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "import tqdm\n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n",
    "# library for handling http requests\n",
    "import requests\n",
    "# library for accessing system functions\n",
    "import os\n",
    "# import custom functions (common to various notebooks)\n",
    "import processing_functions as pr_fns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findable\n",
    "\n",
    "Most of the data objects are assumed to be findable as we were able to find links to them. However, some are references to other pages, references to contact the authors or point to repositories without identifying a specific record.\n",
    "\n",
    "### Findability Score\n",
    "A findability score was calculated for each data object as follows:\n",
    "5 if the object is referenced from the publication web page, it is referenced directly, and further details from it can be recovered (name, type and size) just by accessing that reference. \n",
    "After this, for each additional step points are deucted from the top score, if to get to the referenced object:\n",
    "- Special access to the publication is need (download the pdf, get a password or token for mining the publication, other acces blocks) \\[-1 point\\]\n",
    "- Human access to the publication online is required (there is no metadata or clear pattern to identify a reference on the publication landing page or the pdf version redirects to the article). \\[-1 point\\]\n",
    "- Recovering the reference object details (name, type and size) requires more than a single query. \\[-1 point\\]\n",
    "- The reference is wrong (broken link). \\[-2 points\\] \n",
    "- The reference points to contact the authors or lookup a data repository without an ID. \\[-4 points\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-10-a2bde914cbb4>, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-a2bde914cbb4>\"\u001b[1;36m, line \u001b[1;32m39\u001b[0m\n\u001b[1;33m    data_reference[dr]['f_score'] -= 1 # there is some form of redirect to get to the  object\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# get names and links for references in data mentions\n",
    "data_reference, _ = csvh.get_csv_data('pub_data_fairness.csv', 'num')\n",
    "\n",
    "for dr in tqdm_notebook(data_reference):\n",
    "    if data_reference[dr]['ret_code'] == \"\":\n",
    "        # try to get data object details from reference\n",
    "        print(\"Article Link: https://doi.org/\" + data_reference[dr]['doi'])\n",
    "        ref_name = data_reference[dr]['name']\n",
    "        ref_link = data_reference[dr]['data_url']\n",
    "        print(\"Search for: Data Name:\", ref_name, \"data link:\", ref_link)\n",
    "        head = urlh.getPageHeader(ref_link)\n",
    "\n",
    "        if head != None:\n",
    "            data_reference[dr]['ret_code'] = head.status_code \n",
    "            data_reference[dr]['resoruce_name'] = os.path.basename(head.url)\n",
    "            if head.status_code == 200:\n",
    "                #print (head.headers, head.url)\n",
    "                if 'content-type' in head.headers.keys():\n",
    "                    data_reference[dr]['ref_content'] = head.headers['content-type']\n",
    "                if 'content-length' in head.headers.keys():\n",
    "                    data_reference[dr]['ref_size'] = head.headers['content-length']\n",
    "                data_reference[dr]['ref_redirect'] = head.url\n",
    "            elif head.status_code == 302 or head.status_code == 301:\n",
    "                #print(head, head.headers)\n",
    "                data_reference[dr]['ref_redirect'] = head.headers['location']\n",
    "                data_reference[dr]['resoruce_name'] = os.path.basename(head.headers['location'])\n",
    "            else:\n",
    "                print(head, head.headers)\n",
    "        else:\n",
    "            data_reference[dr]['f_score'] = 1\n",
    "    else:\n",
    "        data_reference[dr]['f_score'] = 5\n",
    "        if data_reference[dr]['html_mined'] == False and data_reference[dr]['pdf_mined'] == True:\n",
    "            data_reference[dr]['f_score'] -= 1 # the publication page is not accessible directly to get the DO reference \n",
    "        if data_reference[dr]['html_mined'] == False and data_reference[dr]['user_mined'] == True:\n",
    "            data_reference[dr]['f_score'] -= 1 # a human user needed to access the resource\n",
    "        if data_reference[dr]['ret_code'] != 200:\n",
    "            if data_reference[dr]['ret_code'] in [0,404]:\n",
    "                data_reference[dr]['f_score'] -= 2 # there is some form of redirect to get to the  object\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data_reference) > 0:\n",
    "    csvh.write_csv_data(data_reference, 'pub_data_fairness.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessible\n",
    "Having an identifier and a link does not guarantee access. Resources may be behind walls (login, email to get it, or similar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusable\n",
    "Finding, retrieving and interpreting an object is not all ther is. For the resource to be reusable it needs to be a) licensed for use and b) in an appropriate format to guarantee long term support (related to 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperable\n",
    "Access to a resource does not guarantee interoperability, it is interoperable if the data is stored in a format which makes it easy to interpret by humans and machines. So an object in an open format is more interoperable that an object in a proprietary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for ChemDataExtractor\n",
    "# not used for mining data references (suplementary/raw) or to get pdf metadata\n",
    "from chemdataextractor import Document\n",
    "\n",
    "# A function for getting a list of files from the directory\n",
    "# This will be modified to get the list from a csv file\n",
    "def get_files_list (source_dir):\n",
    "    i_counter = 0\n",
    "    files_list = []\n",
    "    for filepath in sorted(source_dir.glob('*.pdf')):\n",
    "        i_counter += 1\n",
    "        files_list.append(filepath)\n",
    "    return files_list\n",
    "\n",
    "def cde_read_pdfs(a_file):\n",
    "    pdf_f = open(a_file, 'rb')\n",
    "    doc = Document.from_file(pdf_f)\n",
    "    return doc\n",
    "\n",
    "def find_doi(element_text):\n",
    "    cr_re_01 = '10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.search(cr_re_01, element_text, re.IGNORECASE)\n",
    "    if compare != None:\n",
    "        return compare.group()\n",
    "    return \"\"\n",
    "\n",
    "def get_db_id(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"id\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_db_title(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"title\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_close_dois(str_name, db_name = \"prev_search.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, doi, title, pdf_file\"\n",
    "    filter_str = \"doi like '%\"+str_name+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# functions for PDFminer\n",
    "\n",
    "def get_pdf_text(pdf_file):\n",
    "    return extract_text(pdf_file)\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_ref_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if pr_fns.is_data_stmt(sentence.lower()):\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1,idx,idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_all_data_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if 'data' in sentence.lower() or 'inform' in sentence.lower():\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1, idx, idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        if not full_sentence in return_group:\n",
    "            return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the http strings from references to data\n",
    "def get_http_ref(sentence):\n",
    "    http_frag = \"\"\n",
    "    if 'http' in sentence.lower():\n",
    "        idx_http = sentence.lower().index('http')\n",
    "        http_frag = sentence[idx_http:]\n",
    "        space_in_ref = True\n",
    "        while \" \" in http_frag:\n",
    "            space_idx = http_frag.rfind(\" \")\n",
    "            http_frag = http_frag[:space_idx]\n",
    "        if(http_frag[-1:]==\".\"):\n",
    "            http_frag = http_frag[:-1]\n",
    "    return http_frag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data_mentions) > 0:\n",
    "    csvh.write_csv_data(data_mentions, 'pdf_mentions_filtered_02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"db_files/app_db.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pdfminer to get metadata from pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get publication data from the ukch app\n",
    "db_pubs = pr_fns.get_pub_app_data(ukchapp_db)\n",
    "\n",
    "# get the list of dois already mined for data \n",
    "input_file = 'pub_data_all.csv'\n",
    "id_field = 'num'\n",
    "processed, headings = csvh.get_csv_data(input_file, id_field)\n",
    "for id_num in processed:\n",
    "    current_title = processed[id_num]['doi']\n",
    "processed[1]['num']\n",
    "\n",
    "processed_dois = []\n",
    "for entry in processed:\n",
    "    if not processed[entry]['doi'] in processed_dois:\n",
    "        processed_dois.append( processed[entry]['doi'])\n",
    "\n",
    "data_records = {}\n",
    "data_mentions = {}\n",
    "ref_count = mention_count = 0\n",
    "for a_pub in tqdm_notebook(db_pubs):\n",
    "    data_refs = []\n",
    "    data_sents = []\n",
    "    pub_id = a_pub[0]\n",
    "    pub_title = a_pub[1]\n",
    "    pub_doi = a_pub[2]\n",
    "    pub_url = a_pub[3]\n",
    "    pub_pdf = a_pub[4]\n",
    "    pub_html = a_pub[5]\n",
    "    if pub_pdf == 'None':\n",
    "        print(\"*************************\")\n",
    "        print(\"Missing PDF for:\", pub_doi)\n",
    "        print(\"*************************\")\n",
    "    else:\n",
    "        pdf_file = \"pdf_files/\" + pub_pdf\n",
    "        if not Path(pdf_file).is_file():\n",
    "            print(\"*************************\")\n",
    "            print(\"Missing file for:\", pdf_file, \"for\", pub_doi)\n",
    "            print(\"*************************\")\n",
    "        else: \n",
    "            print(\"PDF filename\", pdf_file)\n",
    "            pdf_text = get_pdf_text(pdf_file)\n",
    "            ref_sentences = get_ref_sentences(pdf_text)\n",
    "            data_sentences = get_all_data_sentences(pdf_text)\n",
    "            for r_sentence in ref_sentences:\n",
    "                dt_link = get_http_ref(r_sentence)\n",
    "                if 'supplem' in r_sentence.lower():\n",
    "                    data_refs.append({'type':'supplementary',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                else:\n",
    "                    data_refs.append({'type':'supporting',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "            for d_sentence in data_sentences:\n",
    "                dt_link = get_http_ref(d_sentence)\n",
    "                if 'supplem' in d_sentence.lower():\n",
    "                    data_sents.append({'type':'supplementary',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "                else:\n",
    "                    data_sents.append({'type':'supporting',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "    if data_refs != []:\n",
    "        for data_ref in data_refs:\n",
    "            data_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "            data_record.update(data_ref)\n",
    "            data_records[ref_count] = data_record\n",
    "            ref_count += 1\n",
    "    if data_sents != []:\n",
    "        for data_sent in data_sents:\n",
    "            sentence_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "            sentence_record.update(data_sent)\n",
    "            data_mentions[mention_count] = sentence_record\n",
    "            mention_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data_records) > 0:\n",
    "    csvh.write_csv_data(data_records, 'pdf_data.csv')\n",
    "    \n",
    "if len(data_mentions) > 0:\n",
    "    csvh.write_csv_data(data_mentions, 'pdf_mentions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names and links for references in data mentions\n",
    "data_mentions, dm_fields = csvh.get_csv_data('pdf_mentions_filtered_02.csv', 'num')\n",
    "\n",
    "for dm in data_mentions:\n",
    "    print(\"https://doi.org/\" + data_mentions[dm]['doi'])\n",
    "    ref_name = data_mentions[dm]['ref_name']\n",
    "    while ref_name == \"\":\n",
    "        print('Please enter the name of data object:')\n",
    "        ref_name = input()\n",
    "    ref_link = data_mentions[dm]['ref_link']\n",
    "    while ref_link == \"\":\n",
    "        print('Please enter the data object link:')\n",
    "        ref_link = input()\n",
    "    data_mentions[dm]['ref_name'] = ref_name\n",
    "    data_mentions[dm]['ref_link'] = ref_link\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
