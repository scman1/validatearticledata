{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are Data Objects referenced from publications FAIR\n",
    "\n",
    "A list of data objects referenced from a set of publications is checked to determine their FAIRness.\n",
    "\n",
    "Instead of only referencing to data, these tests are applied to **data objects**, which are any data which is published to complement the publication, this includes raw data, supplementary data, processing data, tables, images, movies, and compilations containing one or more of such resources.\n",
    "\n",
    "The tests to be performed are aimed at finding out if the data objects are: \n",
    " - **F**indable:  can the object be found easily?\n",
    " - **A**ccessible: can the object be retrieved?\n",
    " - **I**nteroperable: can the object be accesed programatically to extract data and metadata?\n",
    " - **R**eusable: can the object be readily used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "import tqdm\n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n",
    "# library for handling http requests\n",
    "import requests\n",
    "# library for accessing system functions\n",
    "import os\n",
    "# import custom functions (common to various notebooks)\n",
    "import processing_functions as pr_fns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findable\n",
    "\n",
    "Most of the data objects are assumed to be findable as we were able to find links to them. However, some are references to other pages, references to contact the authors or point to repositories without identifying a specific record.\n",
    "\n",
    "### Findability Score\n",
    "A findability score was calculated for each data object as follows:\n",
    "5 if the object is referenced from the publication web page, it is referenced directly, and further details from it can be recovered (name, type and size) just by accessing that reference. \n",
    "After this, for each additional step points are deucted from the top score, if to find to the referenced object:\n",
    "- Special access to the publication is needed (download the pdf, get a password or token for mining the publication, other blocks) \\[-1 point\\]\n",
    "- Human access to the publication online is required (there is no metadata or clear pattern to identify a reference on the publication landing page or the pdf version redirects to the article). \\[-1 point\\]\n",
    "- Recovering the reference object details (name, type and size) requires more than a single query. \\[-1 point\\]\n",
    "- The reference is wrong (broken link). \\[-2 points\\] \n",
    "- The reference points to contact the authors or lookup a data repository without an ID. \\[-4 points\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scman1\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1be97d27314956b696d75d424c8f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=374.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  1 5\n",
      "start  2 5\n",
      "start  3 5\n",
      "start  4 5\n",
      "start  5 5\n",
      "start  6 5\n",
      "start  7 5\n",
      "start  8 5\n",
      "start  9 5\n",
      "start  10 5\n",
      "start  11 5\n",
      "start  12 5\n",
      "start  13 5\n",
      "start  14 5\n",
      "start  15 5\n",
      "start  16 5\n",
      "start  17 5\n",
      "start  18 5\n",
      "start  19 5\n",
      "start  20 5\n",
      "start  21 5\n",
      "start  22 5\n",
      "start  23 5\n",
      "start  24 5\n",
      "start  25 5\n",
      "start  26 5\n",
      "start  27 5\n",
      "start  28 5\n",
      "start  29 5\n",
      "start  30 5\n",
      "start  31 5\n",
      "start  32 5\n",
      "start  33 5\n",
      "Article Link: https://doi.org/10.1007/s11244-018-0923-4\n",
      "Search for: Data Name: The datasets acquired during and/or analysed during the current study are available from the corresponding author on reasonable request. data link: \n",
      "Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "start  35 5\n",
      "start  36 5\n",
      "start  37 5\n",
      "start  38 5\n",
      "start  39 5\n",
      "start  40 5\n",
      "start  41 5\n",
      "start  42 5\n",
      "start  43 5\n",
      "start  44 5\n",
      "start  45 5\n",
      "start  46 5\n",
      "start  47 5\n",
      "start  48 5\n",
      "start  49 5\n",
      "start  50 5\n",
      "start  51 5\n",
      "start  52 5\n",
      "start  53 5\n",
      "start  54 5\n",
      "start  55 5\n",
      "start  56 5\n",
      "start  57 5\n",
      "start  58 5\n",
      "start  59 5\n",
      "start  60 5\n",
      "start  61 5\n",
      "start  62 5\n",
      "start  63 5\n",
      "start  64 5\n",
      "start  65 5\n",
      "start  66 5\n",
      "start  67 5\n",
      "start  68 5\n",
      "start  69 5\n",
      "start  70 5\n",
      "start  71 5\n",
      "start  72 5\n",
      "start  73 5\n",
      "start  74 5\n",
      "start  75 5\n",
      "start  76 5\n",
      "start  77 5\n",
      "start  78 5\n",
      "start  79 5\n",
      "start  80 5\n",
      "start  81 5\n",
      "start  82 5\n",
      "start  83 5\n",
      "start  84 5\n",
      "start  85 5\n",
      "start  86 5\n",
      "start  87 5\n",
      "start  88 5\n",
      "start  89 5\n",
      "start  90 5\n",
      "start  91 5\n",
      "start  92 5\n",
      "start  93 5\n",
      "start  94 5\n",
      "start  95 5\n",
      "start  96 5\n",
      "start  97 5\n",
      "start  98 5\n",
      "start  99 5\n",
      "start  100 5\n",
      "start  101 5\n",
      "start  102 5\n",
      "start  103 5\n",
      "start  104 5\n",
      "start  105 5\n",
      "start  106 5\n",
      "start  107 5\n",
      "start  108 5\n",
      "start  109 5\n",
      "start  110 5\n",
      "start  111 5\n",
      "start  112 5\n",
      "start  113 5\n",
      "start  114 5\n",
      "start  115 5\n",
      "start  116 5\n",
      "start  117 5\n",
      "start  118 5\n",
      "start  119 5\n",
      "start  120 5\n",
      "start  121 5\n",
      "start  122 5\n",
      "start  123 5\n",
      "start  124 5\n",
      "start  125 5\n",
      "start  126 5\n",
      "start  127 5\n",
      "start  128 5\n",
      "start  129 5\n",
      "start  130 5\n",
      "start  131 5\n",
      "start  132 5\n",
      "start  133 5\n",
      "start  134 5\n",
      "start  135 5\n",
      "start  136 5\n",
      "start  137 5\n",
      "start  138 5\n",
      "start  139 5\n",
      "start  140 5\n",
      "start  141 5\n",
      "start  142 5\n",
      "start  143 5\n",
      "start  144 5\n",
      "start  145 5\n",
      "start  146 5\n",
      "start  147 5\n",
      "start  148 5\n",
      "start  149 5\n",
      "start  150 5\n",
      "start  151 5\n",
      "start  152 5\n",
      "start  153 5\n",
      "start  154 5\n",
      "start  155 5\n",
      "start  156 5\n",
      "start  157 5\n",
      "start  158 5\n",
      "start  159 5\n",
      "start  160 5\n",
      "start  161 5\n",
      "start  162 5\n",
      "start  163 5\n",
      "start  164 5\n",
      "start  165 5\n",
      "start  166 5\n",
      "start  167 5\n",
      "start  168 5\n",
      "start  169 5\n",
      "start  170 5\n",
      "start  171 5\n",
      "start  172 5\n",
      "start  173 5\n",
      "start  174 5\n",
      "start  175 5\n",
      "start  176 5\n",
      "start  177 5\n",
      "start  178 5\n",
      "start  179 5\n",
      "start  180 5\n",
      "start  181 5\n",
      "start  182 5\n",
      "start  183 5\n",
      "start  184 5\n",
      "start  185 5\n",
      "start  186 5\n",
      "start  187 5\n",
      "start  188 5\n",
      "start  189 5\n",
      "start  190 5\n",
      "start  191 5\n",
      "start  192 5\n",
      "start  193 5\n",
      "start  194 5\n",
      "start  195 5\n",
      "start  196 5\n",
      "start  197 5\n",
      "start  198 5\n",
      "start  199 5\n",
      "start  200 5\n",
      "start  201 5\n",
      "start  202 5\n",
      "start  203 5\n",
      "start  204 5\n",
      "start  205 5\n",
      "start  206 5\n",
      "start  207 5\n",
      "start  208 5\n",
      "start  209 5\n",
      "start  210 5\n",
      "start  211 5\n",
      "start  212 5\n",
      "start  213 5\n",
      "start  214 5\n",
      "start  215 5\n",
      "start  216 5\n",
      "start  217 5\n",
      "start  218 5\n",
      "start  219 5\n",
      "start  220 5\n",
      "start  221 5\n",
      "start  222 5\n",
      "start  223 5\n",
      "start  224 5\n",
      "start  225 5\n",
      "start  226 5\n",
      "start  227 5\n",
      "start  228 5\n",
      "start  229 5\n",
      "start  230 5\n",
      "Article Link: https://doi.org/10.1038/s41929-018-0206-2\n",
      "Search for: Data Name: All the figures presented in this paper are associated with raw data. The data that support the plots within this paper and other findings of this study are available from the corresponding author upon reasonable request. data link: \n",
      "Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "start  232 5\n",
      "start  233 5\n",
      "start  234 5\n",
      "start  235 5\n",
      "start  236 5\n",
      "start  237 5\n",
      "start  238 5\n",
      "start  239 5\n",
      "start  240 5\n",
      "start  241 5\n",
      "start  242 5\n",
      "start  243 5\n",
      "start  244 5\n",
      "start  245 5\n",
      "start  246 5\n",
      "start  247 5\n",
      "start  248 5\n",
      "start  249 5\n",
      "start  250 5\n",
      "start  251 5\n",
      "start  252 5\n",
      "start  253 5\n",
      "start  254 5\n",
      "Article Link: https://doi.org/10.1039/c8cc01880d\n",
      "Search for: Data Name: 10.17861/14c23fe6-bc65-4806-ba5e-63642a6ad3e9 data link: http://10.0.69.197/14c23fe6-bc65-4806-ba5e-63642a6ad3e9\n",
      "HTTPConnectionPool(host='10.0.69.197', port=80): Max retries exceeded with url: /14c23fe6-bc65-4806-ba5e-63642a6ad3e9 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x010FDF90>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond',))\n",
      "start  256 5\n",
      "start  257 5\n",
      "start  258 5\n",
      "start  259 5\n",
      "start  260 5\n",
      "start  261 5\n",
      "start  262 5\n",
      "start  263 5\n",
      "start  264 5\n",
      "start  265 5\n",
      "start  266 5\n",
      "start  267 5\n",
      "start  268 5\n",
      "start  269 5\n",
      "start  270 5\n",
      "start  271 5\n",
      "start  272 5\n",
      "start  273 5\n",
      "start  274 5\n",
      "start  275 5\n",
      "start  276 5\n",
      "start  277 5\n",
      "start  278 5\n",
      "start  279 5\n",
      "start  280 5\n",
      "start  281 5\n",
      "start  282 5\n",
      "start  283 5\n",
      "start  284 5\n",
      "start  285 5\n",
      "start  286 5\n",
      "start  287 5\n",
      "start  288 5\n",
      "start  289 5\n",
      "start  290 5\n",
      "start  291 5\n",
      "start  292 5\n",
      "start  293 5\n",
      "start  294 5\n",
      "start  295 5\n",
      "Article Link: https://doi.org/10.1038/s41467-020-15445-z\n",
      "Search for: Data Name: The data that support the plots in this paper and the other findings of this study are available from the corresponding authors on reasonable request. data link: \n",
      "Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "start  297 5\n",
      "start  298 5\n",
      "start  299 5\n",
      "start  300 5\n",
      "start  301 5\n",
      "start  302 5\n",
      "start  303 5\n",
      "start  304 5\n",
      "start  305 5\n",
      "start  306 5\n",
      "start  307 5\n",
      "start  308 5\n",
      "Article Link: https://doi.org/10.1038/s41563-019-0562-6\n",
      "Search for: Data Name: All the relevant data are available from the authors, and/or are included with the manuscript. data link: \n",
      "Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "start  310 5\n",
      "start  311 5\n",
      "start  312 5\n",
      "start  313 5\n",
      "start  314 5\n",
      "start  315 5\n",
      "start  316 5\n",
      "start  317 5\n",
      "start  318 5\n",
      "start  319 5\n",
      "start  320 5\n",
      "start  321 5\n",
      "start  322 5\n",
      "start  323 5\n",
      "start  324 5\n",
      "start  325 5\n",
      "start  326 5\n",
      "start  327 5\n",
      "start  328 5\n",
      "start  329 5\n",
      "start  330 5\n",
      "start  331 5\n",
      "start  332 5\n",
      "start  333 5\n",
      "start  334 5\n",
      "start  335 5\n",
      "start  336 5\n",
      "Article Link: https://doi.org/10.1007/s11244-020-01258-3\n",
      "Search for: Data Name: The datasets generated during and/or analysed during the current study are available from the corresponding author on reasonable request. data link: \n",
      "Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "start  338 5\n",
      "start  339 5\n",
      "start  340 5\n",
      "start  341 5\n",
      "start  342 5\n",
      "start  343 5\n",
      "start  344 5\n",
      "start  345 5\n",
      "start  346 5\n",
      "start  347 5\n",
      "start  348 5\n",
      "start  349 5\n",
      "start  350 5\n",
      "start  351 5\n",
      "start  352 5\n",
      "start  353 5\n",
      "start  354 5\n",
      "start  355 5\n",
      "start  356 5\n",
      "start  357 5\n",
      "start  358 5\n",
      "start  359 5\n",
      "start  360 5\n",
      "start  361 5\n",
      "start  362 5\n",
      "start  363 5\n",
      "start  364 5\n",
      "start  365 5\n",
      "start  366 5\n",
      "start  367 5\n",
      "start  368 5\n",
      "start  369 5\n",
      "Article Link: https://doi.org/10.1098/rspa.2016.0054\n",
      "Search for: Data Name: Cardiff University's data catalogue opendata@cardiff.ac.uk data link: \n",
      "Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "start  371 5\n",
      "start  372 5\n",
      "start  373 5\n",
      "start  374 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get names and links for references in data mentions\n",
    "data_reference, _ = csvh.get_csv_data('pub_data_fairness.csv', 'num')\n",
    "\n",
    "for dr in tqdm_notebook(data_reference):\n",
    "    if data_reference[dr]['ret_code'] == \"\":\n",
    "        # try to get data object details from reference\n",
    "        print(\"Article Link: https://doi.org/\" + data_reference[dr]['doi'])\n",
    "        ref_name = data_reference[dr]['name']\n",
    "        ref_link = data_reference[dr]['data_url']\n",
    "        print(\"Search for: Data Name:\", ref_name, \"data link:\", ref_link)\n",
    "        head = urlh.getPageHeader(ref_link)\n",
    "\n",
    "        if head != None:\n",
    "            data_reference[dr]['ret_code'] = head.status_code \n",
    "            data_reference[dr]['resoruce_name'] = os.path.basename(head.url)\n",
    "            if head.status_code == 200:\n",
    "                #print (head.headers, head.url)\n",
    "                if 'content-type' in head.headers.keys():\n",
    "                    data_reference[dr]['ref_content'] = head.headers['content-type']\n",
    "                if 'content-length' in head.headers.keys():\n",
    "                    data_reference[dr]['ref_size'] = head.headers['content-length']\n",
    "                data_reference[dr]['ref_redirect'] = head.url\n",
    "            elif head.status_code == 302 or head.status_code == 301:\n",
    "                #print(head, head.headers)\n",
    "                data_reference[dr]['ref_redirect'] = head.headers['location']\n",
    "                data_reference[dr]['resoruce_name'] = os.path.basename(head.headers['location'])\n",
    "            else:\n",
    "                print(head, head.headers)\n",
    "        else:\n",
    "            data_reference[dr]['f_score'] = 1\n",
    "    else:\n",
    "        data_reference[dr]['f_score'] = 5\n",
    "        print (\"start \", dr, data_reference[dr]['f_score'])\n",
    "        if data_reference[dr]['html_mined'] == 'FALSE' and data_reference[dr]['pdf_mined'] == 'TRUE':\n",
    "            data_reference[dr]['f_score'] -= 1 # the publication page is not accessible directly to get the DO \n",
    "            #print (\"deduct pdf mined\", dr, data_reference[dr]['f_score'])\n",
    "        if data_reference[dr]['html_mined'] == 'FALSE' and data_reference[dr]['user_mined'] == 'TRUE':\n",
    "            data_reference[dr]['f_score'] -= 1 # a human user needed to access the resource\n",
    "            #print (\"deduct manually mined\", dr, data_reference[dr]['f_score'])\n",
    "        if data_reference[dr]['ret_code'] in ['0','404']:\n",
    "            data_reference[dr]['f_score'] -= 2 # there is a problem with the link\n",
    "            #print (\"deduct page not found\", dr, data_reference[dr]['f_score'])\n",
    "        elif data_reference[dr]['ret_code'] != '200' and not 'doi.org' in data_reference[dr]['data_url'].lower():\n",
    "            # dois always redirect\n",
    "            data_reference[dr]['f_score'] -= 1 # there is some form of redirect to get to the  object\n",
    "            #print (\"deduct page redirect\", dr, data_reference[dr]['f_score'], data_reference[dr]['ret_code'])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data_reference) > 0:\n",
    "    csvh.write_csv_data(data_reference, 'pub_data_fairness.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessible\n",
    "Having an identifier and a link does not guarantee access. Resources may be behind walls (login, redirections, email owner to get it, or similar). This is tranlated as: Can we get the resource? Again this is not a yes or no question. Getting the resource means that once the resource is at one's disposal.\n",
    "\n",
    "### Accessibility score\n",
    "An accessibility  score was calculated for each data object as follows: 5 if the object referenced allows direct download of the object just by accessing that reference. After this, for each additional step points are deucted from the top score, if to obtain to the referenced object:\n",
    "\n",
    "- Special access to the publication is need (get a password or token for mining the publication, or similar acces blocks) [-1 point]\n",
    "- Human access to the publication online is required (there is no metadata or clear pattern to identify a reference on the publication landing page or the pdf version redirects to the article). [-1 point]\n",
    "- Recovering the reference object details (name, type and size) requires more than a single query. [-1 point]\n",
    "- The reference is wrong (broken link). [-2 points]\n",
    "- The reference points to contact the authors or lookup a data repository without an ID. [-4 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusable\n",
    "Finding, retrieving and interpreting an object is not all ther is. For the resource to be reusable it needs to be a) licensed for use and b) in an appropriate format to guarantee long term support (related to 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperable\n",
    "Access to a resource does not guarantee interoperability, it is interoperable if the data is stored in a format which makes it easy to interpret by humans and machines. So an object in an open format is more interoperable that an object in a proprietary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for ChemDataExtractor\n",
    "# not used for mining data references (suplementary/raw) or to get pdf metadata\n",
    "from chemdataextractor import Document\n",
    "\n",
    "# A function for getting a list of files from the directory\n",
    "# This will be modified to get the list from a csv file\n",
    "def get_files_list (source_dir):\n",
    "    i_counter = 0\n",
    "    files_list = []\n",
    "    for filepath in sorted(source_dir.glob('*.pdf')):\n",
    "        i_counter += 1\n",
    "        files_list.append(filepath)\n",
    "    return files_list\n",
    "\n",
    "def cde_read_pdfs(a_file):\n",
    "    pdf_f = open(a_file, 'rb')\n",
    "    doc = Document.from_file(pdf_f)\n",
    "    return doc\n",
    "\n",
    "def find_doi(element_text):\n",
    "    cr_re_01 = '10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.search(cr_re_01, element_text, re.IGNORECASE)\n",
    "    if compare != None:\n",
    "        return compare.group()\n",
    "    return \"\"\n",
    "\n",
    "def get_db_id(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"id\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_db_title(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"title\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_close_dois(str_name, db_name = \"prev_search.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, doi, title, pdf_file\"\n",
    "    filter_str = \"doi like '%\"+str_name+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# functions for PDFminer\n",
    "\n",
    "def get_pdf_text(pdf_file):\n",
    "    return extract_text(pdf_file)\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_ref_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if pr_fns.is_data_stmt(sentence.lower()):\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1,idx,idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_all_data_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if 'data' in sentence.lower() or 'inform' in sentence.lower():\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1, idx, idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        if not full_sentence in return_group:\n",
    "            return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the http strings from references to data\n",
    "def get_http_ref(sentence):\n",
    "    http_frag = \"\"\n",
    "    if 'http' in sentence.lower():\n",
    "        idx_http = sentence.lower().index('http')\n",
    "        http_frag = sentence[idx_http:]\n",
    "        space_in_ref = True\n",
    "        while \" \" in http_frag:\n",
    "            space_idx = http_frag.rfind(\" \")\n",
    "            http_frag = http_frag[:space_idx]\n",
    "        if(http_frag[-1:]==\".\"):\n",
    "            http_frag = http_frag[:-1]\n",
    "    return http_frag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data_mentions) > 0:\n",
    "    csvh.write_csv_data(data_mentions, 'pdf_mentions_filtered_02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"db_files/app_db.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pdfminer to get metadata from pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get publication data from the ukch app\n",
    "db_pubs = pr_fns.get_pub_app_data(ukchapp_db)\n",
    "\n",
    "# get the list of dois already mined for data \n",
    "input_file = 'pub_data_all.csv'\n",
    "id_field = 'num'\n",
    "processed, headings = csvh.get_csv_data(input_file, id_field)\n",
    "for id_num in processed:\n",
    "    current_title = processed[id_num]['doi']\n",
    "processed[1]['num']\n",
    "\n",
    "processed_dois = []\n",
    "for entry in processed:\n",
    "    if not processed[entry]['doi'] in processed_dois:\n",
    "        processed_dois.append( processed[entry]['doi'])\n",
    "\n",
    "data_records = {}\n",
    "data_mentions = {}\n",
    "ref_count = mention_count = 0\n",
    "for a_pub in tqdm_notebook(db_pubs):\n",
    "    data_refs = []\n",
    "    data_sents = []\n",
    "    pub_id = a_pub[0]\n",
    "    pub_title = a_pub[1]\n",
    "    pub_doi = a_pub[2]\n",
    "    pub_url = a_pub[3]\n",
    "    pub_pdf = a_pub[4]\n",
    "    pub_html = a_pub[5]\n",
    "    if pub_pdf == 'None':\n",
    "        print(\"*************************\")\n",
    "        print(\"Missing PDF for:\", pub_doi)\n",
    "        print(\"*************************\")\n",
    "    else:\n",
    "        pdf_file = \"pdf_files/\" + pub_pdf\n",
    "        if not Path(pdf_file).is_file():\n",
    "            print(\"*************************\")\n",
    "            print(\"Missing file for:\", pdf_file, \"for\", pub_doi)\n",
    "            print(\"*************************\")\n",
    "        else: \n",
    "            print(\"PDF filename\", pdf_file)\n",
    "            pdf_text = get_pdf_text(pdf_file)\n",
    "            ref_sentences = get_ref_sentences(pdf_text)\n",
    "            data_sentences = get_all_data_sentences(pdf_text)\n",
    "            for r_sentence in ref_sentences:\n",
    "                dt_link = get_http_ref(r_sentence)\n",
    "                if 'supplem' in r_sentence.lower():\n",
    "                    data_refs.append({'type':'supplementary',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                else:\n",
    "                    data_refs.append({'type':'supporting',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "            for d_sentence in data_sentences:\n",
    "                dt_link = get_http_ref(d_sentence)\n",
    "                if 'supplem' in d_sentence.lower():\n",
    "                    data_sents.append({'type':'supplementary',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "                else:\n",
    "                    data_sents.append({'type':'supporting',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "    if data_refs != []:\n",
    "        for data_ref in data_refs:\n",
    "            data_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "            data_record.update(data_ref)\n",
    "            data_records[ref_count] = data_record\n",
    "            ref_count += 1\n",
    "    if data_sents != []:\n",
    "        for data_sent in data_sents:\n",
    "            sentence_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "            sentence_record.update(data_sent)\n",
    "            data_mentions[mention_count] = sentence_record\n",
    "            mention_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data_records) > 0:\n",
    "    csvh.write_csv_data(data_records, 'pdf_data.csv')\n",
    "    \n",
    "if len(data_mentions) > 0:\n",
    "    csvh.write_csv_data(data_mentions, 'pdf_mentions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names and links for references in data mentions\n",
    "data_mentions, dm_fields = csvh.get_csv_data('pdf_mentions_filtered_02.csv', 'num')\n",
    "\n",
    "for dm in data_mentions:\n",
    "    print(\"https://doi.org/\" + data_mentions[dm]['doi'])\n",
    "    ref_name = data_mentions[dm]['ref_name']\n",
    "    while ref_name == \"\":\n",
    "        print('Please enter the name of data object:')\n",
    "        ref_name = input()\n",
    "    ref_link = data_mentions[dm]['ref_link']\n",
    "    while ref_link == \"\":\n",
    "        print('Please enter the data object link:')\n",
    "        ref_link = input()\n",
    "    data_mentions[dm]['ref_name'] = ref_name\n",
    "    data_mentions[dm]['ref_link'] = ref_link\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
