{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2203ecf9",
   "metadata": {},
   "source": [
    "# Sinlge Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8876949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# single layer perceptron\n",
    "from sklearn.linear_model import Perceptron\n",
    "# multilayer perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# handle  csv read and write\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "# managing files and file paths\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd395e9f",
   "metadata": {},
   "source": [
    "### Get test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0f10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainTestData(pdf_data_search_dir, in_name):\n",
    "    data_mentions, dm_headers = csvh.get_csv_data(Path(pdf_data_search_dir, in_name+'.csv'))\n",
    "    corpus = []\n",
    "    targets = []\n",
    "    for a_line in data_mentions:\n",
    "        corpus.append(data_mentions[a_line]['desc'])\n",
    "        targets.append(int(data_mentions[a_line]['DataStatement']))\n",
    "    # Create the training and test sets\n",
    "    \n",
    "    # split the dataset\n",
    "    train_features, test_features, train_targets, test_targets = train_test_split(corpus,targets,test_size=0.2, random_state = 123)\n",
    "    # Turn the corpus into a tf-idf array\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, norm='l1')\n",
    "    train_features = vectorizer.fit_transform(train_features)\n",
    "    test_features = vectorizer.transform(test_features)\n",
    "    \n",
    "    return train_features, test_features, train_targets, test_targets, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f231f75",
   "metadata": {},
   "source": [
    "### Build the single layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b55f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildPerceptron(train_features, train_targets):\n",
    "    classifier = Perceptron(random_state=457)\n",
    "    classifier.fit (train_features, train_targets)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20a2e8",
   "metadata": {},
   "source": [
    "#### Train and test slp perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d8905b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildAndTrainSLP(train_features, train_targets):\n",
    "    slp_classifier = buildPerceptron(train_features, train_targets)\n",
    "\n",
    "    predictions = slp_classifier.predict(test_features)\n",
    "\n",
    "    score = np.round(metrics.accuracy_score(test_targets, predictions),2)\n",
    "\n",
    "    print(\"SLP Mean accuracy of predictions: \"+ str(score))\n",
    "    return slp_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a1853",
   "metadata": {},
   "source": [
    "### Build and test Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "016ebf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildMLPerceptron(train_features, test_features, train_targets, test_targets, num_neurons = 2):\n",
    "    # Build a MultiLayer perceptron and fit the data\n",
    "    # Activation function: ReLU\n",
    "    # Optimisation Function: Stochastic Gradient Descent (SGD)\n",
    "    # Learning Rate: Inverse Scaling\n",
    "    classifier = MLPClassifier(hidden_layer_sizes = num_neurons, max_iter=100, activation='relu', solver='sgd', verbose=10, random_state=762, learning_rate='invscaling')\n",
    "    classifier.fit(train_features,train_targets)\n",
    "    \n",
    "    predictions = classifier.predict(test_features)\n",
    "    \n",
    "    score = np.round(metrics.accuracy_score(test_targets, predictions),2)\n",
    "\n",
    "    print(\"MLP Mean accuracy of predictions: \"+ str(score))\n",
    "    \n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3553ad10",
   "metadata": {},
   "source": [
    "### Build classifier models using prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c7d8fb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLP Mean accuracy of predictions: 0.91\n",
      "Iteration 1, loss = 1.23456292\n",
      "Iteration 2, loss = 1.21764512\n",
      "Iteration 3, loss = 1.20916983\n",
      "Iteration 4, loss = 1.20550058\n",
      "Iteration 5, loss = 1.20378328\n",
      "Iteration 6, loss = 1.20287882\n",
      "Iteration 7, loss = 1.20229993\n",
      "Iteration 8, loss = 1.20187387\n",
      "Iteration 9, loss = 1.20152206\n",
      "Iteration 10, loss = 1.20120911\n",
      "Iteration 11, loss = 1.20092391\n",
      "Iteration 12, loss = 1.20065496\n",
      "Iteration 13, loss = 1.20040020\n",
      "Iteration 14, loss = 1.20015939\n",
      "Iteration 15, loss = 1.19992610\n",
      "Iteration 16, loss = 1.19970475\n",
      "Iteration 17, loss = 1.19948813\n",
      "Iteration 18, loss = 1.19928119\n",
      "Iteration 19, loss = 1.19907924\n",
      "Iteration 20, loss = 1.19888476\n",
      "Iteration 21, loss = 1.19869460\n",
      "Iteration 22, loss = 1.19850911\n",
      "Iteration 23, loss = 1.19832928\n",
      "Iteration 24, loss = 1.19815462\n",
      "Iteration 25, loss = 1.19798167\n",
      "Iteration 26, loss = 1.19781320\n",
      "Iteration 27, loss = 1.19764875\n",
      "Iteration 28, loss = 1.19748713\n",
      "Iteration 29, loss = 1.19732863\n",
      "Iteration 30, loss = 1.19717234\n",
      "Iteration 31, loss = 1.19702060\n",
      "Iteration 32, loss = 1.19687066\n",
      "Iteration 33, loss = 1.19672273\n",
      "Iteration 34, loss = 1.19657664\n",
      "Iteration 35, loss = 1.19643426\n",
      "Iteration 36, loss = 1.19629368\n",
      "Iteration 37, loss = 1.19615480\n",
      "Iteration 38, loss = 1.19601906\n",
      "Iteration 39, loss = 1.19588305\n",
      "Iteration 40, loss = 1.19575028\n",
      "Iteration 41, loss = 1.19561954\n",
      "Iteration 42, loss = 1.19548981\n",
      "Iteration 43, loss = 1.19536156\n",
      "Iteration 44, loss = 1.19523636\n",
      "Iteration 45, loss = 1.19511046\n",
      "Iteration 46, loss = 1.19498878\n",
      "Iteration 47, loss = 1.19486613\n",
      "Iteration 48, loss = 1.19474610\n",
      "Iteration 49, loss = 1.19462578\n",
      "Iteration 50, loss = 1.19450758\n",
      "Iteration 51, loss = 1.19439168\n",
      "Iteration 52, loss = 1.19427583\n",
      "Iteration 53, loss = 1.19416089\n",
      "Iteration 54, loss = 1.19404742\n",
      "Iteration 55, loss = 1.19393585\n",
      "Iteration 56, loss = 1.19382547\n",
      "Iteration 57, loss = 1.19371463\n",
      "Iteration 58, loss = 1.19360620\n",
      "Iteration 59, loss = 1.19349849\n",
      "Iteration 60, loss = 1.19339082\n",
      "Iteration 61, loss = 1.19328540\n",
      "Iteration 62, loss = 1.19317992\n",
      "Iteration 63, loss = 1.19307519\n",
      "Iteration 64, loss = 1.19297247\n",
      "Iteration 65, loss = 1.19286957\n",
      "Iteration 66, loss = 1.19276783\n",
      "Iteration 67, loss = 1.19266733\n",
      "Iteration 68, loss = 1.19256646\n",
      "Iteration 69, loss = 1.19246705\n",
      "Iteration 70, loss = 1.19236816\n",
      "Iteration 71, loss = 1.19227108\n",
      "Iteration 72, loss = 1.19217343\n",
      "Iteration 73, loss = 1.19207651\n",
      "Iteration 74, loss = 1.19198136\n",
      "Iteration 75, loss = 1.19188583\n",
      "Iteration 76, loss = 1.19179159\n",
      "Iteration 77, loss = 1.19169719\n",
      "Iteration 78, loss = 1.19160361\n",
      "Iteration 79, loss = 1.19151187\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP Mean accuracy of predictions: 0.24\n"
     ]
    }
   ],
   "source": [
    "# get train and test data\n",
    "\n",
    "# working directory\n",
    "pdf_data_search_dir = \"./data_search_pdf_b\"\n",
    "\n",
    "# file containing train and test data\n",
    "train_test_file =  'test_train01_c'\n",
    "\n",
    "train_features, test_features, train_targets, test_targets, vectorizer = getTrainTestData(pdf_data_search_dir, train_test_file)\n",
    "\n",
    "\n",
    "# build and train models\n",
    "# single layer\n",
    "slp_classifier = buildAndTrainSLP(train_features, train_targets)\n",
    "# multilayer\n",
    "mlp_classifier =  buildMLPerceptron(train_features, test_features, train_targets, test_targets, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd34b5",
   "metadata": {},
   "source": [
    "### Use the generated models to predict on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7f765b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get evalutation data\n",
    "pdf_data_search_dir = \"./data_search_pdf_b\"\n",
    "\n",
    "start_from = 601\n",
    "stop_at = 700\n",
    "\n",
    "eval_file = \"pdf_mentions_\" + str(start_from).zfill(4) + \"_\" + str(stop_at).zfill(4)\n",
    "\n",
    "res_file = eval_file + \"_pre_res\"\n",
    "\n",
    "eval_mentions, eval_headers = csvh.get_csv_data(Path(pdf_data_search_dir, eval_file+'.csv'))\n",
    "\n",
    "eval_data = [eval_mentions[dic]['desc'] for dic in eval_mentions]\n",
    "\n",
    "eval_features = vectorizer.transform(eval_data)\n",
    "eval_prediction_slp = slp_classifier.predict(eval_features)\n",
    "eval_prediction_mlp = mlp_classifier.predict(eval_features)\n",
    "\n",
    "current_pub = 0\n",
    "for data, slp_pred, mlp_pred in zip(eval_mentions, eval_prediction_slp, eval_prediction_mlp):        \n",
    "    eval_mentions[data]['slp_pred'] = slp_pred\n",
    "    eval_mentions[data]['mlp_pred'] = mlp_pred\n",
    "    eval_mentions[data]['true_val'] = 0 # placeholder for manual evaluation\n",
    "    # mark the first and last lines as new candidates regardles of their value\n",
    "    if eval_mentions[data]['id'] != current_pub:\n",
    "        if current_pub != 0 :\n",
    "            eval_mentions[data]['add'] = 1\n",
    "            eval_mentions[data-1]['add'] = 1\n",
    "        else:\n",
    "            eval_mentions[data]['add'] = 1\n",
    "        current_pub = eval_mentions[data]['id']\n",
    "        \n",
    "\n",
    "csvh.write_csv_data(eval_mentions, Path(pdf_data_search_dir,  res_file +'.csv'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6fc9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1312c88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
