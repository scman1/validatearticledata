{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732dd680",
   "metadata": {},
   "source": [
    "# Look up in the scholix registry\n",
    "\n",
    "Read DOIs from rails app. Look up each DOI in the Scholix registry and save links to a csv file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a167a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library containign read and write functions to csv file\n",
    "import lib.handle_csv as csvh\n",
    "\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "\n",
    "# import custom functions (common to various notebooks)\n",
    "import processing_functions as pr_fns\n",
    "\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "from tqdm import tqdm\n",
    "\n",
    "# regular expressions\n",
    "import re\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e3756ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of already searched DOIs\n",
    "doi_list = ['10.1002/chem.202000067', '10.1016/j.jcat.2018.01.033', '10.1021/acscatal.9b03889', \n",
    "            '10.1039/d0cp01227k', '10.1039/d0cy01061h', '10.1098/rsta.2020.0058', '10.1098/rsta.2020.0063', \n",
    "            '10.1039/D0CY01608J', '10.1021/acs.est.0c04279', '10.1039/D0CP01192D', '10.1039/d0cy01779e', \n",
    "            '10.1021/acsenergylett.0c02614', '10.1039/d1fd00004g', '10.3390/catal10121370', \n",
    "            '10.1039/d1gc00901j', '10.1038/s41467-021-21062-1', '10.1021/acscatal.0c05413',\n",
    "            '10.1021/acscatal.0c04858', '10.1088/1361-648x/abfe16', '10.1088/1361-6463/abe9e1', \n",
    "            '10.1039/d0sc03113e', '10.1007/s11244-021-01447-8', '10.1021/acs.organomet.1c00055', \n",
    "            '10.1021/acscatal.0c05019', '10.1021/acs.inorgchem.1c00327', '10.1002/smsc.202100032', \n",
    "            '10.1039/d0gc02295k', '10.1002/anie.201901592', '10.1021/acs.organomet.9b00845', \n",
    "            '10.1021/jacs.9b13106', '10.1002/anie.202006807', '10.1021/jacs.0c07980', '10.1039/d0cy01484b',\n",
    "            '10.1039/d0cy02164d', '10.1002/anie.202101180', '10.1002/chem.202101140', \n",
    "            '10.1021/acsmacrolett.1c00216', '10.1002/anie.201810245', '10.1039/c9sc00385a', \n",
    "            '10.1021/acs.macromol.8b01224', '10.1039/c9dt02918d', '10.1038/s41467-019-10481-w', \n",
    "            '10.1002/ange.201901592', '10.1039/c9dt00595a', '10.1039/d1cy00238d', \n",
    "            '10.1021/acs.inorgchem.8b02923', '10.1002/ange.202006807', '10.1002/anie.201814320', \n",
    "            '10.1007/s10562-019-02876-7', '10.1021/acs.jpcc.9b09050', '10.1016/j.apcatb.2017.01.042',\n",
    "            '10.1039/d0cc04036c', '10.1002/anie.202015016', '10.1039/d1ta01464a', '10.1002/smtd.202100512',\n",
    "            '10.1107/s1600576720013576', '10.1039/d0cp00793e', '10.1039/d0ta01398f', \n",
    "            '10.1007/s11244-021-01450-z', '10.1039/d0ta08351h', '10.1021/acssuschemeng.1c01451',\n",
    "            '10.1002/cphc.201800721', '10.1021/acssuschemeng.8b04073', '10.1002/cctc.202100286', \n",
    "            '10.1007/s11244-020-01245-8', '10.1021/acscatal.0c03620', '10.1016/j.cattod.2018.06.033', \n",
    "            '10.1016/j.apcatb.2020.118752', '10.1016/j.joule.2020.07.024', '10.1002/anie.201814381', \n",
    "            '10.1002/ange.201902857']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1c6a6",
   "metadata": {},
   "source": [
    "## Search for references direclty in scholexplorer\n",
    "The next code makes a search of scholix references to data using the scholexeplorer of OpenAire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39e62aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up in sholix\n",
    "# Get pdf and html name from previous and put it in current\n",
    "def search_scolix(db_name, work_dir, start_from = 0, cut_date=\"202408\"):\n",
    "    out_name = \"search_scholix_\"+db_name+\"_\"+cut_date\n",
    "    out_file = Path(work_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"Already searched for\", db_name)\n",
    "        return out_name\n",
    "    data_links = {}\n",
    "    a_dl = {}\n",
    "    url_base = 'http://api.scholexplorer.openaire.eu/v3/Links?sourcePid='\n",
    "    ignore_types = ['References','IsReferencedBy'] #test reading all references and see what it comes out\n",
    "    ignore_subtypes = ['IsCitedBy', 'cites', 'Cites','References','IsReferencedBy']\n",
    "    terminate = False\n",
    "\n",
    "    for a_pub in tqdm_notebook(app_pubs):\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        match_found = False\n",
    "        if pr_fns.valid_doi(pub_doi) and pub_id > start_from:\n",
    "            response = urlh.getPageFromURL(url_base + pub_doi.replace('/','%2f'))\n",
    "            data_results = json.loads(response)\n",
    "            id_dl = len(data_links)\n",
    "            for a_result in data_results['result']:\n",
    "                if not a_result['RelationshipType']['Name'] in ignore_types:\n",
    "                    if 'SubType' in a_result['RelationshipType'].keys() \\\n",
    "                      and not a_result['RelationshipType']['SubType'] in ignore_subtypes:                        \n",
    "                        id_dl += 1\n",
    "                        source_doi = pub_doi\n",
    "                        source_title = a_result['source']['Title'].replace('\\n',' ')\n",
    "                        source_published = a_result['source']['PublicationDate']\n",
    "                        target_id = a_result['target']['Identifier'][0]['ID']\n",
    "                        target_url = a_result['target']['Identifier'][0]['IDURL']\n",
    "                        target_type = a_result['target']['Type']\n",
    "                        if not pr_fns.valid_doi(target_id) and target_type != 'literature':\n",
    "                            if a_result['target']['Identifier'][0]['IDScheme'] in ['uniprot','pdb']:\n",
    "                                target_id = a_result['target']['Identifier'][0]['IDURL']\n",
    "                            else:\n",
    "                                for an_id in a_result['target']['Identifier']:\n",
    "                                    print (\"source\", source_doi, \"title\", source_title)\n",
    "                                    print (an_id)\n",
    "                                terminate = True    \n",
    "                        target_title = a_result['target']['Title'].replace('\\n',' ')\n",
    "                        target_published = a_result['target']['PublicationDate']\n",
    "\n",
    "                        rel_type = a_result['RelationshipType']['Name']\n",
    "                        rel_subtype = a_result['RelationshipType']['SubType']\n",
    "                        a_dl = {\"pub_id\": pub_id,\"pub_doi\":source_doi,'source_title':source_title, 'source_published':source_published,\n",
    "                                'target_id':target_id, 'target_title':target_title, \n",
    "                                'target_published': target_published, 'rel_type': rel_type, 'rel_subtype':rel_subtype}\n",
    "                        data_links[id_dl]=a_dl\n",
    "        if terminate:\n",
    "            break\n",
    "    print ('References found:', len(data_links))\n",
    "\n",
    "    \n",
    "    if len(data_links) > 0  and not terminate:\n",
    "        csvh.write_csv_data(data_links, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce15460c",
   "metadata": {},
   "source": [
    "## Remove duplicates\n",
    "search for potential duplicates in list (hapens when same target with different relationship tipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c4d9d4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "# search for potential duplicates in list (hapens when same target with different relationship types)\n",
    "def remove_dups(srf_name, work_dir):\n",
    "    print(srf_name)\n",
    "    in_file = Path(work_dir, srf_name + \".csv\")\n",
    "    out_name = srf_name + \"_no_dups\"\n",
    "    out_file = Path(work_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"already removed duplicates from:\", srf_name)\n",
    "        return out_name\n",
    "    data_links = {}\n",
    "    a_dl = {}\n",
    "    scholix_references, column_names = csvh.get_csv_data(in_file)\n",
    "\n",
    "    scholix_look_up, look_up_names = csvh.get_csv_data(in_file)\n",
    "\n",
    "    for a_ref in tqdm_notebook(scholix_references):\n",
    "        for look_up in scholix_look_up:\n",
    "            if scholix_look_up[look_up] != scholix_references[a_ref] \\\n",
    "               and not 'duplicate' in scholix_references[a_ref].keys():\n",
    "                if scholix_look_up[look_up]['pub_doi'] == scholix_references[a_ref]['pub_doi'] \\\n",
    "                   and scholix_look_up[look_up]['target_id'] == scholix_references[a_ref]['target_id'] \\\n",
    "                   and scholix_look_up[look_up]['target_title'] == scholix_references[a_ref]['target_title']:\n",
    "                    scholix_references[look_up]['duplicate'] = 'TRUE'\n",
    "                    print (a_ref ,scholix_references[a_ref])\n",
    "                    print (look_up, scholix_look_up[look_up])\n",
    "        if not 'duplicate' in scholix_references[a_ref].keys():\n",
    "            scholix_references[a_ref]['duplicate'] = 'FALSE'\n",
    "            with open(out_file, 'a', newline='',encoding='utf8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(scholix_references[a_ref].values())\n",
    "    return out_name\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17236d",
   "metadata": {},
   "source": [
    "## Check if pdf mentions are on DB\n",
    "Merge results with those of references mined from publications in preparation for fairnes validation before upload to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd6961b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# look if already referenced in DB\n",
    "\n",
    "def check_if_in_db(ndf_name, srf_name, work_dir):\n",
    "    in_file = Path(work_dir, srf_name + \".csv\")\n",
    "    out_name = srf_name + \"_not_in_DB\"\n",
    "    out_file = Path(work_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked DB for:\", ndf_name)\n",
    "        return out_name\n",
    "\n",
    "    # get list of references with no duplicates\n",
    "\n",
    "    scholix_references, column_names = csvh.get_csv_data(in_file)\n",
    "    int_counter = 0\n",
    "    unique_refs = {}\n",
    "\n",
    "    pub_id = ''\n",
    "    for a_ref in tqdm_notebook(scholix_references):\n",
    "        pub_id = scholix_references[a_ref]['pub_id']\n",
    "        ref_id = scholix_references[a_ref]['target_id']\n",
    "        ref_title = scholix_references[a_ref]['target_title']\n",
    "        pub_datsets = pr_fns.get_pub_datasets(ukchapp_db, pub_id)\n",
    "        int_counter += 1\n",
    "        #print(int_counter, scholix_references[a_ref], pub_datsets)\n",
    "        print(f'***************PUBLICATION %s******************'%pub_id)\n",
    "        identifier_found = False\n",
    "        for a_ds in pub_datsets:\n",
    "            ds_id = a_ds[0]\n",
    "            ds_doi = a_ds[1]\n",
    "            ds_url = a_ds[2]\n",
    "            ds_name = a_ds[3]\n",
    "            if ds_doi != None and ds_doi.strip().lower() == ref_id.strip().lower():\n",
    "                #print (\"DOI FOUND\")\n",
    "                identifier_found = True\n",
    "            elif ds_url.strip().lower() == ref_id.strip().lower():\n",
    "                #print (\"URL FOUND\")\n",
    "                identifier_found = True\n",
    "            elif '?' in ds_url and not pr_fns.valid_doi(ref_id):\n",
    "                print(\"URL with extra parameters\",ds_url)\n",
    "                print (\"compared to\", ref_id)\n",
    "            if identifier_found == True:\n",
    "                scholix_references[a_ref]['in_db'] = 1\n",
    "                if ds_name == ref_title:\n",
    "                    #print('DS Name Match')\n",
    "                    scholix_references[a_ref]['title_match'] = 1\n",
    "                #else:\n",
    "                    #print('DS Name Different')\n",
    "                break\n",
    "            else:\n",
    "                scholix_references[a_ref]['in_db'] = 0\n",
    "\n",
    "    if len(scholix_references) > 0:\n",
    "        csvh.write_csv_data(scholix_references, out_file)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f93de626",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get names and links for references in data mentions\n",
    "def check_pdf_data(db_name, pdf_out_file, pdf_dir):\n",
    "    in_name = pdf_out_file\n",
    "    in_file = Path(pdf_dir, in_name)\n",
    "    if not in_file.is_file():\n",
    "        print (\"In file not found:\", in_name)\n",
    "        return \"\"\n",
    "    out_name = in_name.replace('valid','checked')\n",
    "    out_file = Path(out_name)\n",
    "    if out_file.is_file():\n",
    "        print (\"Already checked DB for:\", in_name)\n",
    "        return out_name\n",
    "\n",
    "\n",
    "    data_mentions, dm_fields = csvh.get_csv_data(in_file, 'num')\n",
    "    int_counter = 0\n",
    "    for dm in data_mentions:\n",
    "        pub_id = data_mentions[dm]['id']\n",
    "        pub_doi = data_mentions[dm]['doi']\n",
    "        ref_name = data_mentions[dm]['name']\n",
    "        ref_link = data_mentions[dm]['data_url']\n",
    "        ref_id =  data_mentions[dm]['do_id']\n",
    "        #print (ref_name, ref_link, ref_id)\n",
    "        pub_datsets = pr_fns.get_pub_datasets(ukchapp_db, pub_id)\n",
    "        int_counter += 1\n",
    "        #print(int_counter, scholix_references[a_ref], pub_datsets)\n",
    "        print(f'***************PUBLICATION %s******************'%pub_id)\n",
    "        identifier_found = False\n",
    "        if data_mentions[dm]['add'] == '1':\n",
    "            for a_ds in pub_datsets:\n",
    "                ds_id = a_ds[0]\n",
    "                ds_doi = a_ds[1]\n",
    "                ds_url = a_ds[2]\n",
    "                ds_name = a_ds[3]\n",
    "                if ds_doi != None and ds_doi.strip().lower() == ref_id.strip().lower():\n",
    "                    #print (\"DOI FOUND\")\n",
    "                    identifier_found = True\n",
    "                elif ds_url.strip().lower() == ref_id.strip().lower():\n",
    "                    #print (\"URL FOUND\")\n",
    "                    identifier_found = True\n",
    "                elif '?' in ds_url and not pr_fns.valid_doi(ref_id):\n",
    "                    print(\"URL with extra parameters\",ds_url)\n",
    "                    print (\"compared to\", ref_id)\n",
    "                if identifier_found == True:\n",
    "                    data_mentions[dm]['in_db'] = 1\n",
    "                    if ds_name == ref_title:\n",
    "                        #print('DS Name Match')\n",
    "                        data_mentions[dm]['title_match'] = 1\n",
    "                    #else:\n",
    "                        #print('DS Name Different')\n",
    "                    break\n",
    "\n",
    "    if len(data_mentions) > 0:\n",
    "        csvh.write_csv_data(data_mentions, out_name)\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ff99d",
   "metadata": {},
   "source": [
    "## Merge db filtered PDF results and Scholix results\n",
    "Merge results with those of references mined from publications in preparation for fairnes validation before upload to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "749568f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def merge_results (scholix_results, pdf_results, db_name, sclx_dir, pdf_dir, out_dir):\n",
    "    scholix_file = Path(sclx_dir, scholix_results+\".csv\" )\n",
    "    pdfresu_file = Path(pdf_dir, pdf_results)    \n",
    "    print(scholix_file,pdfresu_file)\n",
    "    # if the required files do not exist\n",
    "    if (not scholix_file.is_file()) and (not pdfresu_file.is_file()):\n",
    "        print (\"In file not found:\", in_name)\n",
    "        return \"\"\n",
    "    out_name = 'new_references_'+db_name\n",
    "    out_file = Path(out_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"Already created merge DB for:\", scholix_results, pdf_results)\n",
    "        return out_name\n",
    "\n",
    "    # get names and links for references in db checked data mentions\n",
    "    data_mentions, dm_fields = csvh.get_csv_data(pdfresu_file, 'num')\n",
    "    # get list of references with no duplicates\n",
    "    scholix_references, column_names = csvh.get_csv_data(scholix_file)\n",
    "\n",
    "    merged_references ={}\n",
    "    # first just copy all the references in scholix to the merged set\n",
    "    print(\"Copy all the references in scholix to the merged set\")\n",
    "    for a_ref in tqdm_notebook(scholix_references):\n",
    "        if scholix_references[a_ref]['in_db']!= '1':\n",
    "            merged_references[a_ref] = scholix_references[a_ref]\n",
    "\n",
    "    print (\"Check if the data mention is in the scholix references\")\n",
    "    new_idx = len(scholix_references) # start adding after the highest index for scholix\n",
    "    ccdc_count = len(scholix_references)\n",
    "    found_count = 0\n",
    "    for dm in tqdm_notebook(data_mentions):\n",
    "        pub_id = data_mentions[dm]['id']\n",
    "        pub_doi = data_mentions[dm]['doi']\n",
    "        ref_name = data_mentions[dm]['dataset_name']\n",
    "        ref_link = data_mentions[dm]['data_url']\n",
    "        ref_id =  data_mentions[dm]['do_id']\n",
    "        ref_rel = data_mentions[dm]['type']\n",
    "        print (pub_id,pub_doi,\"REF \",ref_name,ref_link,ref_id,ref_rel)\n",
    "        found_match = False\n",
    "        if data_mentions[dm]['add'] == '1' and data_mentions[dm]['in_db'] != '1' :\n",
    "            for a_ref in merged_references:\n",
    "                if ccdc_count < a_ref:\n",
    "                    break\n",
    "                mr_pub_id = merged_references[a_ref]['pub_id']\n",
    "                mr_pub_doi = merged_references[a_ref]['pub_doi']\n",
    "                mr_id = merged_references[a_ref]['target_id']\n",
    "                mr_title = merged_references[a_ref]['target_title']\n",
    "                # pub_id, pub_doi, and ref_id must match if the reference is already found in scholix\n",
    "                if pub_doi.strip().lower() == mr_pub_doi.strip().lower() and \\\n",
    "                    pub_id == mr_pub_id and \\\n",
    "                    ref_id.strip().lower() == mr_id.strip().lower():\n",
    "                    found_count += 1\n",
    "                    print(\"found match\", found_count, dm, a_ref)\n",
    "                    found_match = True\n",
    "                    merged_references[a_ref]['in_pdf']=1\n",
    "                    print (pub_doi.strip().lower(), mr_pub_doi.strip().lower(), pub_id, mr_pub_id,\n",
    "                           ref_id, mr_id)\n",
    "                    break\n",
    "            if not found_match:\n",
    "                new_idx += 1 \n",
    "                a_dl = {\"pub_id\": pub_id,\"pub_doi\":pub_doi,'source_title':'', \n",
    "                        'source_published':'',\n",
    "                        'target_id':ref_id, \n",
    "                        'target_title':ref_name, \n",
    "                        'target_published': '', \n",
    "                        'rel_type': ref_rel,\n",
    "                        'in_pdf':1}\n",
    "                if not pr_fns.valid_doi(ref_id):\n",
    "                    #print(ref_id, ref_link)\n",
    "                    a_dl['target_id'] = ref_link\n",
    "                merged_references[new_idx] = a_dl\n",
    "\n",
    "    if len(merged_references) > 0:\n",
    "        csvh.write_csv_data(merged_references, out_file)\n",
    "        print (len(merged_references), \"merged references\")\n",
    "    return out_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3201f1ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scman1\\AppData\\Local\\Temp\\ipykernel_9324\\1197869285.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for a_pub in tqdm_notebook(app_pubs):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44feff7817eb4181afc7255175827c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/774 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References found: 11\n",
      "1. Shcolix results stored in: search_scholix_production202412_20241231.csv\n",
      "search_scholix_production202412_20241231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scman1\\AppData\\Local\\Temp\\ipykernel_9324\\4081264769.py:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for a_ref in tqdm_notebook(scholix_references):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31a5d62aa1e494199b5196ddaf76a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 {'pub_id': '1056', 'pub_doi': '10.1002/cssc.201600246', 'source_title': 'Design of Highly Selective Platinum Nanoparticle Catalysts for the Aerobic Oxidation of KA‐Oil using Continuous‐Flow Chemistry', 'source_published': '2016-03-02', 'target_id': '10.1016/s0920-5861(98)00198-9', 'target_title': 'Catalytic synthesis of 2,6-dimethylphenol from methanol and KA-oil over magnesium oxide catalysts', 'target_published': '1998-09-01', 'rel_type': 'IsRelatedTo', 'rel_subtype': 'isamongtopnsimilardocuments'}\n",
      "6 {'pub_id': '1056', 'pub_doi': '10.1002/cssc.201600246', 'source_title': 'Design of Highly Selective Platinum Nanoparticle Catalysts for the Aerobic Oxidation of KA‐Oil using Continuous‐Flow Chemistry', 'source_published': '2016-03-02', 'target_id': '10.1016/s0920-5861(98)00198-9', 'target_title': 'Catalytic synthesis of 2,6-dimethylphenol from methanol and KA-oil over magnesium oxide catalysts', 'target_published': '1998-09-01', 'rel_type': 'IsRelatedTo', 'rel_subtype': 'hasamongtopnsimilardocuments'}\n",
      "5 {'pub_id': '1056', 'pub_doi': '10.1002/cssc.201600246', 'source_title': 'Design of Highly Selective Platinum Nanoparticle Catalysts for the Aerobic Oxidation of KA‐Oil using Continuous‐Flow Chemistry', 'source_published': '2016-03-02', 'target_id': '10.3390/catal11101187', 'target_title': 'Engineering Pt-Bi2O3 Interface to Boost Cyclohexanone Selectivity in Oxidative Dehydrogenation of KA-Oil', 'target_published': '2021-09-29', 'rel_type': 'IsRelatedTo', 'rel_subtype': 'isamongtopnsimilardocuments'}\n",
      "9 {'pub_id': '1056', 'pub_doi': '10.1002/cssc.201600246', 'source_title': 'Design of Highly Selective Platinum Nanoparticle Catalysts for the Aerobic Oxidation of KA‐Oil using Continuous‐Flow Chemistry', 'source_published': '2016-03-02', 'target_id': '10.3390/catal11101187', 'target_title': 'Engineering Pt-Bi2O3 Interface to Boost Cyclohexanone Selectivity in Oxidative Dehydrogenation of KA-Oil', 'target_published': '2021-09-29', 'rel_type': 'IsRelatedTo', 'rel_subtype': 'hasamongtopnsimilardocuments'}\n",
      "7 {'pub_id': '1056', 'pub_doi': '10.1002/cssc.201600246', 'source_title': 'Design of Highly Selective Platinum Nanoparticle Catalysts for the Aerobic Oxidation of KA‐Oil using Continuous‐Flow Chemistry', 'source_published': '2016-03-02', 'target_id': '10.1016/j.mcat.2019.01.019', 'target_title': 'Hydrotalcite-derived Co-MgAlO mixed metal oxides as efficient and stable catalyst for the solvent-free selective oxidation of cyclohexane with molecular oxygen', 'target_published': '2019-04-01', 'rel_type': 'IsRelatedTo', 'rel_subtype': 'hasamongtopnsimilardocuments'}\n",
      "10 {'pub_id': '1056', 'pub_doi': '10.1002/cssc.201600246', 'source_title': 'Design of Highly Selective Platinum Nanoparticle Catalysts for the Aerobic Oxidation of KA‐Oil using Continuous‐Flow Chemistry', 'source_published': '2016-03-02', 'target_id': '10.1016/j.mcat.2019.01.019', 'target_title': 'Hydrotalcite-derived Co-MgAlO mixed metal oxides as efficient and stable catalyst for the solvent-free selective oxidation of cyclohexane with molecular oxygen', 'target_published': '2019-04-01', 'rel_type': 'IsRelatedTo', 'rel_subtype': 'isamongtopnsimilardocuments'}\n",
      "8 {'pub_id': '1056', 'pub_doi': '10.1002/cssc.201600246', 'source_title': 'Design of Highly Selective Platinum Nanoparticle Catalysts for the Aerobic Oxidation of KA‐Oil using Continuous‐Flow Chemistry', 'source_published': '2016-03-02', 'target_id': '10.1002/jccs.200000019', 'target_title': 'The Mechanism Study on the Catalytic Synthesis of 2,6‐Dimethylphenol from KA‐Oil and Methanol over Magnesium Oxide Catalysts', 'target_published': '2000-02-01', 'rel_type': 'IsRelatedTo', 'rel_subtype': 'hasamongtopnsimilardocuments'}\n",
      "11 {'pub_id': '1056', 'pub_doi': '10.1002/cssc.201600246', 'source_title': 'Design of Highly Selective Platinum Nanoparticle Catalysts for the Aerobic Oxidation of KA‐Oil using Continuous‐Flow Chemistry', 'source_published': '2016-03-02', 'target_id': '10.1002/jccs.200000019', 'target_title': 'The Mechanism Study on the Catalytic Synthesis of 2,6‐Dimethylphenol from KA‐Oil and Methanol over Magnesium Oxide Catalysts', 'target_published': '2000-02-01', 'rel_type': 'IsRelatedTo', 'rel_subtype': 'isamongtopnsimilardocuments'}\n",
      "2. Non duplicate shcolix results stored in: search_scholix_production202412_20241231_no_dups.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scman1\\AppData\\Local\\Temp\\ipykernel_9324\\737100462.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for a_ref in tqdm_notebook(scholix_references):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f63d83da66445d8bba7b59885733fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************PUBLICATION 1055******************\n",
      "***************PUBLICATION 1055******************\n",
      "***************PUBLICATION 1055******************\n",
      "***************PUBLICATION 1056******************\n",
      "***************PUBLICATION 1056******************\n",
      "***************PUBLICATION 1056******************\n",
      "***************PUBLICATION 1056******************\n",
      "***************PUBLICATION 1056******************\n",
      "***************PUBLICATION 1056******************\n",
      "***************PUBLICATION 1056******************\n",
      "***************PUBLICATION 1056******************\n",
      "3. Not in DB shcolix results stored in: search_scholix_production202412_20241231_not_in_DB.csv\n",
      "In file not found: pdf_mentionsproduction202412_checked.csv\n",
      "4. Not in DB pdf results stored in: .csv\n",
      "data_search_scholix\\search_scholix_production202412_20241231_not_in_DB.csv data_search_pdf\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data_search_pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m pdfsrc_db \u001b[38;5;241m=\u001b[39m check_pdf_data(db_name, pdf_results, pdf_data_search_dir)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4. Not in DB pdf results stored in:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pdfsrc_db\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m merged_res \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschlx_search_ndb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdfsrc_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscholix_data_search_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpdf_data_search_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_load_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5. all data mentions stored in:\u001b[39m\u001b[38;5;124m\"\u001b[39m, merged_res \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m, in \u001b[0;36mmerge_results\u001b[1;34m(scholix_results, pdf_results, db_name, sclx_dir, pdf_dir, out_dir)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out_name\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# get names and links for references in db checked data mentions\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m data_mentions, dm_fields \u001b[38;5;241m=\u001b[39m \u001b[43mcsvh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_csv_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdfresu_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# get list of references with no duplicates\u001b[39;00m\n\u001b[0;32m     18\u001b[0m scholix_references, column_names \u001b[38;5;241m=\u001b[39m csvh\u001b[38;5;241m.\u001b[39mget_csv_data(scholix_file)\n",
      "File \u001b[1;32mC:\\harwell\\validatearticledata\\lib\\handle_csv.py:9\u001b[0m, in \u001b[0;36mget_csv_data\u001b[1;34m(input_file, id_field)\u001b[0m\n\u001b[0;32m      7\u001b[0m fieldnames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m identifier \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8-sig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[0;32m     10\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(csvfile)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:  \n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'data_search_pdf'"
     ]
    }
   ],
   "source": [
    "# Set the name of currend app DB\n",
    "db_name = 'production202412'\n",
    "ukchapp_db = \"db_files/\"+db_name+\".sqlite3\"\n",
    "cut_date = \"20241231\"\n",
    "\n",
    "pdf_results = \"pdf_mentionsproduction202412_checked.csv\"\n",
    "\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "start_from = 1047\n",
    "# working dirs\n",
    "pdf_data_search_dir = \"./data_search_pdf\"\n",
    "scholix_data_search_dir = \"./data_search_scholix\"\n",
    "data_load_dir = \"./data_load\"\n",
    "    \n",
    "# Get publication data from the ukch app\n",
    "app_pubs = pr_fns.get_pub_app_data(ukchapp_db)\n",
    "\n",
    "schlx_search_result = search_scolix(db_name, scholix_data_search_dir, start_from, cut_date)\n",
    "\n",
    "print (\"1. Shcolix results stored in:\", schlx_search_result+\".csv\") \n",
    "schlx_search_nd = remove_dups(schlx_search_result, scholix_data_search_dir)\n",
    "print (\"2. Non duplicate shcolix results stored in:\", schlx_search_nd+\".csv\") \n",
    "schlx_search_ndb = check_if_in_db(schlx_search_nd, schlx_search_result, scholix_data_search_dir)\n",
    "print (\"3. Not in DB shcolix results stored in:\", schlx_search_ndb+\".csv\")\n",
    "pdfsrc_db = check_pdf_data(db_name, pdf_results, pdf_data_search_dir)\n",
    "print (\"4. Not in DB pdf results stored in:\", pdfsrc_db+\".csv\")\n",
    "merged_res = merge_results(schlx_search_ndb, pdfsrc_db, db_name, scholix_data_search_dir,pdf_data_search_dir, data_load_dir)\n",
    "print (\"5. all data mentions stored in:\", merged_res +\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6c2a7",
   "metadata": {},
   "source": [
    "## Getting Data from article landing pages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa853c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrap landing page\n",
    "# get abstract and graphic abstract url if possible\n",
    "# also get reference to SI\n",
    "# Get pdf and html name from previous and put it in current\n",
    "def get_doi_landings(db_name, work_dir, start_from = 0, cut_date=\"202310\"):\n",
    "    out_name = \"search_scholix_\"+db_name+\"_\"+cut_date\n",
    "    out_file = Path(work_dir, out_name + \".csv\")\n",
    "    if out_file.is_file():\n",
    "        print (\"Already searched for\", db_name)\n",
    "        return out_name\n",
    "    data_links = {}\n",
    "    a_dl = {}\n",
    "    url_base = 'https://www.doi.org/'\n",
    "    ignore_types = ['References','IsReferencedBy']\n",
    "\n",
    "    terminate = False\n",
    "\n",
    "    for a_pub in tqdm_notebook(app_pubs):\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        match_found = False\n",
    "        if pr_fns.valid_doi(pub_doi) and pub_id >= start_from:\n",
    "            print(pub_id,\"\\t\", pub_doi)\n",
    "            print(url_base+pub_doi)\n",
    "            list_urls = []\n",
    "            if '['in pub_url:\n",
    "                list_urls = eval(pub_url.replace(\"=>\",\":\"))\n",
    "            for a_ref in list_urls:\n",
    "                if \"URL\" in a_ref.keys() and not 'pdf' in a_ref['URL'] :\n",
    "                    print (\"\\t -\", a_ref['URL'])\n",
    "                    #response = urlh.getPageFromURL(a_ref['URL'])\n",
    "                    #print(len(response))\n",
    "                    \n",
    "            #for a_result in data_results['result']:\n",
    "            #    if not a_result['RelationshipType']['Name'] in ignore_types:\n",
    "            #        id_dl += 1\n",
    "            #        source_doi = pub_doi\n",
    "            #        source_title = a_result['source']['Title'].replace('\\n',' ')\n",
    "            #        source_published = a_result['source']['PublicationDate']\n",
    "            #        target_id = a_result['target']['Identifier'][0]['ID']\n",
    "            #        target_type = a_result['target']['Type']\n",
    "            #        if not pr_fns.valid_doi(target_id) and target_type != 'literature':\n",
    "            #            if a_result['target']['Identifier'][0]['IDScheme'] in ['uniprot','pdb']:\n",
    "            #                target_id = a_result['target']['Identifier'][0]['IDURL']\n",
    "            #            else:\n",
    "            #                for an_id in a_result['target']['Identifier']:\n",
    "            #                    print (\"source\", source_doi, \"title\", source_title)\n",
    "            #                    print (an_id)\n",
    "            #                terminate = True    \n",
    "            #        target_title = a_result['target']['Title'].replace('\\n',' ')\n",
    "            #        target_published = a_result['target']['PublicationDate']\n",
    "\n",
    "            #        rel_type = a_result['RelationshipType']['Name']\n",
    "\n",
    "            #        a_dl = {\"pub_id\": pub_id,\"pub_doi\":source_doi,'source_title':source_title, 'source_published':source_published,\n",
    "            #                'target_id':target_id, 'target_title':target_title, \n",
    "            #                'target_published': target_published, 'rel_type': rel_type}\n",
    "            #        data_links[id_dl]=a_dl\n",
    "        \n",
    "        if terminate:\n",
    "            break\n",
    "        \n",
    "        \n",
    "    #print ('References found:', len(data_links))\n",
    "\n",
    "    \n",
    "    #if len(data_links) > 0  and not terminate:\n",
    "    #    csvh.write_csv_data(data_links, out_file)\n",
    "    #return out_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0696f4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already searched for production_n\n"
     ]
    }
   ],
   "source": [
    "# Set the name of currend app DB\n",
    "db_name = 'production_n'\n",
    "ukchapp_db = \"db_files/\"+db_name+\".sqlite3\"\n",
    "cut_date = \"20240415\"\n",
    "\n",
    "pdf_results = \"pdf_mentionsproduction202312_checked.csv\"\n",
    "\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "start_from = 972\n",
    "# working dirs\n",
    "pdf_data_search_dir = \"./data_search_pdf\"\n",
    "scholix_data_search_dir = \"./data_search_scholix\"\n",
    "data_load_dir = \"./data_load\"\n",
    "\n",
    "\n",
    "# Get publication data from the ukch app\n",
    "app_pubs = pr_fns.get_pub_app_data(ukchapp_db)\n",
    "\n",
    "schlx_search_result = get_doi_landings(db_name, scholix_data_search_dir, start_from, cut_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f862e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020edff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
