{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of references to UK Catalysis Hub \n",
    "A list of articles is obtainded from publish or perish. This list will contain a titles and some IDs whic need to be verified. \n",
    "\n",
    "The criteria for adding a publication to the database are: \n",
    "a) has an explicit acknowledgement of UK Catalysis Hub\n",
    "b) mentions one of the UK Catalysis Hub grants\n",
    "c) has two or more authors with affiliation to UK Catalysis Hub\n",
    "d) acknowledges support from a scientist affiliated to UK Catalysis Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the crossreference json page from doi\n",
    "def get_cr_json_object(cr_doi):\n",
    "  crjd = None\n",
    "  doi_file = 'json_files/' + cr_doi.replace('/','_').lower() + '.json'\n",
    "  if not Path(doi_file).is_file():\n",
    "    crjd = cr_api.getBibData(cr_doi)\n",
    "    with open(doi_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(crjd, f, ensure_ascii=False, indent=4)\n",
    "  else:\n",
    "    jf = open(doi_file, 'r', encoding='utf-8')\n",
    "    crjd = json.load(jf)\n",
    "  # return the content and the file name \n",
    "  return crjd, doi_file\n",
    "\n",
    "# get the landing page for the publication from uri\n",
    "def get_pub_html_doi(cr_doi):\n",
    "    html_file = 'html_files/' + cr_doi.replace('/','_').lower() + '.html'\n",
    "    if not Path(html_file).is_file():\n",
    "        page_content = urlh.getPageFromDOI(doi_text)\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(page_content.decode(\"utf-8\") )\n",
    "    else:\n",
    "        f = open(html_file, \"r\")\n",
    "        page_content = f.read()\n",
    "    return page_content, html_file\n",
    "             \n",
    "def get_titles(str_pub_title, db_name = \"prev_search.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'prev_pop_searches'\n",
    "    fields_required = \"Num, Title\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "def get_titles_and_dois(str_pub_title, db_name = \"app_db.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "# get the current csv working file\n",
    "def get_working_file(nr_wf):\n",
    "    working_file = wf_fields = None\n",
    "    current_pass = 0\n",
    "    if Path(nr_wf).is_file():\n",
    "        working_file, wf_fields = csvh.get_csv_data(nr_wf,'Num')\n",
    "        for art_num in tqdm_notebook(working_file):\n",
    "            if 'ignore' in working_file[art_num].keys():\n",
    "                if current_pass < int(working_file[art_num]['ignore']):\n",
    "                    current_pass = int(working_file[art_num]['ignore'])\n",
    "            else:\n",
    "                break\n",
    "    print(\"Current pass:\", current_pass)\n",
    "    return working_file, wf_fields, current_pass\n",
    "\n",
    "\n",
    "\n",
    "def get_pub_html_url(text_url, entry_id):\n",
    "    html_file = 'html_files/' +  entry_id + '.html'\n",
    "    if not Path(html_file).is_file():\n",
    "        print(\"\")\n",
    "        page_content = urlh.getPageFromURL(text_url)\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(page_content)\n",
    "    else:\n",
    "        f = open(html_file, \"r\")\n",
    "        page_content = f.read()\n",
    "    return page_content, html_file\n",
    "\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# get a semicolon separated list of authors from CR json data\n",
    "def get_cr_author_list(article_data):\n",
    "    authors = []\n",
    "    if 'author' in article_data.keys():\n",
    "        for author in article_data['author']:\n",
    "            new_author=\"\"\n",
    "            new_author = author['family']\n",
    "            if 'given' in author.keys():\n",
    "                new_author += \", \" + author['given']\n",
    "            authors.append(new_author)\n",
    "    return (\"; \").join(authors)\n",
    "\n",
    "# get the publication date from CR json data\n",
    "def get_cr_year_published(article_data):\n",
    "    year_print = 0\n",
    "    if 'published-print' in article_data.keys() \\\n",
    "        and article_data['published-print'] != None \\\n",
    "        and article_data['published-print']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['published-print']['date-parts'][0][0])    \n",
    "    elif 'journal-issue' in article_data.keys() \\\n",
    "        and article_data['journal-issue'] != None \\\n",
    "        and 'published-print' in article_data['journal-issue'].keys() \\\n",
    "        and article_data['journal-issue']['published-print'] != None \\\n",
    "        and article_data['journal-issue']['published-print']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['journal-issue']['published-print']['date-parts'][0][0])\n",
    "\n",
    "    year_online = 0\n",
    "    if 'published-online' in article_data.keys() \\\n",
    "        and article_data['published-online'] != None \\\n",
    "        and article_data['published-online']['date-parts'][0] != None:\n",
    "        year_online = int(article_data['published-online']['date-parts'][0][0])    \n",
    "    elif 'journal-issue' in article_data.keys() \\\n",
    "        and article_data['journal-issue'] != None \\\n",
    "        and 'published-online' in article_data['journal-issue'].keys() \\\n",
    "        and article_data['journal-issue']['published-online'] != None \\\n",
    "        and article_data['journal-issue']['published-online']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['journal-issue']['published-online']['date-parts'][0][0])\n",
    "    \n",
    "    if year_print != 0 and year_online != 0:\n",
    "        return year_print if year_print < year_online else year_online\n",
    "    else:\n",
    "        return year_print if year_online == 0 else year_online\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the file with the results of the PoP search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file with path: pop_searches/PoPCites20201017.csv\n",
    "new_results_file = \"pop_searches/PoPCites202105gs1.csv\"\n",
    "while not Path(new_results_file).is_file():\n",
    "    print('Please enter the name of the input file:')\n",
    "    new_results_file = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the db file with previous results of the PoP search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous results db file with path: db_files/prev_search.sqlite3\n",
    "previous_db = \"db_files/prev_search.sqlite3\"\n",
    "while not Path(previous_db).is_file():\n",
    "    print('Please enter the name of the previous results file:')\n",
    "    previous_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"db_files/app_db20210601.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the name of the output file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_wf = new_results_file[:-4]+\"_wf.csv\"\n",
    "print(\"Verifying if the articles listed in: \\n\\t\", Path(new_results_file).name)\n",
    "print(\"where included in previous searches: \\n\\t\", Path(previous_db).name)\n",
    "\n",
    "print(\"The results will bt saved in: \\n\\t\", nr_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the working file before each step\n",
    "working_file = wf_fields = None\n",
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "# in first pass then make working file = new results\n",
    "if working_file == None:\n",
    "    working_file, wf_fields, current_pass = get_working_file(new_results_file)\n",
    "    csvh.write_csv_data(working_file, nr_wf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if already processed titles are included\n",
    "Read data and verify if results in file have already been included in previous searches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass == 0:\n",
    "    current_initial = \"\"\n",
    "    db_titles = []\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        new_title = working_file[art_num]['Title'].lower()\n",
    "        if new_title != \"\":\n",
    "            working_file[art_num]['ignore'] = 0 \n",
    "            working_file[art_num]['previous'] = 0 \n",
    "            working_file[art_num]['similarity'] = 0.0\n",
    "            if current_initial == \"\" or current_initial != new_title[0]:\n",
    "                print(\"new intital \", new_title[0])\n",
    "                current_initial = new_title[0]\n",
    "                db_titles = get_titles(current_initial, previous_db)\n",
    "\n",
    "            for prev_pair in db_titles:\n",
    "                prev_num = prev_pair[0]\n",
    "                used_title = prev_pair[1].lower()\n",
    "                # if titles match exactly or simialarity > 0.8 ignore\n",
    "                title_similarity = txtc.similar(new_title, used_title)\n",
    "                if title_similarity > 0.80:\n",
    "                    #print(art_num, 'Title:', new_title, \"already processed\", prev_num, used_title)\n",
    "                    working_file[art_num]['ignore'] = 1\n",
    "                    working_file[art_num]['previous'] = prev_num\n",
    "                    working_file[art_num]['similarity'] = title_similarity\n",
    "                    break\n",
    "        else:\n",
    "            working_file[art_num]['ignore'] = 1\n",
    "\n",
    "    csvh.write_csv_data(working_file, nr_wf)  \n",
    "    print(nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Titles in app\n",
    "Verify if the title is in the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "# verify that titles are not in the app_db (if they are  also get DOI)\n",
    "if current_pass == 1: \n",
    "    db_titles = []\n",
    "    current_initial = \"\"\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            new_title = working_file[art_num]['Title'].lower()\n",
    "            if new_title != \"\":\n",
    "                if current_initial == \"\" or current_initial != new_title[0]:\n",
    "                    print(\"new intital \", new_title[0])\n",
    "                    current_initial = new_title[0]\n",
    "                    db_titles = get_titles_and_dois(current_initial, ukchapp_db)\n",
    "                for art_in_db in db_titles:\n",
    "                    prev_num = art_in_db[0]\n",
    "                    used_title = art_in_db[1].lower()\n",
    "                    # if titles match exactly or simialarity > 0.8 ignore\n",
    "                    title_similarity = txtc.similar(new_title, used_title)\n",
    "                    if title_similarity > 0.80:\n",
    "                        #print(art_num, 'Title:', new_title, \"already processed\", prev_num, used_title)\n",
    "                        working_file[art_num]['ignore'] = 2\n",
    "                        working_file[art_num]['previous'] = prev_num\n",
    "                        working_file[art_num]['similarity'] = title_similarity\n",
    "                        working_file[art_num]['DOIcr'] = art_in_db[2]\n",
    "                        break                \n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Title Wording\n",
    "Using the workds in previous catalysis hub papers check if the title is likely to be a cat hub title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass < 3:\n",
    "    # pass 2\n",
    "    # check titles for likelihood of being catalysis articles using keywords from titles in current DB \n",
    "    print(\"Get word list from DB\")\n",
    "    x = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    db_titles = x.get_value_list('articles','title')\n",
    "    title_words = set()\n",
    "    ignore_words=set(['the','of','to','and','a','in','is','it', 'their', 'so', 'as'])\n",
    "    average = 0\n",
    "    words_sum = 0.0\n",
    "    for title in db_titles:\n",
    "        one_title = set(title.lower().split())\n",
    "        one_title = one_title - ignore_words\n",
    "        title_words = title_words.union(one_title)\n",
    "        words_sum += len(one_title) \n",
    "        \n",
    "    average = words_sum /len(db_titles)\n",
    "    print(\"Average words per title:\", average)\n",
    "    title_words = title_words - ignore_words\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if 0 == int(working_file[art_num]['ignore']):\n",
    "            art_title = working_file[art_num]['Title']\n",
    "            art_words = set(art_title.lower().split())\n",
    "            occurrences = len(title_words.intersection(art_words))\n",
    "            working_file[art_num]['keywords']=occurrences\n",
    "            if occurrences == 0:\n",
    "                print(\"occurrences:\", occurrences, \"in title:\", art_title)\n",
    "                working_file[art_num]['ignore']=3\n",
    "            else:\n",
    "                print(\"occurrences:\", occurrences, \"in title:\", art_title)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    x.close()\n",
    "    current_pass = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass == 3:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            inspected = False\n",
    "            while not inspected:\n",
    "                new_title = working_file[art_num]['Title']\n",
    "                keywords = int(working_file[art_num]['keywords'])\n",
    "                #print (keywords, new_title)\n",
    "                if keywords <= 4 and not (\"cataly\" in new_title.lower()):\n",
    "                # ignore  it because it does not contains cataly in title\n",
    "                    working_file[art_num]['ignore']=4 # visual inspection\n",
    "                    inspected = True\n",
    "                else:\n",
    "                    inspected = True\n",
    "    print(\"To Process:\", i, \"Pass:\", current_pass)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DOIs for Articles\n",
    "The remaining titles need to be further analysed. Recovering their DOIs helps to obtain abstracts and acknowledgement statements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass == 4:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if (working_file[art_num]['ignore']=='0' and not 'DOIcr' in working_file[art_num].keys()) \\\n",
    "        or (working_file[art_num]['ignore']=='0' and working_file[art_num]['DOIcr']==\"\"):\n",
    "            new_title = working_file[art_num]['Title']\n",
    "            new_doi = cr_api.getDOIForTitle(new_title)\n",
    "            if new_doi == \"\":\n",
    "                #print(\"Missing DOI:\", new_title)\n",
    "                working_file[art_num]['ignore'] = '5'\n",
    "                i +=1\n",
    "            else:\n",
    "                #print(\"DOI found:\", new_doi, \"for:\", new_title)\n",
    "                working_file[art_num]['DOIcr'] = new_doi\n",
    "                working_file[art_num]['ignore'] = '0'\n",
    "    print(\"without DOI:\", i)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify DOIs in DB\n",
    "Verify that articles do not exist in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 4:\n",
    "    i = 0\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            new_title = working_file[art_num]['Title']\n",
    "            new_doi = working_file[art_num]['DOIcr'].strip()\n",
    "            db_title = db_conn.get_title(new_doi)\n",
    "            if db_title == None:\n",
    "                print(\"Not in DB:\", new_doi, new_title)\n",
    "            else:\n",
    "                print(\"Already in DB:\", new_doi, \"for:\", new_title, db_title)\n",
    "                working_file[art_num]['ignore'] = '6'\n",
    "    print(\"without DOI:\", i)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 6\n",
    "    db_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full json files for remaining articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 4:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            data, file_name = get_cr_json_object(article_doi)\n",
    "            if data != {}:\n",
    "                working_file[art_num]['file'] = file_name\n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if CR json files contain funder details for UKCH grants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 4:\n",
    "    i = 1\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            try:\n",
    "                data, file_name = get_cr_json_object(article_doi)\n",
    "                print(i, article_title, article_doi)\n",
    "                #print(data.keys())\n",
    "                epsrc_keys = ['EP/R026645/1', 'EP/K014668/1', 'EP/K014714/1', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                              'EP/M013219/1', 'EP/R027129/1', 'EP/K014854/1', 'EP/K014706/2']\n",
    "                confirmed_in_cr = []\n",
    "                if 'funder' in data.keys():\n",
    "                    for a_funder in data['funder']:\n",
    "                        for an_award in a_funder['award']:\n",
    "                            if an_award in epsrc_keys:\n",
    "                                print(\"Found\", an_award)\n",
    "                                confirmed_in_cr.append(an_award)\n",
    "                    working_file[art_num]['award_in_cr'] = ', '.join(confirmed_in_cr)\n",
    "                i += 1\n",
    "            except:\n",
    "                working_file[art_num]['ignore'] = 7\n",
    "                \n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full HTML files for remaining articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nr_wf = \"pop_searches/PoPCites20201017_wf.csv\"\n",
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 6:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_id = working_file[art_num]['Num']\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr'].strip().lower()\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            article_type =working_file[art_num]['type']\n",
    "            html_content = file_name = None\n",
    "            if valid_doi(article_doi):\n",
    "                html_content, file_name = get_pub_html_doi(article_doi)\n",
    "            else:\n",
    "                #try with url\n",
    "                html_content = None\n",
    "                #identifier = \"id\" + str((1000000 + int(article_id)))[1,6] + article_type \n",
    "                #html_content, file_name = get_pub_html_doi(article_url, identifier)\n",
    "            if html_content != None:\n",
    "                working_file[art_num]['html_file'] = file_name\n",
    "                \n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get HTML page from DOI and verify if it contains UKCH acknowledgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cworking_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "if current_pass >= 4:\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if (working_file[art_num]['ignore']=='0' and not 'ack_fragment' in working_file[art_num].keys()) or \\\n",
    "           (working_file[art_num]['ignore']=='0' and working_file[art_num]['ack_fragment'] == \"\"):\n",
    "            article_id = working_file[art_num]['Num']\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            request_str = \"https://doi.org/\" + article_doi \n",
    "            if valid_doi(article_doi):\n",
    "                request_str = \"https://doi.org/\" + article_doi \n",
    "                print(request_str)\n",
    "                #display(HTML('<h1>Hello, world!</h1>'))\n",
    "                #%%html\n",
    "                #<iframe src=request_str  width=\"600\" height=\"400\"></iframe>\n",
    "                IFrame(request_str, width=700, height=350)\n",
    "                inspected = False\n",
    "                while not inspected:\n",
    "                    #new_title = working_file[art_num]['Title']\n",
    "                    print('Title: ', article_title)\n",
    "                    print('***************************************************************')\n",
    "                    print(\"Options:\\n\\ta) add ack text\\n\\tb) mark as not relevant\\n\\tc) go to next\")\n",
    "                    print(\"selection:\")\n",
    "                    usr_select = input()\n",
    "                    if usr_select == 'b':\n",
    "                        #working_file[art_num]['ignore']=3 # visual inspection\n",
    "                        inspected = True\n",
    "                        working_file[art_num]['ignore'] = '9'\n",
    "                        working_file[art_num]['send_to_corinne'] = 'no'\n",
    "                        working_file[art_num]['reason_send'] = \"not acknowledged, no UKCH authors\"\n",
    "                        print(\"going to next\")\n",
    "                    elif usr_select == 'c':\n",
    "                        #working_file[art_num]['ignore']=3 # visual inspection\n",
    "                        inspected = True\n",
    "                        print(\"going to next\")\n",
    "                    elif usr_select == 'a':\n",
    "                        inspected = True\n",
    "                        ack_text = \"\"\n",
    "                        while ack_text == \"\":\n",
    "                            print(\"Enter ack text: \")\n",
    "                            ack_text = input()\n",
    "                            working_file[art_num]['ack_fragment'] = ack_text\n",
    "                            working_file[art_num]['send_to_corinne'] = 'yes'\n",
    "                            working_file[art_num]['reason_send'] = \"confirmed in acknowledgements\"\n",
    "            else:\n",
    "                print(article_doi, \"is not a valid DOI\")\n",
    "    csvh.write_csv_data(working_file, nr_wf)  \n",
    "    print(nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get bib data from CR to send for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nr_wf = \"pop_searches/PoPCites20201017_wf.csv\"\n",
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "article_title = \"\"\n",
    "article_doi = \"\"\n",
    "article_url = \"\"\n",
    "data = None    \n",
    "try:\n",
    "    if current_pass >= 6:\n",
    "        for art_num in tqdm_notebook(working_file):\n",
    "            if working_file[art_num]['send_to_corinne'] == 'yes':\n",
    "                article_title = working_file[art_num]['Title']\n",
    "                article_doi = working_file[art_num]['DOIcr']\n",
    "                article_url =working_file[art_num]['ArticleURL']\n",
    "                if valid_doi(article_doi):\n",
    "                    data, file_name = get_cr_json_object(article_doi)\n",
    "                    # get authors\n",
    "                    working_file[art_num]['cr_authors'] = get_cr_author_list(data)\n",
    "                    # get article year\n",
    "                    working_file[art_num]['cr_year'] = get_cr_year_published(data)\n",
    "                    working_file[art_num]['cr_title'] = data['title']\n",
    "                    working_file[art_num]['cr_journal'] = data['container-title']\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "except:\n",
    "    print(article_title, article_doi, article_url)\n",
    "    print(data)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#***************************************************************************************************************\n",
    "# Wait do not run this yet\n",
    "#***************************************************************************************************************\n",
    "if current_pass >= 6:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            print(\"Analysing:\", article_title, article_doi, article_url)\n",
    "            # try to retrive html page for article using link from crossref first\n",
    "            # and if not try url from pop\n",
    "            # find reference to uk catalysis hub in html text\n",
    "            # if found mark as relevant\n",
    "            found = \"\"\n",
    "            referents = [\"uk catalysis hub\", \"uk catalysis\", \"catalysis hub\",\n",
    "                 'EP/R026645/1', 'resources', 'EP/K014668/1', 'EPSRC', 'EP/K014714/1',\n",
    "                 'Hub','provided', 'grant', 'biocatalysis', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                 'support', 'membership', 'EP/M013219/1', 'UK', 'kindly', 'Catalysis',\n",
    "                 'funded', 'EP/R027129/1', 'Consortium', 'thanked', 'EP/K014854/1', 'EP/K014706/2']\n",
    "            found = urlh.findFromDOI(article_title, article_doi, referents)\n",
    "            working_file[art_num]['checked_doi'] = 1\n",
    "            working_file[art_num]['ack_doi'] = found\n",
    "            found = urlh.findFromURI(article_title, article_url, referents)\n",
    "            working_file[art_num]['checked_url'] = 1\n",
    "            working_file[art_num]['ack_url'] = found\n",
    "            print(\"Ack:\", found)\n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_text = '10.1039/d0cy00036a'\n",
    "\n",
    "url_text = \"https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.808495\"\n",
    "\n",
    "html_content, file_name = get_pub_html_doi(doi_text)\n",
    "\n",
    "print(file_name)#, html_content)\n",
    "\n",
    "import re\n",
    "\n",
    "#print(len(doi_text))\n",
    "# CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "# CR DOIs re1\n",
    "# /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "\n",
    "cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "\n",
    "compare = re.match(cr_re_01, doi_text, re.IGNORECASE)\n",
    "\n",
    "print(compare)\n",
    "print(compare.start())\n",
    "print(compare.end())\n",
    "print(compare.group())\n",
    "\n",
    "if compare != None and doi_text == compare.group():\n",
    "    print(\"This is a DOI: \", doi_text)\n",
    "else:\n",
    "    print(\"This is not a DOI: \", doi_text)\n",
    "\n",
    "compare = re.match(cr_re_01, url_text, re.IGNORECASE)\n",
    "    \n",
    "print(url_text, valid_doi(url_text))\n",
    "print(doi_text, valid_doi(doi_text))\n",
    "\n",
    "# url_text = \"https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.808495\"\n",
    "# id = id000069_thesis\n",
    "entry_id = 'id000069_thesis'\n",
    "\n",
    "html_content, file_name = get_pub_html_url(url_text, entry_id)\n",
    "print(file_name, html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_text = '10.1039/d0cy00036a'\n",
    "print(doi_text, valid_doi(doi_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsrc_keys = ['EP/R026645/1', 'EP/K014668/1', 'EP/K014714/1', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                          'EP/M013219/1', 'EP/R027129/1', 'EP/K014854/1', 'EP/K014706/2']\n",
    "', '.join(epsrc_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
