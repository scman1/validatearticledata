{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of references to UK Catalysis Hub \n",
    "A list of articles is obtainded from publish or perish. This list will contain a titles and some IDs whic need to be verified. \n",
    "\n",
    "The criteria for adding a publication to the database are: \n",
    "a) has an explicit acknowledgement of UK Catalysis Hub\n",
    "b) mentions one of the UK Catalysis Hub grants\n",
    "c) has two or more authors with affiliation to UK Catalysis Hub\n",
    "d) acknowledges support from a scientist affiliated to UK Catalysis Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the crossreference json page from doi\n",
    "def get_cr_json_object(cr_doi):\n",
    "  crjd = None\n",
    "  doi_file = 'json_files/' + cr_doi.replace('/','_').lower() + '.json'\n",
    "  if not Path(doi_file).is_file():\n",
    "    crjd = cr_api.getBibData(cr_doi)\n",
    "    with open(doi_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(crjd, f, ensure_ascii=False, indent=4)\n",
    "  else:\n",
    "    jf = open(doi_file, 'r')\n",
    "    crjd = json.load(jf)\n",
    "  # return the content and the file name \n",
    "  return crjd, doi_file\n",
    "\n",
    "# get the landing page for the publication from uri\n",
    "def get_pub_html_doi(cr_doi):\n",
    "    html_file = 'html_files/' + cr_doi.replace('/','_').lower() + '.html'\n",
    "    if not Path(html_file).is_file():\n",
    "        page_content = urlh.getPageFromDOI(doi_text)\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(page_content.decode(\"utf-8\") )\n",
    "    else:\n",
    "        f = open(html_file, \"r\")\n",
    "        page_content = f.read()\n",
    "    return page_content, html_file\n",
    "             \n",
    "def get_titles(str_pub_title, db_name = \"prev_search.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'prev_pop_searches'\n",
    "    fields_required = \"Num, Title\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "def get_titles_and_dois(str_pub_title, db_name = \"app_db.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "# get the current csv working file\n",
    "def get_working_file(nr_wf):\n",
    "    working_file = wf_fields = None\n",
    "    current_pass = 0\n",
    "    if Path(nr_wf).is_file():\n",
    "        working_file, wf_fields = csvh.get_csv_data(nr_wf,'Num')\n",
    "        for art_num in tqdm_notebook(working_file):\n",
    "            if 'ignore' in working_file[art_num].keys():\n",
    "                if current_pass < int(working_file[art_num]['ignore']):\n",
    "                    current_pass = int(working_file[art_num]['ignore'])\n",
    "            else:\n",
    "                break\n",
    "    print(\"Current pass:\", current_pass)\n",
    "    return working_file, wf_fields, current_pass\n",
    "\n",
    "\n",
    "\n",
    "def get_pub_html_url(text_url, entry_id):\n",
    "    html_file = 'html_files/' +  entry_id + '.html'\n",
    "    if not Path(html_file).is_file():\n",
    "        print(\"\")\n",
    "        page_content = urlh.getPageFromURL(text_url)\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(page_content)\n",
    "    else:\n",
    "        f = open(html_file, \"r\")\n",
    "        page_content = f.read()\n",
    "    return page_content, html_file\n",
    "\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# get a semicolon separated list of authors from CR json data\n",
    "def get_cr_author_list(article_data):\n",
    "    authors = []\n",
    "    if 'author' in article_data.keys():\n",
    "        for author in article_data['author']:\n",
    "            new_author=\"\"\n",
    "            new_author = author['family']\n",
    "            if 'given' in author.keys():\n",
    "                new_author += \", \" + author['given']\n",
    "            authors.append(new_author)\n",
    "    return (\"; \").join(authors)\n",
    "\n",
    "# get the publication date from CR json data\n",
    "def get_cr_year_published(article_data):\n",
    "    year_print = 0\n",
    "    if 'published-print' in article_data.keys() \\\n",
    "        and article_data['published-print'] != None \\\n",
    "        and article_data['published-print']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['published-print']['date-parts'][0][0])    \n",
    "    elif 'journal-issue' in article_data.keys() \\\n",
    "        and article_data['journal-issue'] != None \\\n",
    "        and 'published-print' in article_data['journal-issue'].keys() \\\n",
    "        and article_data['journal-issue']['published-print'] != None \\\n",
    "        and article_data['journal-issue']['published-print']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['journal-issue']['published-print']['date-parts'][0][0])\n",
    "\n",
    "    year_online = 0\n",
    "    if 'published-online' in article_data.keys() \\\n",
    "        and article_data['published-online'] != None \\\n",
    "        and article_data['published-online']['date-parts'][0] != None:\n",
    "        year_online = int(article_data['published-online']['date-parts'][0][0])    \n",
    "    elif 'journal-issue' in article_data.keys() \\\n",
    "        and article_data['journal-issue'] != None \\\n",
    "        and 'published-online' in article_data['journal-issue'].keys() \\\n",
    "        and article_data['journal-issue']['published-online'] != None \\\n",
    "        and article_data['journal-issue']['published-online']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['journal-issue']['published-online']['date-parts'][0][0])\n",
    "    \n",
    "    if year_print != 0 and year_online != 0:\n",
    "        return year_print if year_print < year_online else year_online\n",
    "    else:\n",
    "        return year_print if year_online == 0 else year_online\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the file with the results of the PoP search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the name of the input file:\n",
      "pop_searches/PoPCites20201017CR.csv\n"
     ]
    }
   ],
   "source": [
    "# input file with path: pop_searches/PoPCites20201017.csv\n",
    "new_results_file = \"\"\n",
    "while not Path(new_results_file).is_file():\n",
    "    print('Please enter the name of the input file:')\n",
    "    new_results_file = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the db file with previous results of the PoP search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the name of the previous results file:\n",
      "db_files/prev_search.sqlite3\n"
     ]
    }
   ],
   "source": [
    "# previous results db file with path: db_files/prev_search.sqlite3\n",
    "previous_db = \"\"\n",
    "while not Path(previous_db).is_file():\n",
    "    print('Please enter the name of the previous results file:')\n",
    "    previous_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the name of the previous results file:\n",
      "db_files/app_db.sqlite3\n"
     ]
    }
   ],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the name of the output file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying if the articles listed in: \n",
      "\t PoPCites20201017CR.csv\n",
      "where included in previous searches: \n",
      "\t prev_search.sqlite3\n",
      "The results will bt saved in: \n",
      "\t pop_searches/PoPCites20201017CR_wf.csv\n"
     ]
    }
   ],
   "source": [
    "nr_wf = new_results_file[:-4]+\"_wf.csv\"\n",
    "print(\"Verifying if the articles listed in: \\n\\t\", Path(new_results_file).name)\n",
    "print(\"where included in previous searches: \\n\\t\", Path(previous_db).name)\n",
    "\n",
    "print(\"The results will bt saved in: \\n\\t\", nr_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0874f359a0436db9b0bd96264e566f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 1\n"
     ]
    }
   ],
   "source": [
    "# get the working file before each step\n",
    "working_file = wf_fields = None\n",
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "# in first pass then make working file = new results\n",
    "if working_file == None:\n",
    "    working_file, wf_fields, current_pass = get_working_file(new_results_file)\n",
    "    csvh.write_csv_data(working_file, nr_wf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if already processed titles are included\n",
    "Read data and verify if results in file have already been included in previous searches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54a7b04db244cc1adbe580a1623f137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 1\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass == 0:\n",
    "    current_initial = \"\"\n",
    "    db_titles = []\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        new_title = working_file[art_num]['Title'].lower()\n",
    "        working_file[art_num]['ignore'] = 0 \n",
    "        working_file[art_num]['previous'] = 0 \n",
    "        working_file[art_num]['similarity'] = 0.0\n",
    "        if current_initial == \"\" or current_initial != new_title[0]:\n",
    "            print(\"new intital \", new_title[0])\n",
    "            current_initial = new_title[0]\n",
    "            db_titles = get_titles(current_initial, previous_db)\n",
    "            \n",
    "        for prev_pair in db_titles:\n",
    "            prev_num = prev_pair[0]\n",
    "            used_title = prev_pair[1].lower()\n",
    "            # if titles match exactly or simialarity > 0.8 ignore\n",
    "            title_similarity = txtc.similar(new_title, used_title)\n",
    "            if title_similarity > 0.80:\n",
    "                #print(art_num, 'Title:', new_title, \"already processed\", prev_num, used_title)\n",
    "                working_file[art_num]['ignore'] = 1\n",
    "                working_file[art_num]['previous'] = prev_num\n",
    "                working_file[art_num]['similarity'] = title_similarity\n",
    "                break\n",
    "\n",
    "    csvh.write_csv_data(working_file, nr_wf)  \n",
    "    print(nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Titles in app\n",
    "Verify if the title is in the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149fc084b05747f1bab8bc83f2a4724e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53210e82c9dd4d45b709a3c66435df95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new intital  0\n",
      "db_files/app_db.sqlite3\n",
      "new intital  1\n",
      "db_files/app_db.sqlite3\n",
      "new intital  2\n",
      "db_files/app_db.sqlite3\n",
      "new intital  4\n",
      "db_files/app_db.sqlite3\n",
      "new intital  a\n",
      "db_files/app_db.sqlite3\n",
      "new intital  b\n",
      "db_files/app_db.sqlite3\n",
      "new intital  c\n",
      "db_files/app_db.sqlite3\n",
      "new intital  d\n",
      "db_files/app_db.sqlite3\n",
      "new intital  e\n",
      "db_files/app_db.sqlite3\n",
      "new intital  f\n",
      "db_files/app_db.sqlite3\n",
      "new intital  g\n",
      "db_files/app_db.sqlite3\n",
      "new intital  h\n",
      "db_files/app_db.sqlite3\n",
      "new intital  i\n",
      "db_files/app_db.sqlite3\n",
      "new intital  j\n",
      "db_files/app_db.sqlite3\n",
      "new intital  l\n",
      "db_files/app_db.sqlite3\n",
      "new intital  m\n",
      "db_files/app_db.sqlite3\n",
      "new intital  n\n",
      "db_files/app_db.sqlite3\n",
      "new intital  o\n",
      "db_files/app_db.sqlite3\n",
      "new intital  p\n",
      "db_files/app_db.sqlite3\n",
      "new intital  r\n",
      "db_files/app_db.sqlite3\n",
      "new intital  s\n",
      "db_files/app_db.sqlite3\n",
      "new intital  t\n",
      "db_files/app_db.sqlite3\n",
      "new intital  u\n",
      "db_files/app_db.sqlite3\n",
      "new intital  v\n",
      "db_files/app_db.sqlite3\n",
      "new intital  w\n",
      "db_files/app_db.sqlite3\n",
      "new intital  n\n",
      "db_files/app_db.sqlite3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "# verify that titles are not in the app_db (if they are  also get DOI)\n",
    "if current_pass == 1: \n",
    "    db_titles = []\n",
    "    current_initial = \"\"\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            new_title = working_file[art_num]['Title'].lower()\n",
    "            if current_initial == \"\" or current_initial != new_title[0]:\n",
    "                print(\"new intital \", new_title[0])\n",
    "                current_initial = new_title[0]\n",
    "                db_titles = get_titles_and_dois(current_initial, ukchapp_db)\n",
    "            for art_in_db in db_titles:\n",
    "                prev_num = art_in_db[0]\n",
    "                used_title = art_in_db[1].lower()\n",
    "                # if titles match exactly or simialarity > 0.8 ignore\n",
    "                title_similarity = txtc.similar(new_title, used_title)\n",
    "                if title_similarity > 0.80:\n",
    "                    #print(art_num, 'Title:', new_title, \"already processed\", prev_num, used_title)\n",
    "                    working_file[art_num]['ignore'] = 2\n",
    "                    working_file[art_num]['previous'] = prev_num\n",
    "                    working_file[art_num]['similarity'] = title_similarity\n",
    "                    working_file[art_num]['DOIcr'] = art_in_db[2]\n",
    "                    break                \n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Title Wording\n",
    "Using the workds in previous catalysis hub papers check if the title is likely to be a cat hub title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e2db019ede480d8ea33b1f1cbdd615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 1\n",
      "Get word list from DB\n",
      "Average words per title: 10.11271676300578\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39161bddafe43e4bce68b9447a95104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occurrences: 6 in title: 0 2 0)-Textured tungsten trioxide nanostructure with enhanced photoelectrochemical activity\n",
      "occurrences: 0 in title: 1. Dangerous Classes\n",
      "occurrences: 1 in title: 2. The Exchange Hub\n",
      "occurrences: 0 in title: 4. Hacking Infrastructures\n",
      "occurrences: 2 in title: A Case of External Auditory Canal Sebaceous Carcinoma: Literature Review and Treatment Discussion\n",
      "occurrences: 10 in title: A detailed speciation of iron on FCC catalysts based on an integrated use of advanced characterisation methods and thermodynamic equilibrium simulation\n",
      "occurrences: 3 in title: A Non-Invasive Flexible Glucose Monitoring Sensor using a Broadband Reject Filter\n",
      "occurrences: 4 in title: An allosteric interaction controls the activation mechanism of SHP2 tyrosine phosphatase\n",
      "occurrences: 5 in title: Anylogic Simulation Research on Passenger Evacuation System of Urban Transportation Hub\n",
      "occurrences: 11 in title: Beyond surface redox and oxygen mobility at pd-polar ceria (100) interface: Underlying principle for strong metal-support interactions in green catalysis\n",
      "occurrences: 1 in title: BRANDING HUB FOR DESIGNERS\n",
      "occurrences: 7 in title: Carbon dioxide decomposition through gas exchange in barium calcium iron niobates\n",
      "occurrences: 3 in title: Combination Therapy with Etilefrine and Pleurodesis for Refractory Congenital Chylothorax\n",
      "occurrences: 3 in title: Comprehensive Modeling of Dust Accumulation on PV Modules Through Dry Deposition Processes\n",
      "occurrences: 6 in title: Computational prediction of muon stopping sites: A novel take on the unperturbed electrostatic potential method\n",
      "occurrences: 1 in title: Congenital Laryngeal Stenosis and Concomitant Birth Defects in a Term Newborn: A Case Report\n",
      "occurrences: 0 in title: Contents\n",
      "occurrences: 0 in title: Cranial Nerve VI Palsy as Presenting Sign of Previously Undiagnosed Metastatic Prostate Adenocarcinoma to the Clivus\n",
      "occurrences: 5 in title: Decarbonising Kenya's domestic & industry Sectors through bioenergy: An assessment of biomass resource potential & GHG performances\n",
      "occurrences: 6 in title: Designer metalloenzymes for synthetic biology: Enzyme hybrids for catalysis\n",
      "occurrences: 0 in title: Die kartellrechtliche Beurteilung von Preisgleitklauseln als Ausprägung von Hub & Spoke Verhältnissen\n",
      "occurrences: 2 in title: Encoder seal assembly for a vehicle’s hub bearing\n",
      "occurrences: 8 in title: Enhancing activity and durability of Pd nanoparticle electrocatalyst by ceria undercoating on carbon support\n",
      "occurrences: 4 in title: Ethnicity and risk of death in patients hospitalised for COVID-19 infection in the UK: an observational cohort study in an urban catchment area\n",
      "occurrences: 2 in title: Evaluation of Sanitation Strategies and Initiatives Implemented in Mexico from Community Capitals Point of View\n",
      "occurrences: 3 in title: Exploring temporal aspects of climate-change effects due to bioenergy\n",
      "occurrences: 11 in title: Feasibility Study of Combining Direct Air Capture of CO2 and Methanation at Isothermal Conditions with Dual Function Materials\n",
      "occurrences: 2 in title: Fire dynamics in mass timber compartments\n",
      "occurrences: 2 in title: Focus on Subgroups within the Hub\n",
      "occurrences: 2 in title: Formula Electric Vehicle Wheel Hub Design and Optimization for in-Hub Motor in Rear Wheel Drive Configuration\n",
      "occurrences: 2 in title: Government bill could enable hub-and-spoke dispensing between separate pharmacies\n",
      "occurrences: 0 in title: Hub Location Problems\n",
      "occurrences: 0 in title: Hub-based subspace clustering\n",
      "occurrences: 0 in title: Hub-Systeme\n",
      "occurrences: 1 in title: Hypovitaminosis D Is Associated with Some Metabolic Indices in Gestational Diabetes Mellitus\n",
      "occurrences: 0 in title: Iconology, Neoplatonism, and the Arts in the Renaissance\n",
      "occurrences: 0 in title: Ierse balie wil global legal hub\n",
      "occurrences: 1 in title: Investigation of Filamentous Basidiomycetes in the Airway Is the Third Unmet Need in the Management of Unexplained Chronic Cough in Adults\n",
      "occurrences: 0 in title: IOT-HUB: A DIGITAL LIBRARY\n",
      "occurrences: 0 in title: JEDDAH’S INTERNATIONAL WRITERS HUB\n",
      "occurrences: 1 in title: Lifestyle Hub Pilot Study\n",
      "occurrences: 0 in title: Literacy Hub (L-hub): Studi Strategi Literasi Pemerintah Daerah (Studi Kasus di Kota Baubau)\n",
      "occurrences: 1 in title: LongGeneDB: a data hub for long genes\n",
      "occurrences: 2 in title: Low-cost multiple FBG interrogation technique for static applications\n",
      "occurrences: 2 in title: Magnification Effect of Foveal Avascular Zone Measurement Using Optical Coherence Tomography Angiography\n",
      "occurrences: 0 in title: Meningocele of the Internal Auditory Canal Requiring Facial-Nerve Decompression\n",
      "occurrences: 1 in title: Modeling, Analysis, and Validation of Controller Signal Interharmonic Effects in DFIG Drives\n",
      "occurrences: 2 in title: Murine Skin Carcinogenesis and the Role of Immune System Dysregulation in the Tumorigenicity of 2-Ethylhexyl Acrylate\n",
      "occurrences: 8 in title: Ni-Fe-Al mixed oxide for combined dry reforming and decomposition of methane with CO2 utilization\n",
      "occurrences: 3 in title: Online Hybrid Motion Planning for Dyadic Collaborative Manipulation via Bilevel Optimization\n",
      "occurrences: 0 in title: Outward Orientation, Leveling, and Hub Social Geometry\n",
      "occurrences: 3 in title: Peer Review #1 of \"Identification of hub genes and small-molecule compounds in medulloblastoma by integrated bioinformatic analyses (v0.1)\"\n",
      "occurrences: 3 in title: Peer Review #1 of \"Identification of hub genes and small-molecule compounds in medulloblastoma by integrated bioinformatic analyses (v0.2)\"\n",
      "occurrences: 3 in title: Peer Review #2 of \"Identification of hub genes and small-molecule compounds in medulloblastoma by integrated bioinformatic analyses (v0.1)\"\n",
      "occurrences: 1 in title: Post-Burn Infantile Hemangioma in an Extremely Premature Neonate\n",
      "occurrences: 2 in title: Research on Modeling and Lightweight of Automobile Hub\n",
      "occurrences: 4 in title: Review for \"Functional organisation and connectivity of the dorsal column nuclei complex reveals a sensorimotor integration and distribution hub\"\n",
      "occurrences: 4 in title: Review for \"Functional organisation and connectivity of the dorsal column nuclei complex reveals a sensorimotor integration and distribution hub\"\n",
      "occurrences: 4 in title: Review for \"Functional organisation and connectivity of the dorsal column nuclei complex reveals a sensorimotor integration and distribution hub\"\n",
      "occurrences: 3 in title: Review for \"Robust energy hub optimization with <scp>cross‐vector</scp> demand response\"\n",
      "occurrences: 3 in title: Review for \"Robust energy hub optimization with <scp>cross‐vector</scp> demand response\"\n",
      "occurrences: 3 in title: Review for \"Robust energy hub optimization with <scp>cross‐vector</scp> demand response\"\n",
      "occurrences: 3 in title: Review for \"Robust energy hub optimization with <scp>cross‐vector</scp> demand response\"\n",
      "occurrences: 3 in title: Review for \"Robust energy hub optimization with <scp>cross‐vector</scp> demand response\"\n",
      "occurrences: 3 in title: Review for \"Robust energy hub optimization with <scp>cross‐vector</scp> demand response\"\n",
      "occurrences: 0 in title: RFIC 2020 Hub Page\n",
      "occurrences: 0 in title: Scrub the Hub:\n",
      "occurrences: 0 in title: Sinonasal Angioleiomyoma: A Rare Entity\n",
      "occurrences: 2 in title: Tattoo-Associated Basal Cell Carcinoma: Coincident or Coincidence\n",
      "occurrences: 0 in title: The Hub\n",
      "occurrences: 0 in title: The making of hub airports\n",
      "occurrences: 0 in title: The paradox of a transit hub\n",
      "occurrences: 0 in title: THE RESILIENCE HUB\n",
      "occurrences: 0 in title: TRADITIONAL ARTISANS HUB: ARTISANAL HIJAZAI HERITAGE ARTISANS HUB\n",
      "occurrences: 3 in title: Transportable optical atomic clocks for use in out-of-the-lab environments\n",
      "occurrences: 2 in title: UK Wesseling DME process\n",
      "occurrences: 3 in title: UK Wesseling HP methanol process\n",
      "occurrences: 1 in title: Understanding the Security Risks of Docker Hub\n",
      "occurrences: 2 in title: Update on the ASH Research Collaborative Data Hub\n",
      "occurrences: 0 in title: Variations in the Management of Cervical Thoracic Duct Cyst\n",
      "occurrences: 2 in title: Volunteering and Policy Makers: The Political Uses of the UK Conservative Party’s International Development Volunteering Projects\n",
      "occurrences: 0 in title: Wider hub-and-spoke dispensing expected in 2021\n",
      "occurrences: 0 in title: None\n",
      "occurrences: 0 in title: None\n",
      "occurrences: 0 in title: None\n",
      "occurrences: 0 in title: None\n",
      "occurrences: 0 in title: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass < 3:\n",
    "    # pass 2\n",
    "    # check titles for likelihood of being catalysis articles using keywords from titles in current DB \n",
    "    print(\"Get word list from DB\")\n",
    "    x = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    db_titles = x.get_value_list('articles','title')\n",
    "    title_words = set()\n",
    "    ignore_words=set(['the','of','to','and','a','in','is','it', 'their', 'so', 'as'])\n",
    "    average = 0\n",
    "    words_sum = 0.0\n",
    "    for title in db_titles:\n",
    "        one_title = set(title.lower().split())\n",
    "        one_title = one_title - ignore_words\n",
    "        title_words = title_words.union(one_title)\n",
    "        words_sum += len(one_title) \n",
    "        \n",
    "    average = words_sum /len(db_titles)\n",
    "    print(\"Average words per title:\", average)\n",
    "    title_words = title_words - ignore_words\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if 0 == int(working_file[art_num]['ignore']):\n",
    "            art_title = working_file[art_num]['Title']\n",
    "            art_words = set(art_title.lower().split())\n",
    "            occurrences = len(title_words.intersection(art_words))\n",
    "            working_file[art_num]['keywords']=occurrences\n",
    "            if occurrences == 0:\n",
    "                print(\"occurrences:\", occurrences, \"in title:\", art_title)\n",
    "                working_file[art_num]['ignore']=3\n",
    "            else:\n",
    "                print(\"occurrences:\", occurrences, \"in title:\", art_title)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    x.close()\n",
    "    current_pass = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577a9f09d4b64b88a023fb07b700aed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass == 3:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            inspected = False\n",
    "            while not inspected:\n",
    "                new_title = working_file[art_num]['Title']\n",
    "                keywords = int(working_file[art_num]['keywords'])\n",
    "                #print (keywords, new_title)\n",
    "                if keywords <= 4 and not (\"cataly\" in new_title.lower()):\n",
    "                # ignore  it because it does not contains cataly in title\n",
    "                    working_file[art_num]['ignore']=4 # visual inspection\n",
    "                    inspected = True\n",
    "                else:\n",
    "                    inspected = True\n",
    "    print(\"To Process:\", i, \"Pass:\", current_pass)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DOIs for Articles\n",
    "The remaining titles need to be further analysed. Recovering their DOIs helps to obtain abstracts and acknowledgement statements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd34d7a430c043e9a3219bcdf45ad2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eecaff130b94d03802d49b8bfc1bbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "without DOI: 0\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass == 4:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0' and not ('DOIcr' in working_file[art_num].keys() \\\n",
    "        or working_file[art_num]['ignore']=='0' and working_file[art_num]['DOIcr']==\"\":\n",
    "            new_title = working_file[art_num]['Title']\n",
    "            new_doi = cr_api.getDOIForTitle(new_title)\n",
    "            if new_doi == \"\":\n",
    "                #print(\"Missing DOI:\", new_title)\n",
    "                working_file[art_num]['ignore'] = '5'\n",
    "                i +=1\n",
    "            else:\n",
    "                #print(\"DOI found:\", new_doi, \"for:\", new_title)\n",
    "                working_file[art_num]['DOIcr'] = new_doi\n",
    "                working_file[art_num]['ignore'] = '0'\n",
    "    print(\"without DOI:\", i)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify DOIs in DB\n",
    "Verify that articles do not exist in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fde5b10fb243b48f76646400486bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47e00428054411db72a2a314a06d69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in DB: 10.1016/j.jcat.2020.06.012 0 2 0)-Textured tungsten trioxide nanostructure with enhanced photoelectrochemical activity\n",
      "Not in DB: 10.1016/j.apcata.2020.117597 A detailed speciation of iron on FCC catalysts based on an integrated use of advanced characterisation methods and thermodynamic equilibrium simulation\n",
      "Not in DB: 10.5220/0010002800050011 Anylogic Simulation Research on Passenger Evacuation System of Urban Transportation Hub\n",
      "Not in DB: 10.1016/j.apcatb.2020.118843 Beyond surface redox and oxygen mobility at pd-polar ceria (100) interface: Underlying principle for strong metal-support interactions in green catalysis\n",
      "Not in DB: 10.1016/j.cattod.2020.05.024 Carbon dioxide decomposition through gas exchange in barium calcium iron niobates\n",
      "Not in DB: 10.1063/5.0012381 Computational prediction of muon stopping sites: A novel take on the unperturbed electrostatic potential method\n",
      "Not in DB: 10.1016/j.biombioe.2020.105757 Decarbonising Kenya's domestic & industry Sectors through bioenergy: An assessment of biomass resource potential & GHG performances\n",
      "Not in DB: 10.1016/j.cbpa.2020.06.004 Designer metalloenzymes for synthetic biology: Enzyme hybrids for catalysis\n",
      "Not in DB: 10.1016/j.jcat.2020.02.010 Enhancing activity and durability of Pd nanoparticle electrocatalyst by ceria undercoating on carbon support\n",
      "Not in DB: 10.1016/j.apcatb.2020.119416 Feasibility Study of Combining Direct Air Capture of CO2 and Methanation at Isothermal Conditions with Dual Function Materials\n",
      "Not in DB: 10.1016/j.cattod.2020.02.030 Ni-Fe-Al mixed oxide for combined dry reforming and decomposition of methane with CO2 utilization\n",
      "\n",
      "without DOI: 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'lib.handle_db' has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-9e9163decf0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcsvh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_wf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcurrent_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdbh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'lib.handle_db' has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 4:\n",
    "    i = 0\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            new_title = working_file[art_num]['Title']\n",
    "            new_doi = working_file[art_num]['DOIcr'].strip()\n",
    "            db_title = db_conn.get_title(new_doi)\n",
    "            if db_title == None:\n",
    "                print(\"Not in DB:\", new_doi, new_title)\n",
    "            else:\n",
    "                print(\"Already in DB:\", new_doi, \"for:\", new_title, db_title)\n",
    "                working_file[art_num]['ignore'] = '6'\n",
    "    print(\"without DOI:\", i)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 6\n",
    "    dbh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full json files for remaining articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274a6ce7f724412a8ec2da1e6e356e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a684bb234e9e4217b9c713eb4167aa52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 4:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            data, file_name = get_cr_json_object(article_doi)\n",
    "            if data != {}:\n",
    "                working_file[art_num]['file'] = file_name\n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if CR json files contain funder details for UKCH grants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91a636025c14536973c64e16216b622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c136a26d28346b58db39f1f545d2450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 2 0)-Textured tungsten trioxide nanostructure with enhanced photoelectrochemical activity 10.1016/j.jcat.2020.06.012\n",
      "2 A detailed speciation of iron on FCC catalysts based on an integrated use of advanced characterisation methods and thermodynamic equilibrium simulation 10.1016/j.apcata.2020.117597\n",
      "3 Anylogic Simulation Research on Passenger Evacuation System of Urban Transportation Hub 10.5220/0010002800050011\n",
      "4 Beyond surface redox and oxygen mobility at pd-polar ceria (100) interface: Underlying principle for strong metal-support interactions in green catalysis 10.1016/j.apcatb.2020.118843\n",
      "5 Carbon dioxide decomposition through gas exchange in barium calcium iron niobates 10.1016/j.cattod.2020.05.024\n",
      "6 Computational prediction of muon stopping sites: A novel take on the unperturbed electrostatic potential method 10.1063/5.0012381\n",
      "7 Decarbonising Kenya's domestic & industry Sectors through bioenergy: An assessment of biomass resource potential & GHG performances 10.1016/j.biombioe.2020.105757\n",
      "8 Designer metalloenzymes for synthetic biology: Enzyme hybrids for catalysis 10.1016/j.cbpa.2020.06.004\n",
      "9 Enhancing activity and durability of Pd nanoparticle electrocatalyst by ceria undercoating on carbon support 10.1016/j.jcat.2020.02.010\n",
      "10 Feasibility Study of Combining Direct Air Capture of CO2 and Methanation at Isothermal Conditions with Dual Function Materials 10.1016/j.apcatb.2020.119416\n",
      "11 Ni-Fe-Al mixed oxide for combined dry reforming and decomposition of methane with CO2 utilization 10.1016/j.cattod.2020.02.030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 4:\n",
    "    i = 1\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            data, file_name = get_cr_json_object(article_doi)\n",
    "            print(i, article_title, article_doi)\n",
    "            #print(data.keys())\n",
    "            epsrc_keys = ['EP/R026645/1', 'EP/K014668/1', 'EP/K014714/1', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                          'EP/M013219/1', 'EP/R027129/1', 'EP/K014854/1', 'EP/K014706/2']\n",
    "            confirmed_in_cr = []\n",
    "            if 'funder' in data.keys():\n",
    "                for a_funder in data['funder']:\n",
    "                    for an_award in a_funder['award']:\n",
    "                        if an_award in epsrc_keys:\n",
    "                            print(\"Found\", an_award)\n",
    "                            confirmed_in_cr.append(an_award)\n",
    "                working_file[art_num]['award_in_cr'] = ', '.join(confirmed_in_cr)\n",
    "            i += 1\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full HTML files for remaining articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64db10ebad474b7b921511918f271c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000e45feee9c41a8be2d06f74fbd8b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nr_wf = \"pop_searches/PoPCites20201017_wf.csv\"\n",
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 6:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_id = working_file[art_num]['Num']\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr'].strip().lower()\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            article_type =working_file[art_num]['type']\n",
    "            html_content = file_name = None\n",
    "            if valid_doi(article_doi):\n",
    "                html_content, file_name = get_pub_html_doi(article_doi)\n",
    "            else:\n",
    "                #try with url\n",
    "                html_content = None\n",
    "                #identifier = \"id\" + str((1000000 + int(article_id)))[1,6] + article_type \n",
    "                #html_content, file_name = get_pub_html_doi(article_url, identifier)\n",
    "            if html_content != None:\n",
    "                working_file[art_num]['html_file'] = file_name\n",
    "                \n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get HTML page from DOI and verify if it contains UKCH acknowledgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba0c7fbc6a445dcafad4631ef74e917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2171251c9b462fbec7de09a9f426fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://doi.org/10.1177/1468087420962294\n",
      "Title:  A comparative study into the effects of pre and post catalyst exhaust gas recirculation on the onset of knock\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1039/d0cp03852k\n",
      "Title:  An approach to calculate the free energy changes of surface reactions using free energy decomposition on ab initio brute-force molecular dynamics …\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1016/j.respol.2020.104045\n",
      "Title:  Anchor entrepreneurship and industry catalysis: The rise of the Italian Biomedical Valley\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1039/c8fd90016g\n",
      "Title:  Application of new nanoparticle structures as catalysts: general discussion\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1021/acscatal.7b02733\n",
      "Title:  Atmospheric Pressure and Room Temperature Synthesis of Methanol through Plasma-Catalytic Hydrogenation of CO2\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1021/acsami.7b15456\n",
      "Title:  Biocatalytic self-assembly on magnetic nanoparticles\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1039/d0sc01918f\n",
      "Title:  Catalyst control of selectivity in the C–O bond alumination of biomass derived furans\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1515/chem-2018-0055\n",
      "Title:  Catalytic activities of heterogeneous catalysts obtained by copolymerization of metal-containing 2-(acetoacetoxy) ethyl methacrylate\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1142/9789813237179_0010\n",
      "Title:  CHALLENGES FOR ORGANOCATALYSIS\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1021/acs.jpcc.8b08461\n",
      "Title:  CO and O2 Adsorption on K/Pt(111)\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1016/j.trechm.2020.03.002\n",
      "Title:  Cofactor NAD (P) H Regeneration: How Selective Are the Reactions?\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "thesis is not a valid DOI\n",
      "10.1039/c8cy90033g  is not a valid DOI\n",
      "https://doi.org/10.5286/edata/736\n",
      "Title:  Dataset for the publication:\" The methyl torsion in unsaturated compounds\" published in ACS Omega (2019) by Zachariou et al.\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1038/s41929-018-0169-3\n",
      "Title:  Detection of catalytic intermediates at an electrode surface during carbon dioxide reduction by an earth-abundant catalyst\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1016/j.mcat.2018.06.002\n",
      "Title:  Effect of the addition of different doping agents on visible light activity of porous TiO2 photocatalysts\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1038/s41598-017-19135-7\n",
      "Title:  Effects of distal mutations on the structure, dynamics and catalysis of human monoacylglycerol lipase\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1016/j.elecom.2017.12.017\n",
      "Title:  Electrocatalytic activity of CoFe2O4 thin films prepared by AACVD towards the oxygen evolution reaction in alkaline media\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1016/j.apsusc.2017.11.076\n",
      "Title:  Electrocatalytic behavior of a nanocomposite of Ni/Pd supported by carbonized PVA nanofibers towards formic acid, ethanol and urea oxidation: a physicochemical …\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1021/acs.cgd.7b01552\n",
      "Title:  Formation and Elimination of Anti-site Defects during Crystallization in Perovskite Ba1–xSrxLiF3\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1021/jacs.0c04747\n",
      "Title:  Hydration of a 2D Supramolecular Assembly: Bitartrate on Cu (110)\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-87775d806350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Options:\\n\\ta) add ack text\\n\\tb) mark as not relevant\\n\\tc) go to next\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"selection:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     \u001b[0musr_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0musr_select\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0;31m#working_file[art_num]['ignore']=3 # visual inspection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         )\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "if current_pass >= 6:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0' and working_file[art_num]['ack_fragment'] == \"\":\n",
    "            article_id = working_file[art_num]['Num']\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            request_str = \"https://doi.org/\" + article_doi \n",
    "            if valid_doi(article_doi):\n",
    "                request_str = \"https://doi.org/\" + article_doi \n",
    "                print(request_str)\n",
    "                \n",
    "                #display(HTML('<h1>Hello, world!</h1>'))\n",
    "                #%%html\n",
    "                #<iframe src=request_str  width=\"600\" height=\"400\"></iframe>\n",
    "                IFrame(request_str, width=700, height=350)\n",
    "                inspected = False\n",
    "                while not inspected:\n",
    "                    #new_title = working_file[art_num]['Title']\n",
    "                    print('Title: ', article_title)\n",
    "                    print('***************************************************************')\n",
    "                    print(\"Options:\\n\\ta) add ack text\\n\\tb) mark as not relevant\\n\\tc) go to next\")\n",
    "                    print(\"selection:\")\n",
    "                    usr_select = input()\n",
    "                    if usr_select == 'c':\n",
    "                        #working_file[art_num]['ignore']=3 # visual inspection\n",
    "                        inspected = True\n",
    "                        working_file[art_num]['send_to_corinne'] = 'no'\n",
    "                        working_file[art_num]['reason_send'] = \"not acknowledged, no UKCH authors\"\n",
    "                        print(\"going to next\")\n",
    "                    elif usr_select == 'b':\n",
    "                        #working_file[art_num]['ignore']=3 # visual inspection\n",
    "                        inspected = True\n",
    "                        print(\"going to next\")\n",
    "                    elif usr_select == 'a':\n",
    "                        inspected = True\n",
    "                        ack_text = \"\"\n",
    "                        while ack_text == \"\":\n",
    "                            print(\"Enter ack text: \")\n",
    "                            ack_text = input()\n",
    "                            working_file[art_num]['ack_fragment'] = ack_text\n",
    "                            working_file[art_num]['send_to_corinne'] = 'yes'\n",
    "                            working_file[art_num]['reason_send'] = \"confirmed in acknowledgements\"\n",
    "            else:\n",
    "                print(article_doi, \"is not a valid DOI\")\n",
    "    csvh.write_csv_data(working_file, nr_wf)  \n",
    "    print(nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get bib data from CR to send for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nr_wf = \"pop_searches/PoPCites20201017_wf.csv\"\n",
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "article_title = \"\"\n",
    "article_doi = \"\"\n",
    "article_url = \"\"\n",
    "data = None    \n",
    "try:\n",
    "    if current_pass >= 6:\n",
    "        for art_num in tqdm_notebook(working_file):\n",
    "            if working_file[art_num]['send_to_corinne'] == 'yes':\n",
    "                article_title = working_file[art_num]['Title']\n",
    "                article_doi = working_file[art_num]['DOIcr']\n",
    "                article_url =working_file[art_num]['ArticleURL']\n",
    "                if valid_doi(article_doi):\n",
    "                    data, file_name = get_cr_json_object(article_doi)\n",
    "                    # get authors\n",
    "                    working_file[art_num]['cr_authors'] = get_cr_author_list(data)\n",
    "                    # get article year\n",
    "                    working_file[art_num]['cr_year'] = get_cr_year_published(data)\n",
    "                    working_file[art_num]['cr_title'] = data['title']\n",
    "                    working_file[art_num]['cr_journal'] = data['container-title']\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "except:\n",
    "    print(article_title, article_doi, article_url)\n",
    "    print(data)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#***************************************************************************************************************\n",
    "# Wait do not run this yet\n",
    "#***************************************************************************************************************\n",
    "if current_pass >= 6:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            print(\"Analysing:\", article_title, article_doi, article_url)\n",
    "            # try to retrive html page for article using link from crossref first\n",
    "            # and if not try url from pop\n",
    "            # find reference to uk catalysis hub in html text\n",
    "            # if found mark as relevant\n",
    "            found = \"\"\n",
    "            referents = [\"uk catalysis hub\", \"uk catalysis\", \"catalysis hub\",\n",
    "                 'EP/R026645/1', 'resources', 'EP/K014668/1', 'EPSRC', 'EP/K014714/1',\n",
    "                 'Hub','provided', 'grant', 'biocatalysis', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                 'support', 'membership', 'EP/M013219/1', 'UK', 'kindly', 'Catalysis',\n",
    "                 'funded', 'EP/R027129/1', 'Consortium', 'thanked', 'EP/K014854/1', 'EP/K014706/2']\n",
    "            found = urlh.findFromDOI(article_title, article_doi, referents)\n",
    "            working_file[art_num]['checked_doi'] = 1\n",
    "            working_file[art_num]['ack_doi'] = found\n",
    "            found = urlh.findFromURI(article_title, article_url, referents)\n",
    "            working_file[art_num]['checked_url'] = 1\n",
    "            working_file[art_num]['ack_url'] = found\n",
    "            print(\"Ack:\", found)\n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_text = '10.1039/d0cy00036a'\n",
    "\n",
    "url_text = \"https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.808495\"\n",
    "\n",
    "html_content, file_name = get_pub_html_doi(doi_text)\n",
    "\n",
    "print(file_name)#, html_content)\n",
    "\n",
    "import re\n",
    "\n",
    "#print(len(doi_text))\n",
    "# CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "# CR DOIs re1\n",
    "# /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "\n",
    "cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "\n",
    "compare = re.match(cr_re_01, doi_text, re.IGNORECASE)\n",
    "\n",
    "print(compare)\n",
    "print(compare.start())\n",
    "print(compare.end())\n",
    "print(compare.group())\n",
    "\n",
    "if compare != None and doi_text == compare.group():\n",
    "    print(\"This is a DOI: \", doi_text)\n",
    "else:\n",
    "    print(\"This is not a DOI: \", doi_text)\n",
    "\n",
    "compare = re.match(cr_re_01, url_text, re.IGNORECASE)\n",
    "    \n",
    "print(url_text, valid_doi(url_text))\n",
    "print(doi_text, valid_doi(doi_text))\n",
    "\n",
    "# url_text = \"https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.808495\"\n",
    "# id = id000069_thesis\n",
    "entry_id = 'id000069_thesis'\n",
    "\n",
    "html_content, file_name = get_pub_html_url(url_text, entry_id)\n",
    "print(file_name, html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_text = '10.1039/d0cy00036a'\n",
    "print(doi_text, valid_doi(doi_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsrc_keys = ['EP/R026645/1', 'EP/K014668/1', 'EP/K014714/1', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                          'EP/M013219/1', 'EP/R027129/1', 'EP/K014854/1', 'EP/K014706/2']\n",
    "', '.join(epsrc_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
