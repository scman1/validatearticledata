{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of references to UK Catalysis Hub \n",
    "A list of articles is obtainded from publish or perish. This list will contain a titles and some IDs whic need to be verified. \n",
    "The alternative criteria for adding a publication to the database are: \n",
    "a) has an explicit acknowledgement of UK Catalysis Hub\n",
    "b) mentions one of the UK Catalysis Hub grants\n",
    "c) has two or more authors with affiliation to UK Catalysis Hub\n",
    "d) acknowledges support from a scientist affiliated to UK Catalysis Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# input files\n",
    "new_results_file = 'pop_searches/PoPCites3.csv'\n",
    "previous_results = 'pop_searches/ukch_pop_2VT.csv'\n",
    "\n",
    "#output files\n",
    "nr_wf = new_results_file[:-4]+\"_wf.csv\"\n",
    "working_filem = wf_fields = None\n",
    "current_pass = 0\n",
    "if Path(nr_wf).is_file():\n",
    "    working_file, wf_fields = csvh.get_csv_data(nr_wf,'Num')\n",
    "    for art_num in working_file:\n",
    "        if current_pass < int(working_file[art_num]['ignore']) :\n",
    "            current_pass = int(working_file[art_num]['ignore'])\n",
    "#print(nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if already processed titles are included\n",
    "Read data and verify if results in file have already been included in previous searches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(nr_wf).is_file():\n",
    "    csv_articles, fn_articles = csvh.get_csv_data(new_results_file,'Num')\n",
    "    prev_articles, fn_prev = csvh.get_csv_data(previous_results,'Num')\n",
    "    # pass 1a exact match\n",
    "    for art_num in csv_articles:\n",
    "        new_title = csv_articles[art_num]['Title']\n",
    "        for prev_num in prev_articles:\n",
    "            prev_articles[prev_num]['Title']\n",
    "            if new_title == prev_articles[prev_num]['Title']:\n",
    "                #print(art_num, 'Title:', csv_articles[art_num]['Title'], \"already processed\", prev_num, prev_articles[prev_num]['Title'])\n",
    "                csv_articles[art_num]['ignore']=1\n",
    "            break\n",
    "        if not 'ignore' in csv_articles[art_num].keys():\n",
    "            csv_articles[art_num]['ignore']=0\n",
    "    # pass 1b approximate match\n",
    "    for art_num in csv_articles:\n",
    "        if csv_articles[art_num]['ignore']==0:\n",
    "            new_title = csv_articles[art_num]['Title']\n",
    "            for prev_num in prev_articles:\n",
    "                if txtc.similar(new_title, prev_articles[prev_num]['Title'])> 0.80:\n",
    "                    #print(art_num, 'Title:', csv_articles[art_num]['Title'], \"already processed\", prev_num, prev_articles[prev_num]['Title'])\n",
    "                    csv_articles[art_num]['ignore']=1\n",
    "                    break\n",
    "    csvh.write_csv_data(csv_articles, nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Title Wording\n",
    "Using the workds in previous catalysis hub papers check if the title is likely to be a cat hub title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_pass > 0 and current_pass < 2:\n",
    "    # pass 2\n",
    "    # check titles for likelihood of being catalysis articles using keywords from titles in current DB \n",
    "    print(\"Get word list from DB\")\n",
    "    x = dbh.DataBaseAdapter('ukch_articles.sqlite')\n",
    "    db_titles = x.get_value_list('articles','title')\n",
    "    title_words = set()\n",
    "    ignore_words=set(['the','of','to','and','a','in','is','it', 'their', 'so', 'as'])\n",
    "    average = 0\n",
    "    words_sum = 0.0\n",
    "    for title in db_titles:\n",
    "        one_title = set(title.lower().split())\n",
    "        one_title = one_title - ignore_words\n",
    "        title_words = title_words.union(one_title)\n",
    "        words_sum += len(one_title) \n",
    "        \n",
    "    average = words_sum /len(db_titles)\n",
    "    print(\"Average words per title:\", average)\n",
    "    title_words = title_words - ignore_words\n",
    "    for art_num in working_file:\n",
    "        if 0 == int(working_file[art_num]['ignore']):\n",
    "            art_title = working_file[art_num]['Title']\n",
    "            art_words = set(art_title.lower().split())\n",
    "            occurrences = len(title_words.intersection(art_words))\n",
    "            if occurrences <= 4:\n",
    "                print(\"occurrences:\", occurrences, \"in title:\", art_title)\n",
    "                working_file[art_num]['ignore']=2\n",
    "            else:\n",
    "                print(\"occurrences:\", occurrences, \"in title:\", art_title)\n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_pass == 2:\n",
    "    i = 0\n",
    "    for art_num in working_file:\n",
    "        #print('Title:', working_file[art_num]['Title'],working_file[art_num]['ignore'])\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            inspected = False\n",
    "            while not inspected:\n",
    "                new_title = working_file[art_num]['Title']\n",
    "                print('Title:', working_file[art_num]['Title'])\n",
    "                print('***************************************************************')\n",
    "                print(\"Oprions:\\n\\ta) add\\n\\tb) ignore\")\n",
    "                print(\"selection:\")\n",
    "                usr_select = input()\n",
    "                if usr_select == 'b':\n",
    "                    working_file[art_num]['ignore']=3 # visual inspection\n",
    "                    inspected = True\n",
    "                elif usr_select == 'a':\n",
    "                    inspected = True\n",
    "            i += 1\n",
    "    print(\"To Process:\", i, \"Pass:\", current_pass)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DOIs for Articles\n",
    "The remaining titles need to be further analysed. Recovering their DOIs can help obtain abstracts and acknowledgement statements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_pass == 3:\n",
    "    i = 0\n",
    "    for art_num in working_file:\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            new_title = working_file[art_num]['Title']\n",
    "            new_doi = cr_api.getDOIForTitle(new_title)\n",
    "            if new_doi == \"\":\n",
    "                print(\"Missing DOI:\", new_title)\n",
    "                i +=1\n",
    "            else:\n",
    "                print(\"DOI found:\", new_doi, \"for:\", new_title)\n",
    "                working_file[art_num]['DOI'] = new_doi\n",
    "                working_file[art_num]['ignore'] = '4'\n",
    "    print(\"without DOI:\", i)\n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify DOIs in DB\n",
    "Verify that articles do not exist in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_pass == 4:\n",
    "    i = 0\n",
    "    db_conn = dbh.DataBaseAdapter('ukch_articles.sqlite')\n",
    "    for art_num in working_file:\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            new_title = working_file[art_num]['Title']\n",
    "            new_doi = working_file[art_num]['DOI']\n",
    "            db_title = db_conn.get_title(new_doi)\n",
    "            if db_title == None:\n",
    "                print(\"Not in DB:\", new_doi, new_title)\n",
    "            else:\n",
    "                print(\"Already in DB:\", new_doi, \"for:\", new_title, db_title)\n",
    "                working_file[art_num]['ignore'] = '5'\n",
    "    print(\"without DOI:\", i)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Acknowledgement statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing: A design of a fixed bed plasma DRIFTS cell for studying the NTP-assisted heterogeneously catalysed reactions 10.1039/d0cy00036a https://pubs.rsc.org/en/content/articlehtml/2020/cy/d0cy00036a\n",
      "Ack: \n"
     ]
    }
   ],
   "source": [
    "if current_pass > 5:\n",
    "    i = 0\n",
    "    for art_num in working_file:\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOI']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            print(\"Analysing:\", article_title, article_doi, article_url)\n",
    "            # try to retrive html page for article using link from crossref first\n",
    "            # and if not try url from pop\n",
    "            # find reference to uk catalysis hub in html text\n",
    "            # if found mark as relevant\n",
    "            found = \"\"\n",
    "            referents = [\"uk catalysis hub\", \"uk catalysis\", \"catalysis hub\",\n",
    "                 'EP/R026645/1', 'resources', 'EP/K014668/1', 'EPSRC', 'EP/K014714/1',\n",
    "                 'Hub','provided', 'grant', 'biocatalysis', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                 'support', 'membership', 'EP/M013219/1', 'UK', 'kindly', 'Catalysis',\n",
    "                 'funded', 'EP/R027129/1', 'Consortium', 'thanked', 'EP/K014854/1', 'EP/K014706/2']\n",
    "            \n",
    "            found = urlh.findFromDOI(article_title, article_doi, referents)\n",
    "            working_file[art_num]['checked_doi'] = 1\n",
    "            working_file[art_num]['ack_doi'] = found\n",
    "            found = urlh.findFromURI(article_title, article_url, referents)\n",
    "            working_file[art_num]['checked_url'] = 1\n",
    "            working_file[art_num]['ack_url'] = found\n",
    "            print(\"Ack:\", found)\n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
