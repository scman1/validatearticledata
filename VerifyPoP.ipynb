{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of references to UK Catalysis Hub \n",
    "A list of articles is obtainded from publish or perish. This list will contain a titles and some IDs whic need to be verified. \n",
    "\n",
    "The criteria for adding a publication to the database are: \n",
    "a) has an explicit acknowledgement of UK Catalysis Hub\n",
    "b) mentions one of the UK Catalysis Hub grants\n",
    "c) has two or more authors with affiliation to UK Catalysis Hub\n",
    "d) acknowledges support from a scientist affiliated to UK Catalysis Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the crossreference json page from doi\n",
    "def get_cr_json_object(cr_doi):\n",
    "  crjd = None\n",
    "  doi_file = 'json_files/' + cr_doi.replace('/','_').lower() + '.json'\n",
    "  if not Path(doi_file).is_file():\n",
    "    crjd = cr_api.getBibData(cr_doi)\n",
    "    with open(doi_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(crjd, f, ensure_ascii=False, indent=4)\n",
    "  else:\n",
    "    jf = open(doi_file, 'r')\n",
    "    crjd = json.load(jf)\n",
    "  # return the content and the file name \n",
    "  return crjd, doi_file\n",
    "\n",
    "# get the landing page for the publication from uri\n",
    "\n",
    "                    \n",
    "def get_titles(str_pub_title, db_name = \"prev_search.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'prev_pop_searches'\n",
    "    fields_required = \"Num, Title\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "def get_titles_and_dois(str_pub_title, db_name = \"app_db.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "# get the current csv working file\n",
    "def get_working_file(nr_wf):\n",
    "    working_file = wf_fields = None\n",
    "    current_pass = 0\n",
    "    if Path(nr_wf).is_file():\n",
    "        working_file, wf_fields = csvh.get_csv_data(nr_wf,'Num')\n",
    "        for art_num in tqdm_notebook(working_file):\n",
    "            if current_pass < int(working_file[art_num]['ignore']):\n",
    "                current_pass = int(working_file[art_num]['ignore'])\n",
    "    print(\"Current pass:\", current_pass)\n",
    "    return working_file, wf_fields, current_pass\n",
    "\n",
    "def get_pub_html_doi(cr_doi):\n",
    "    html_file = 'html_files/' + cr_doi.replace('/','_').lower() + '.html'\n",
    "    if not Path(html_file).is_file():\n",
    "        page_content = urlh.getPageFromDOI(doi_text)\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(page_content.decode(\"utf-8\") )\n",
    "    else:\n",
    "        f = open(html_file, \"r\")\n",
    "        page_content = f.read()\n",
    "    return page_content, html_file\n",
    "\n",
    "def get_pub_html_url(text_url, entry_id):\n",
    "    html_file = 'html_files/' +  entry_id + '.html'\n",
    "    if not Path(html_file).is_file():\n",
    "        print(\"\")\n",
    "        page_content = urlh.getPageFromURL(text_url)\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(page_content)\n",
    "    else:\n",
    "        f = open(html_file, \"r\")\n",
    "        page_content = f.read()\n",
    "    return page_content, html_file\n",
    "\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a DOI:  10.1039/d0cy00036a\n",
      "10.1039/d0cy00036a True\n"
     ]
    }
   ],
   "source": [
    "doi_text = '10.1039/d0cy00036a'\n",
    "print(doi_text, valid_doi(doi_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html_files/10.1039_d0cy00036a.html\n",
      "<re.Match object; span=(0, 18), match='10.1039/d0cy00036a'>\n",
      "0\n",
      "18\n",
      "10.1039/d0cy00036a\n",
      "This is a DOI:  10.1039/d0cy00036a\n",
      "This is not a DOI:  10.1039/d0cy00036a\n",
      "https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.808495 False\n",
      "This is a DOI:  10.1039/d0cy00036a\n",
      "10.1039/d0cy00036a True\n",
      "html_files/id000069_thesis.html \n"
     ]
    }
   ],
   "source": [
    "doi_text = '10.1039/d0cy00036a'\n",
    "\n",
    "url_text = \"https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.808495\"\n",
    "\n",
    "html_content, file_name = get_pub_html_doi(doi_text)\n",
    "\n",
    "print(file_name)#, html_content)\n",
    "\n",
    "import re\n",
    "\n",
    "#print(len(doi_text))\n",
    "# CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "# CR DOIs re1\n",
    "# /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "\n",
    "cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "\n",
    "compare = re.match(cr_re_01, doi_text, re.IGNORECASE)\n",
    "\n",
    "print(compare)\n",
    "print(compare.start())\n",
    "print(compare.end())\n",
    "print(compare.group())\n",
    "\n",
    "if compare != None and doi_text == compare.group():\n",
    "    print(\"This is a DOI: \", doi_text)\n",
    "else:\n",
    "    print(\"This is not a DOI: \", doi_text)\n",
    "\n",
    "compare = re.match(cr_re_01, url_text, re.IGNORECASE)\n",
    "    \n",
    "print(url_text, valid_doi(url_text))\n",
    "print(doi_text, valid_doi(doi_text))\n",
    "\n",
    "# url_text = \"https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.808495\"\n",
    "# id = id000069_thesis\n",
    "entry_id = 'id000069_thesis'\n",
    "\n",
    "html_content, file_name = get_pub_html_url(url_text, entry_id)\n",
    "print(file_name, html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the file with the results of the PoP search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file with path: pop_searches/PoPCites20201017.csv\n",
    "new_results_file = \"\"\n",
    "while not Path(new_results_file).is_file():\n",
    "    print('Please enter the name of the input file:')\n",
    "    new_results_file = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the db file with previous results of the PoP search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous results db file with path: db_files/prev_search.sqlite3\n",
    "\n",
    "previous_db = \"\"\n",
    "while not Path(previous_db).is_file():\n",
    "    print('Please enter the name of the previous results file:')\n",
    "    previous_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of the previous results file:')\n",
    "    ukchapp_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the name of the output file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_wf = new_results_file[:-4]+\"_wf.csv\"\n",
    "print(\"Verifying if the articles listed in: \\n\\t\", Path(new_results_file).name)\n",
    "print(\"where included in previous searches: \\n\\t\", Path(previous_db).name)\n",
    "\n",
    "print(\"The results will bt saved in: \\n\\t\", nr_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the working file before each step\n",
    "working_file = wf_fields = None\n",
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if already processed titles are included\n",
    "Read data and verify if results in file have already been included in previous searches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass == 0:\n",
    "    current_initial = \"\"\n",
    "    db_titles = []\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        new_title = working_file[art_num]['Title'].lower()\n",
    "        working_file[art_num]['ignore'] = 0 \n",
    "        working_file[art_num]['previous'] = 0 \n",
    "        working_file[art_num]['similarity'] = 0.0\n",
    "        if current_initial == \"\" or current_initial != new_title[0]:\n",
    "            print(\"new intital \", new_title[0])\n",
    "            current_initial = new_title[0]\n",
    "            db_titles = get_titles(current_initial, previous_db)\n",
    "            \n",
    "        for prev_pair in db_titles:\n",
    "            prev_num = prev_pair[0]\n",
    "            used_title = prev_pair[1].lower()\n",
    "            # if titles match exactly or simialarity > 0.8 ignore\n",
    "            title_similarity = txtc.similar(new_title, used_title)\n",
    "            if title_similarity > 0.80:\n",
    "                #print(art_num, 'Title:', new_title, \"already processed\", prev_num, used_title)\n",
    "                working_file[art_num]['ignore'] = 1\n",
    "                working_file[art_num]['previous'] = prev_num\n",
    "                working_file[art_num]['similarity'] = title_similarity\n",
    "                break\n",
    "\n",
    "    csvh.write_csv_data(working_file, nr_wf)  \n",
    "    print(nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Titles in app\n",
    "Verify if the title is in the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "# verify that titles are not in the app_db (if they are  also get DOI)\n",
    "if current_pass == 1: \n",
    "    db_titles = []\n",
    "    current_initial = \"\"\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            new_title = working_file[art_num]['Title'].lower()\n",
    "            if current_initial == \"\" or current_initial != new_title[0]:\n",
    "                print(\"new intital \", new_title[0])\n",
    "                current_initial = new_title[0]\n",
    "                db_titles = get_titles_and_dois(current_initial, ukchapp_db)\n",
    "            for art_in_db in db_titles:\n",
    "                prev_num = art_in_db[0]\n",
    "                used_title = art_in_db[1].lower()\n",
    "                # if titles match exactly or simialarity > 0.8 ignore\n",
    "                title_similarity = txtc.similar(new_title, used_title)\n",
    "                if title_similarity > 0.80:\n",
    "                    #print(art_num, 'Title:', new_title, \"already processed\", prev_num, used_title)\n",
    "                    working_file[art_num]['ignore'] = 2\n",
    "                    working_file[art_num]['previous'] = prev_num\n",
    "                    working_file[art_num]['similarity'] = title_similarity\n",
    "                    working_file[art_num]['DOIcr'] = art_in_db[2]\n",
    "                    break                \n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Title Wording\n",
    "Using the workds in previous catalysis hub papers check if the title is likely to be a cat hub title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass < 3:\n",
    "    # pass 2\n",
    "    # check titles for likelihood of being catalysis articles using keywords from titles in current DB \n",
    "    print(\"Get word list from DB\")\n",
    "    x = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    db_titles = x.get_value_list('articles','title')\n",
    "    title_words = set()\n",
    "    ignore_words=set(['the','of','to','and','a','in','is','it', 'their', 'so', 'as'])\n",
    "    average = 0\n",
    "    words_sum = 0.0\n",
    "    for title in db_titles:\n",
    "        one_title = set(title.lower().split())\n",
    "        one_title = one_title - ignore_words\n",
    "        title_words = title_words.union(one_title)\n",
    "        words_sum += len(one_title) \n",
    "        \n",
    "    average = words_sum /len(db_titles)\n",
    "    print(\"Average words per title:\", average)\n",
    "    title_words = title_words - ignore_words\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if 0 == int(working_file[art_num]['ignore']):\n",
    "            art_title = working_file[art_num]['Title']\n",
    "            art_words = set(art_title.lower().split())\n",
    "            occurrences = len(title_words.intersection(art_words))\n",
    "            working_file[art_num]['keywords']=occurrences\n",
    "            if occurrences == 0:\n",
    "                print(\"occurrences:\", occurrences, \"in title:\", art_title)\n",
    "                working_file[art_num]['ignore']=3\n",
    "            else:\n",
    "                print(\"occurrences:\", occurrences, \"in title:\", art_title)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    x.close()\n",
    "    current_pass = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass == 3:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            inspected = False\n",
    "            while not inspected:\n",
    "                new_title = working_file[art_num]['Title']\n",
    "                keywords = working_file[art_num]['keywords']\n",
    "                #print (keywords, new_title)\n",
    "                if keywords < 3 and not (\"cataly\" in new_title.lower()):\n",
    "                # ignore  it because it does not contains cataly in title\n",
    "                    working_file[art_num]['ignore']=4 # visual inspection\n",
    "                    inspected = True\n",
    "                else:\n",
    "                    inspected = True\n",
    "    print(\"To Process:\", i, \"Pass:\", current_pass)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DOIs for Articles\n",
    "The remaining titles need to be further analysed. Recovering their DOIs helps to obtain abstracts and acknowledgement statements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass == 4:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0' and working_file[art_num]['DOIcr']==\"\":\n",
    "            new_title = working_file[art_num]['Title']\n",
    "            new_doi = cr_api.getDOIForTitle(new_title)\n",
    "            if new_doi == \"\":\n",
    "                #print(\"Missing DOI:\", new_title)\n",
    "                working_file[art_num]['ignore'] = '5'\n",
    "                i +=1\n",
    "            else:\n",
    "                #print(\"DOI found:\", new_doi, \"for:\", new_title)\n",
    "                working_file[art_num]['DOIcr'] = new_doi\n",
    "                working_file[art_num]['ignore'] = '0'\n",
    "    print(\"without DOI:\", i)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify DOIs in DB\n",
    "Verify that articles do not exist in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass == 5:\n",
    "    i = 0\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            new_title = working_file[art_num]['Title']\n",
    "            new_doi = working_file[art_num]['DOIcr'].strip()\n",
    "            db_title = db_conn.get_title(new_doi)\n",
    "            if db_title == None:\n",
    "                print(\"Not in DB:\", new_doi, new_title)\n",
    "            else:\n",
    "                print(\"Already in DB:\", new_doi, \"for:\", new_title, db_title)\n",
    "                working_file[art_num]['ignore'] = '6'\n",
    "    print(\"without DOI:\", i)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 6\n",
    "    dbh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full json files for remaining articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 6:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            data, file_name = get_cr_json_object(article_doi)\n",
    "            if data != {}:\n",
    "                working_file[art_num]['file'] = file_name\n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217e3e14fae24afdb7124c7503f25b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5410e43909a9415eabf26b9af39e62ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 A comparative study into the effects of pre and post catalyst exhaust gas recirculation on the onset of knock 10.1177/1468087420962294\n",
      "2 A design of a fixed bed plasma DRIFTS cell for studying the NTP-assisted heterogeneously catalysed reactions 10.1039/d0cy00036a\n",
      "Found EP/R026939/1\n",
      "Found EP/R026645/1\n",
      "Found EP/R026815/1\n",
      "Found EP/R027129/1\n",
      "3 Adsorption and activation of molecular oxygen over atomic copper (I/II) site on ceria 10.1038/s41467-020-17852-8\n",
      "4 Alcohol Dehydrogenase Triggered Oxa‐Michael Reaction for the Asymmetric Synthesis of Disubstituted Tetrahydropyrans and Tetrahydrofurans 10.1002/cctc.201900658\n",
      "Found EP/K014668/1\n",
      "Found EP/K014706/2\n",
      "Found EP/K014854/1\n",
      "Found EP/M013219/1\n",
      "5 An approach to calculate the free energy changes of surface reactions using free energy decomposition on ab initio brute-force molecular dynamics … 10.1039/d0cp03852k\n",
      "6 Anchor entrepreneurship and industry catalysis: The rise of the Italian Biomedical Valley 10.1016/j.respol.2020.104045\n",
      "7 Application of “Smart” Amine Donors for Rapid Screening and Scale‐Up of Transaminase‐Mediated Biotransformations 10.1002/ejoc.201800799\n",
      "8 Application of new nanoparticle structures as catalysts: general discussion 10.1039/c8fd90016g\n",
      "9 Asymmetric Construction of Alkaloids Employing a Key ω‐Transaminase Cascade 10.1002/chem.202000067\n",
      "10 Asymmetric synthesis of primary amines catalyzed by thermotolerant fungal reductive aminases 10.1039/d0sc02253e\n",
      "11 Atmospheric Pressure and Room Temperature Synthesis of Methanol through Plasma-Catalytic Hydrogenation of CO2 10.1021/acscatal.7b02733\n",
      "12 Biocatalytic self-assembly on magnetic nanoparticles 10.1021/acsami.7b15456\n",
      "13 Biomass hydrodeoxygenation catalysts innovation from atomistic activity predictors 10.1098/rsta.2020.0056\n",
      "Found EP/K014714/1\n",
      "Found EP/K014668/1\n",
      "Found EP/K014854/1\n",
      "Found EP/M013219/1\n",
      "14 Carbidisation of Pd nanoparticles by ethene decomposition, with methane production 10.1002/cctc.201900795\n",
      "Found EP/K014714/1\n",
      "15 Catalysis of the oxygen evolution reaction by 4–10 nm cobalt nanoparticles 10.1007/s11244-018-0923-4\n",
      "16 Catalyst control of selectivity in the C–O bond alumination of biomass derived furans 10.1039/d0sc01918f\n",
      "17 Catalytic activities of heterogeneous catalysts obtained by copolymerization of metal-containing 2-(acetoacetoxy) ethyl methacrylate 10.1515/chem-2018-0055\n",
      "18 Catalytic and biophysical investigation of rhodium hydroformylase 10.1039/c9cy01679a\n",
      "Found EP/K014714/1\n",
      "Found EP/K014668/1\n",
      "Found EP/K014706/2\n",
      "Found EP/K014854/1\n",
      "Found EP/M013219/1\n",
      "19 CHALLENGES FOR ORGANOCATALYSIS 10.1142/9789813237179_0010\n",
      "20 CO and O2 Adsorption on K/Pt(111) 10.1021/acs.jpcc.8b08461\n",
      "21 Cobalt-containing zeolitic imidazole frameworks for C–H activation using visible-light redox photocatalysis 10.1039/d0cy01061h\n",
      "22 Cofactor NAD (P) H Regeneration: How Selective Are the Reactions? 10.1016/j.trechm.2020.03.002\n",
      "23 Combined computational and neutron scattering studies of hydrocarbons confined in mesoporous materials thesis\n",
      "24 Computational studies of DNA base repair mechanisms by nonheme iron dioxygenases: selective epoxidation and hydroxylation pathways 10.1039/d0dt00007h\n",
      "Found EP/K014854/1\n",
      "Found EP/M013219/1\n",
      "Found EP/R026815/1\n",
      "25 Confinement Effects on the Benzene Orientational Structure 10.1002/anie.201713115\n",
      "Found EP/K014714/1\n",
      "Found EP/K014668/1\n",
      "Found EP/K014854/1\n",
      "Found EP/M013219/1\n",
      "26 Correction: In situ spectroscopic investigations of MoOx/Fe2O3 catalysts for the selective oxidation of methanol 10.1039/c8cy90033g \n",
      "27 Dataset for the publication:\" The methyl torsion in unsaturated compounds\" published in ACS Omega (2019) by Zachariou et al. 10.5286/edata/736\n",
      "28 Design, Identification and Evolution of Surface Ruthenium (II/III) Single‐Site for CO Activation 10.1002/ange.202008370\n",
      "29 Detection of catalytic intermediates at an electrode surface during carbon dioxide reduction by an earth-abundant catalyst 10.1038/s41929-018-0169-3\n",
      "30 Direct Synthesis of Amides from Nonactivated Carboxylic Acids using Urea as Nitrogen Source and Mg (NO 3) 2 or Imidazole as Catalysts 10.1039/d0sc01317j\n",
      "31 Effect of mass transport on the electrochemical oxidation of alcohols over electrodeposited film and carbon-supported Pt electrodes 10.1007/s11244-018-0893-6\n",
      "32 Effect of the addition of different doping agents on visible light activity of porous TiO2 photocatalysts 10.1016/j.mcat.2018.06.002\n",
      "33 Effects of distal mutations on the structure, dynamics and catalysis of human monoacylglycerol lipase 10.1038/s41598-017-19135-7\n",
      "34 Effects of zeolite particle size and internal grain boundaries on Pt/Beta catalyzed isomerization of n-pentane 10.1016/j.jcat.2018.01.033\n",
      "35 Electrocatalytic activity of CoFe2O4 thin films prepared by AACVD towards the oxygen evolution reaction in alkaline media 10.1016/j.elecom.2017.12.017\n",
      "36 Electrocatalytic behavior of a nanocomposite of Ni/Pd supported by carbonized PVA nanofibers towards formic acid, ethanol and urea oxidation: a physicochemical … 10.1016/j.apsusc.2017.11.076\n",
      "37 Enantioselective Synthesis of Chiral Vicinal Amino Alcohols Using Amine Dehydrogenases 10.1021/acscatal.9b03889\n",
      "38 Enhancement in the rate of nitrate degradation on Au-and Ag-decorated TiO 2 photocatalysts 10.1039/c9cy02473e\n",
      "Found EP/K014714/1\n",
      "Found EP/K014668/1\n",
      "Found EP/K014706/2\n",
      "Found EP/K014854/1\n",
      "39 Enlighten2: molecular dynamics simulations of protein-ligand systems made accessible 10.1093/bioinformatics/btaa643\n",
      "Found EP/M013219/1\n",
      "40 Formation and Elimination of Anti-site Defects during Crystallization in Perovskite Ba1–xSrxLiF3 10.1021/acs.cgd.7b01552\n",
      "41 Hydration of a 2D Supramolecular Assembly: Bitartrate on Cu (110) 10.1021/jacs.0c04747\n",
      "42 Hydrogen generation by photocatalytic reforming of potential biofuels: Polyols, cyclic alcohols, and saccharides 10.1016/j.jphotochem.2018.01.031\n",
      "Found EP/K014668/1\n",
      "Found EP/M013219/1\n",
      "43 Hydrogen production from formic acid decomposition in the liquid phase using Pd nanoparticles supported on CNFs with different surface properties 10.1039/c8se00338f\n",
      "44 Hydrogenation of Carbon Dioxide with Organic Base by PCIIP-Ir Catalysts 10.1021/acs.organomet.8b00377\n",
      "45 Improving Synchrotron Methods for Advanced Characterisation of Heterogeneous Catalysts thesis\n",
      "46 In Situ Monitoring of Nanoparticle Formation during Iridium‐Catalysed Oxygen Evolution by Real‐Time Small Angle X‐Ray Scattering 10.1002/cctc.201901268\n",
      "47 Investigation of ZSM-5 catalysts for dimethylether conversion using inelastic neutron scattering 10.1016/j.apcata.2018.10.010\n",
      "Found EP/K014714/1\n",
      "Found EP/K014668/1\n",
      "Found EP/K014854/1\n",
      "Found EP/M013219/1\n",
      "48 Iron porphyrin-derived ordered carbonaceous frameworks 10.1016/j.cattod.2020.07.003\n",
      "49 Lonely atoms with special gifts: Breaking linear scaling relationships in heterogeneous catalysis with single-atom alloys 10.1021/acs.jpclett.8b01888\n",
      "50 Mechanistic Insight into the Framework Methylation of H-ZSM-5 for Varying Methanol Loadings and Si/Al Ratios Using First-Principles Molecular Dynamics … 10.1021/acscatal.0c01454\n",
      "51 Membrane-Supported Recovery of Homogeneous Organocatalysts: A Review 10.3390/chemistry2030048\n",
      "52 Metal Oxide Preparation for Heterogeneous Catalysis thesis\n",
      "53 Methanation of Carbon Dioxide over Zeolite‐Encapsulated Nickel Nanoparticles 10.1002/cctc.201701946\n",
      "54 Methanol oxidation over shell-core MOx/Fe2O3 (M= Mo, V, Nb) catalysts 10.1016/s1872-2067(19)63350-4\n",
      "Found EP/K014714/1\n",
      "Found EP/K014854/1\n",
      "55 Methanol photo-reforming with water on pure titania for hydrogen production 10.1098/rsta.2020.0058\n",
      "56 MFI zeolite coating with intrazeolitic aluminum (acidic) gradient supported on SiC foams to improve the methanol-to-propylene (MTP) reaction 10.1016/j.apcata.2018.04.006\n",
      "57 Mild and Chemoselective, Light-Driven Post-translational Modification Allows Installation of Reactive Side-Chains into Protein dataset\n",
      "58 Molecular Cobalt Catalysts Grafted onto Polymers for Efficient Hydrogen Generation Cathodes 10.1002/solr.202000281\n",
      "59 N–N Bond Formation using an Iodonitrene as an Umpolung of Ammonia: Straightforward and Chemoselective Synthesis of Hydrazinium Salts 10.1002/adsc.202001047\n",
      "60 Neutron spectroscopy studies of methanol to hydrocarbons catalysis over ZSM-5 10.1016/j.cattod.2020.05.030\n",
      "Found EP/K014714/1\n",
      "Found EP/K014668/1\n",
      "Found EP/K014854/1\n",
      "Found EP/M013219/1\n",
      "61 Ni and Cu ion-exchanged nanostructured mesoporous zeolite: A noble metal free, efficient, and durable electrocatalyst for alkaline methanol oxidation reaction 10.1016/j.mtener.2018.02.007\n",
      "62 Observation of visible light activated photocatalytic degradation of stearic acid on thin films of tantalum oxynitride synthesized by aerosol assisted chemical vapour … 10.1039/c8dt04638g\n",
      "63 Octane isomer dynamics in H-ZSM-5 as a function of Si/Al ratio: a quasi-elastic neutron scattering study 10.1098/rsta.2020.0063\n",
      "64 Octane isomer dynamics in H-ZSM-5 as a function of Si/Al ratio: a quasi-elastic neutron scattering study 10.1098/rsta.2020.0063\n",
      "65 Operando potassium K-edge X-ray absorption spectroscopy: investigating potassium catalysts during soot oxidation 10.1039/d0cp01227k\n",
      "Found EP/K014854/1\n",
      "66 Operando Spectroscopic Studies of Cu–SSZ-13 for NH3–SCR deNOx Investigates the Role of NH3 in Observed Cu(II) Reduction at High NO Conversions 10.1007/s11244-018-0888-3\n",
      "67 Palladium‐Catalyzed C(sp3)−H Arylation of Primary Amines Using a Catalytic Alkyl Acetal to Form a Transient Directing Group 10.1002/chem.201804515\n",
      "68 Pendant Hydrogen-Bond Donors in Cobalt Catalysts Independently Enhance CO2 Reduction 10.1021/acscentsci.7b00607\n",
      "69 Photocatalytic removal of the cyanobacterium Microcystis aeruginosa PCC7813 and four microcystins by TiO2 coated porous glass beads with UV-LED irradiation 10.1016/j.scitotenv.2020.141154\n",
      "70 Policies and motivations for the CO2 valorization through the sabatier reaction using structured catalysts. A review of the most recent advances 10.3390/catal8120578\n",
      "71 Preface for the Issue of Topics in Catalysis Dedicated to the Memory of Professor Frank S. Stone 10.1007/s11244-019-01204-y\n",
      "72 Preparation of a highly active ternary Cu-Zn-Al oxide methanol synthesis catalyst by supercritical CO2 anti-solvent precipitation 10.1016/j.cattod.2018.03.046\n",
      "Found EP/K014714/1\n",
      "Found EP/K014668/1\n",
      "73 Preparation of bifunctional Au-Pd/TiO2 catalysts and research on methanol liquid phase one-step oxidation to methyl formate 10.1016/j.cattod.2018.01.033\n",
      "74 Probing the electronic and geometric structures of photoactive electrodeposited Cu2O films by X-ray absorption spectroscopy 10.1016/j.jcat.2020.06.021\n",
      "75 Protecting Ceria Nanocatalysts—The Role of Sacrificial Barriers 10.1021/acsami.8b08674\n",
      "76 Protein kinase Cα gain-of-function variant in Alzheimer's disease displays enhanced catalysis by a mechanism that evades down-regulation 10.1073/pnas.1805046115\n",
      "77 Protonation-Induced Dynamic Allostery in PDZ Domain: Evidence of Perturbation-Independent Universal Response Network 10.1021/acs.jpclett.0c02885\n",
      "78 Rapid synthesis of [Au25 (Cys) 18] nanoclusters via carbon monoxide in microfluidic liquid-liquid segmented flow system and their antimicrobial performance 10.1016/j.cej.2019.123176\n",
      "Found EP/K014714/1\n",
      "Found EP/K014668/1\n",
      "Found EP/K014706/2\n",
      "Found EP/K014854/1\n",
      "Found EP/M013219/1\n",
      "79 Re-evaluating how charge transfer modifies the conformation of adsorbed molecules 10.1039/c8nr02237b\n",
      "80 Real-time tomographic diffraction imaging of catalytic membrane reactors for the oxidative coupling of methane 10.1016/j.cattod.2020.05.045\n",
      "81 Redesigning the materials and catalysts database construction process using ontologies 10.1021/acs.jcim.8b00165\n",
      "82 Reversible C− H Activation, Facile C− B/B− H Metathesis and Apparent Hydroboration Catalysis by a Dimethylxanthene‐Based Frustrated Lewis Pair 10.1002/chem.201801871\n",
      "Found EP/K014714/1\n",
      "83 Selection of Manganese oxide catalysts for catalytic oxidation of Carbon monoxide at ambient conditions 10.1016/j.resenv.2020.100003\n",
      "84 Strain engineering and Raman spectroscopy of monolayer transition metal dichalcogenides 10.1021/acs.chemmater.8b01672.s001\n",
      "85 Strategies for the deposition of LaFeO 3 photocathodes: improving the photocurrent with a polymer template 10.1039/c9se01103j\n",
      "Found EP/K014714/1\n",
      "86 Structured Ni@ NaA zeolite supported on silicon carbide foam catalysts for catalytic carbon dioxide methanation 10.1002/aic.17007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 Sulfated Zirconia Catalysts for D-Sorbitol Cascade Cyclodehydration to Isosorbide: Impact of Zirconia Phase 10.1021/acssuschemeng.8b03268\n",
      "88 Synchrotron Radiation and Neutrons for Catalysis, Materials Research and Development 10.1080/08940886.2018.1460180\n",
      "89 Synthesis and characterization of AgCoO2 catalyst for oxidation of CO at a low temperature 10.1016/j.poly.2018.08.027\n",
      "90 Synthesis, characterisation and water-gas shift activity of nano-particulate mixed-metal (Al, Ti) cobalt oxides 10.1039/c9dt01634a\n",
      "91 The Cardiff Catalysis Institute: A Premium Research Institute 10.21820/23987073.2018.9.65\n",
      "92 The direct synthesis of hydrogen peroxide over Au and Pd nanoparticles: A DFT study 10.1016/j.cattod.2020.09.001\n",
      "93 The role of computational results databases in accelerating the discovery of catalysts 10.1038/s41929-018-0176-4\n",
      "94 Towards a deeper understanding of catalytic activity in supported precious metal catalysts, EPSRC 10.21820/23987073.2018.5.16\n",
      "95 Two 3′-O-β-glucosylated nucleoside fluorometabolites related to nucleocidin in Streptomyces calvus 10.1039/c9sc03374b\n",
      "96 Understanding structure-activity relationships in highly active La promoted Ni catalysts for CO2 methanation 10.1016/j.apcatb.2020.119256\n",
      "97 Visible light sensitive activated carbon-metal oxide (TiO2, WO3, NiO, and SnO) nano-catalysts for photo-degradation of methylene blue: a comparative study 10.1080/02772248.2018.1497634\n",
      "98 Waste not, want not: CO2 (re) cycling into block polymers 10.1039/c9cc02459j\n",
      "Found EP/K014668/1\n",
      "99 Water oxidation catalysed by quantum-sized BiVO 4 10.1039/c8ta08015a\n",
      "100 Light-driven post-translational installation of reactive protein side chains of reactive protein side chains 10.1038/s41586-020-2733-7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 6:\n",
    "    i = 1\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            data, file_name = get_cr_json_object(article_doi)\n",
    "            print(i, article_title, article_doi)\n",
    "            #print(data.keys())\n",
    "            epsrc_keys = ['EP/R026645/1', 'EP/K014668/1', 'EP/K014714/1', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                          'EP/M013219/1', 'EP/R027129/1', 'EP/K014854/1', 'EP/K014706/2']\n",
    "            confirmed_in_cr = []\n",
    "            if 'funder' in data.keys():\n",
    "                for a_funder in data['funder']:\n",
    "                    for an_award in a_funder['award']:\n",
    "                        if an_award in epsrc_keys:\n",
    "                            print(\"Found\", an_award)\n",
    "                            confirmed_in_cr.append(an_award)\n",
    "                working_file[art_num]['award_in_cr'] = ', '.join(confirmed_in_cr)\n",
    "            i += 1\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EP/R026645/1, EP/K014668/1, EP/K014714/1, EP/R026815/1, EP/R026939/1, EP/M013219/1, EP/R027129/1, EP/K014854/1, EP/K014706/2'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsrc_keys = ['EP/R026645/1', 'EP/K014668/1', 'EP/K014714/1', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                          'EP/M013219/1', 'EP/R027129/1', 'EP/K014854/1', 'EP/K014706/2']\n",
    "', '.join(epsrc_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full HTML files for remaining articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4a7f0458154467ad58fb6bd86e22d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ffad31bab64779955c80203db47bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nr_wf = \"pop_searches/PoPCites20201017_wf.csv\"\n",
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 6:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_id = working_file[art_num]['Num']\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr'].strip().lower()\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            article_type =working_file[art_num]['type']\n",
    "            html_content = file_name = None\n",
    "            if valid_doi(article_doi):\n",
    "                html_content, file_name = get_pub_html_doi(article_doi)\n",
    "            else:\n",
    "                #try with url\n",
    "                html_content = None\n",
    "                #identifier = \"id\" + str((1000000 + int(article_id)))[1,6] + article_type \n",
    "                #html_content, file_name = get_pub_html_doi(article_url, identifier)\n",
    "            if html_content != None:\n",
    "                working_file[art_num]['html_file'] = file_name\n",
    "                \n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#***************************************************************************************************************\n",
    "# Wait do not run this yet\n",
    "#***************************************************************************************************************\n",
    "if current_pass >= 6:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            print(\"Analysing:\", article_title, article_doi, article_url)\n",
    "            # try to retrive html page for article using link from crossref first\n",
    "            # and if not try url from pop\n",
    "            # find reference to uk catalysis hub in html text\n",
    "            # if found mark as relevant\n",
    "            found = \"\"\n",
    "            referents = [\"uk catalysis hub\", \"uk catalysis\", \"catalysis hub\",\n",
    "                 'EP/R026645/1', 'resources', 'EP/K014668/1', 'EPSRC', 'EP/K014714/1',\n",
    "                 'Hub','provided', 'grant', 'biocatalysis', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                 'support', 'membership', 'EP/M013219/1', 'UK', 'kindly', 'Catalysis',\n",
    "                 'funded', 'EP/R027129/1', 'Consortium', 'thanked', 'EP/K014854/1', 'EP/K014706/2']\n",
    "            found = urlh.findFromDOI(article_title, article_doi, referents)\n",
    "            working_file[art_num]['checked_doi'] = 1\n",
    "            working_file[art_num]['ack_doi'] = found\n",
    "            found = urlh.findFromURI(article_title, article_url, referents)\n",
    "            working_file[art_num]['checked_url'] = 1\n",
    "            working_file[art_num]['ack_url'] = found\n",
    "            print(\"Ack:\", found)\n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
