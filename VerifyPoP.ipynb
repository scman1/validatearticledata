{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of references to UK Catalysis Hub \n",
    "A list of articles is obtainded from publish or perish. This list will contain a titles and some IDs whic need to be verified. \n",
    "\n",
    "The criteria for adding a publication to the database are: \n",
    "a) has an explicit acknowledgement of UK Catalysis Hub\n",
    "b) mentions one of the UK Catalysis Hub grants\n",
    "c) has two or more authors with affiliation to UK Catalysis Hub\n",
    "d) acknowledges support from a scientist affiliated to UK Catalysis Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the crossreference json page from doi\n",
    "def get_cr_json_object(cr_doi):\n",
    "  crjd = None\n",
    "  doi_file = 'json_files/' + cr_doi.replace('/','_').lower() + '.json'\n",
    "  if not Path(doi_file).is_file():\n",
    "    crjd = cr_api.getBibData(cr_doi)\n",
    "    with open(doi_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(crjd, f, ensure_ascii=False, indent=4)\n",
    "  else:\n",
    "    jf = open(doi_file, 'r')\n",
    "    crjd = json.load(jf)\n",
    "  # return the content and the file name \n",
    "  return crjd, doi_file\n",
    "\n",
    "# get the landing page for the publication from uri\n",
    "def get_pub_html_doi(cr_doi):\n",
    "    html_file = 'html_files/' + cr_doi.replace('/','_').lower() + '.html'\n",
    "    if not Path(html_file).is_file():\n",
    "        page_content = urlh.getPageFromDOI(doi_text)\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(page_content.decode(\"utf-8\") )\n",
    "    else:\n",
    "        f = open(html_file, \"r\")\n",
    "        page_content = f.read()\n",
    "    return page_content, html_file\n",
    "             \n",
    "def get_titles(str_pub_title, db_name = \"prev_search.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'prev_pop_searches'\n",
    "    fields_required = \"Num, Title\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "def get_titles_and_dois(str_pub_title, db_name = \"app_db.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "# get the current csv working file\n",
    "def get_working_file(nr_wf):\n",
    "    working_file = wf_fields = None\n",
    "    current_pass = 0\n",
    "    if Path(nr_wf).is_file():\n",
    "        working_file, wf_fields = csvh.get_csv_data(nr_wf,'Num')\n",
    "        for art_num in tqdm_notebook(working_file):\n",
    "            if 'ignore' in working_file[art_num].keys():\n",
    "                if current_pass < int(working_file[art_num]['ignore']):\n",
    "                    current_pass = int(working_file[art_num]['ignore'])\n",
    "            else:\n",
    "                break\n",
    "    print(\"Current pass:\", current_pass)\n",
    "    return working_file, wf_fields, current_pass\n",
    "\n",
    "\n",
    "\n",
    "def get_pub_html_url(text_url, entry_id):\n",
    "    html_file = 'html_files/' +  entry_id + '.html'\n",
    "    if not Path(html_file).is_file():\n",
    "        print(\"\")\n",
    "        page_content = urlh.getPageFromURL(text_url)\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(page_content)\n",
    "    else:\n",
    "        f = open(html_file, \"r\")\n",
    "        page_content = f.read()\n",
    "    return page_content, html_file\n",
    "\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# get a semicolon separated list of authors from CR json data\n",
    "def get_cr_author_list(article_data):\n",
    "    authors = []\n",
    "    if 'author' in article_data.keys():\n",
    "        for author in article_data['author']:\n",
    "            new_author=\"\"\n",
    "            new_author = author['family']\n",
    "            if 'given' in author.keys():\n",
    "                new_author += \", \" + author['given']\n",
    "            authors.append(new_author)\n",
    "    return (\"; \").join(authors)\n",
    "\n",
    "# get the publication date from CR json data\n",
    "def get_cr_year_published(article_data):\n",
    "    year_print = 0\n",
    "    if 'published-print' in article_data.keys() \\\n",
    "        and article_data['published-print'] != None \\\n",
    "        and article_data['published-print']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['published-print']['date-parts'][0][0])    \n",
    "    elif 'journal-issue' in article_data.keys() \\\n",
    "        and article_data['journal-issue'] != None \\\n",
    "        and 'published-print' in article_data['journal-issue'].keys() \\\n",
    "        and article_data['journal-issue']['published-print'] != None \\\n",
    "        and article_data['journal-issue']['published-print']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['journal-issue']['published-print']['date-parts'][0][0])\n",
    "\n",
    "    year_online = 0\n",
    "    if 'published-online' in article_data.keys() \\\n",
    "        and article_data['published-online'] != None \\\n",
    "        and article_data['published-online']['date-parts'][0] != None:\n",
    "        year_online = int(article_data['published-online']['date-parts'][0][0])    \n",
    "    elif 'journal-issue' in article_data.keys() \\\n",
    "        and article_data['journal-issue'] != None \\\n",
    "        and 'published-online' in article_data['journal-issue'].keys() \\\n",
    "        and article_data['journal-issue']['published-online'] != None \\\n",
    "        and article_data['journal-issue']['published-online']['date-parts'][0] != None:\n",
    "        year_print = int(article_data['journal-issue']['published-online']['date-parts'][0][0])\n",
    "    \n",
    "    if year_print != 0 and year_online != 0:\n",
    "        return year_print if year_print < year_online else year_online\n",
    "    else:\n",
    "        return year_print if year_online == 0 else year_online\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the file with the results of the PoP search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the name of the input file:\n",
      "pop_searches/PoPCites20201017CR.csv\n"
     ]
    }
   ],
   "source": [
    "# input file with path: pop_searches/PoPCites20201017.csv\n",
    "new_results_file = \"\"\n",
    "while not Path(new_results_file).is_file():\n",
    "    print('Please enter the name of the input file:')\n",
    "    new_results_file = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the db file with previous results of the PoP search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the name of the previous results file:\n",
      "db_files/prev_search.sqlite3\n"
     ]
    }
   ],
   "source": [
    "# previous results db file with path: db_files/prev_search.sqlite3\n",
    "previous_db = \"\"\n",
    "while not Path(previous_db).is_file():\n",
    "    print('Please enter the name of the previous results file:')\n",
    "    previous_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the name of app db file:\n",
      "db_files/app_db.sqlite3\n"
     ]
    }
   ],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the name of the output file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying if the articles listed in: \n",
      "\t PoPCites20201017CR.csv\n",
      "where included in previous searches: \n",
      "\t prev_search.sqlite3\n",
      "The results will bt saved in: \n",
      "\t pop_searches/PoPCites20201017CR_wf.csv\n"
     ]
    }
   ],
   "source": [
    "nr_wf = new_results_file[:-4]+\"_wf.csv\"\n",
    "print(\"Verifying if the articles listed in: \\n\\t\", Path(new_results_file).name)\n",
    "print(\"where included in previous searches: \\n\\t\", Path(previous_db).name)\n",
    "\n",
    "print(\"The results will bt saved in: \\n\\t\", nr_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f60ceae22fe4c9e931f248bfa5cdaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    }
   ],
   "source": [
    "# get the working file before each step\n",
    "working_file = wf_fields = None\n",
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "# in first pass then make working file = new results\n",
    "if working_file == None:\n",
    "    working_file, wf_fields, current_pass = get_working_file(new_results_file)\n",
    "    csvh.write_csv_data(working_file, nr_wf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if already processed titles are included\n",
    "Read data and verify if results in file have already been included in previous searches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f51607b86444d98d1e680c98f66a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass == 0:\n",
    "    current_initial = \"\"\n",
    "    db_titles = []\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        new_title = working_file[art_num]['Title'].lower()\n",
    "        working_file[art_num]['ignore'] = 0 \n",
    "        working_file[art_num]['previous'] = 0 \n",
    "        working_file[art_num]['similarity'] = 0.0\n",
    "        if current_initial == \"\" or current_initial != new_title[0]:\n",
    "            print(\"new intital \", new_title[0])\n",
    "            current_initial = new_title[0]\n",
    "            db_titles = get_titles(current_initial, previous_db)\n",
    "            \n",
    "        for prev_pair in db_titles:\n",
    "            prev_num = prev_pair[0]\n",
    "            used_title = prev_pair[1].lower()\n",
    "            # if titles match exactly or simialarity > 0.8 ignore\n",
    "            title_similarity = txtc.similar(new_title, used_title)\n",
    "            if title_similarity > 0.80:\n",
    "                #print(art_num, 'Title:', new_title, \"already processed\", prev_num, used_title)\n",
    "                working_file[art_num]['ignore'] = 1\n",
    "                working_file[art_num]['previous'] = prev_num\n",
    "                working_file[art_num]['similarity'] = title_similarity\n",
    "                break\n",
    "\n",
    "    csvh.write_csv_data(working_file, nr_wf)  \n",
    "    print(nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Titles in app\n",
    "Verify if the title is in the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d17677391a4df2ae65f975d4db1c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "# verify that titles are not in the app_db (if they are  also get DOI)\n",
    "if current_pass == 1: \n",
    "    db_titles = []\n",
    "    current_initial = \"\"\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            new_title = working_file[art_num]['Title'].lower()\n",
    "            if current_initial == \"\" or current_initial != new_title[0]:\n",
    "                print(\"new intital \", new_title[0])\n",
    "                current_initial = new_title[0]\n",
    "                db_titles = get_titles_and_dois(current_initial, ukchapp_db)\n",
    "            for art_in_db in db_titles:\n",
    "                prev_num = art_in_db[0]\n",
    "                used_title = art_in_db[1].lower()\n",
    "                # if titles match exactly or simialarity > 0.8 ignore\n",
    "                title_similarity = txtc.similar(new_title, used_title)\n",
    "                if title_similarity > 0.80:\n",
    "                    #print(art_num, 'Title:', new_title, \"already processed\", prev_num, used_title)\n",
    "                    working_file[art_num]['ignore'] = 2\n",
    "                    working_file[art_num]['previous'] = prev_num\n",
    "                    working_file[art_num]['similarity'] = title_similarity\n",
    "                    working_file[art_num]['DOIcr'] = art_in_db[2]\n",
    "                    break                \n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Title Wording\n",
    "Using the workds in previous catalysis hub papers check if the title is likely to be a cat hub title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0b5feeac52491ca7e7b108ea55249d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass < 3:\n",
    "    # pass 2\n",
    "    # check titles for likelihood of being catalysis articles using keywords from titles in current DB \n",
    "    print(\"Get word list from DB\")\n",
    "    x = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    db_titles = x.get_value_list('articles','title')\n",
    "    title_words = set()\n",
    "    ignore_words=set(['the','of','to','and','a','in','is','it', 'their', 'so', 'as'])\n",
    "    average = 0\n",
    "    words_sum = 0.0\n",
    "    for title in db_titles:\n",
    "        one_title = set(title.lower().split())\n",
    "        one_title = one_title - ignore_words\n",
    "        title_words = title_words.union(one_title)\n",
    "        words_sum += len(one_title) \n",
    "        \n",
    "    average = words_sum /len(db_titles)\n",
    "    print(\"Average words per title:\", average)\n",
    "    title_words = title_words - ignore_words\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if 0 == int(working_file[art_num]['ignore']):\n",
    "            art_title = working_file[art_num]['Title']\n",
    "            art_words = set(art_title.lower().split())\n",
    "            occurrences = len(title_words.intersection(art_words))\n",
    "            working_file[art_num]['keywords']=occurrences\n",
    "            if occurrences == 0:\n",
    "                print(\"occurrences:\", occurrences, \"in title:\", art_title)\n",
    "                working_file[art_num]['ignore']=3\n",
    "            else:\n",
    "                print(\"occurrences:\", occurrences, \"in title:\", art_title)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    x.close()\n",
    "    current_pass = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4655762695b64f0bbf831749d4613a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass == 3:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            inspected = False\n",
    "            while not inspected:\n",
    "                new_title = working_file[art_num]['Title']\n",
    "                keywords = int(working_file[art_num]['keywords'])\n",
    "                #print (keywords, new_title)\n",
    "                if keywords <= 4 and not (\"cataly\" in new_title.lower()):\n",
    "                # ignore  it because it does not contains cataly in title\n",
    "                    working_file[art_num]['ignore']=4 # visual inspection\n",
    "                    inspected = True\n",
    "                else:\n",
    "                    inspected = True\n",
    "    print(\"To Process:\", i, \"Pass:\", current_pass)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DOIs for Articles\n",
    "The remaining titles need to be further analysed. Recovering their DOIs helps to obtain abstracts and acknowledgement statements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58c4fd009c045f884ed067a626b010b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9dbe3222c44fcb840a7db46e1e6427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "without DOI: 0\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "if current_pass == 4:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if (working_file[art_num]['ignore']=='0' and not 'DOIcr' in working_file[art_num].keys()) \\\n",
    "        or (working_file[art_num]['ignore']=='0' and working_file[art_num]['DOIcr']==\"\"):\n",
    "            new_title = working_file[art_num]['Title']\n",
    "            new_doi = cr_api.getDOIForTitle(new_title)\n",
    "            if new_doi == \"\":\n",
    "                #print(\"Missing DOI:\", new_title)\n",
    "                working_file[art_num]['ignore'] = '5'\n",
    "                i +=1\n",
    "            else:\n",
    "                #print(\"DOI found:\", new_doi, \"for:\", new_title)\n",
    "                working_file[art_num]['DOIcr'] = new_doi\n",
    "                working_file[art_num]['ignore'] = '0'\n",
    "    print(\"without DOI:\", i)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify DOIs in DB\n",
    "Verify that articles do not exist in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f090116950a7487b86f2aa53d2fbd29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf69fbaa90c49f98f17d4378cbe816f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in DB: 10.1016/j.jcat.2020.06.012 0 2 0)-Textured tungsten trioxide nanostructure with enhanced photoelectrochemical activity\n",
      "Not in DB: 10.1016/j.apcata.2020.117597 A detailed speciation of iron on FCC catalysts based on an integrated use of advanced characterisation methods and thermodynamic equilibrium simulation\n",
      "Not in DB: 10.5220/0010002800050011 Anylogic Simulation Research on Passenger Evacuation System of Urban Transportation Hub\n",
      "Not in DB: 10.1016/j.apcatb.2020.118843 Beyond surface redox and oxygen mobility at pd-polar ceria (100) interface: Underlying principle for strong metal-support interactions in green catalysis\n",
      "Not in DB: 10.1016/j.cattod.2020.05.024 Carbon dioxide decomposition through gas exchange in barium calcium iron niobates\n",
      "Not in DB: 10.1063/5.0012381 Computational prediction of muon stopping sites: A novel take on the unperturbed electrostatic potential method\n",
      "Not in DB: 10.1016/j.biombioe.2020.105757 Decarbonising Kenya's domestic & industry Sectors through bioenergy: An assessment of biomass resource potential & GHG performances\n",
      "Not in DB: 10.1016/j.cbpa.2020.06.004 Designer metalloenzymes for synthetic biology: Enzyme hybrids for catalysis\n",
      "Not in DB: 10.1016/j.jcat.2020.02.010 Enhancing activity and durability of Pd nanoparticle electrocatalyst by ceria undercoating on carbon support\n",
      "Not in DB: 10.1016/j.apcatb.2020.119416 Feasibility Study of Combining Direct Air Capture of CO2 and Methanation at Isothermal Conditions with Dual Function Materials\n",
      "Not in DB: 10.1016/j.cattod.2020.02.030 Ni-Fe-Al mixed oxide for combined dry reforming and decomposition of methane with CO2 utilization\n",
      "\n",
      "without DOI: 0\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 4:\n",
    "    i = 0\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            new_title = working_file[art_num]['Title']\n",
    "            new_doi = working_file[art_num]['DOIcr'].strip()\n",
    "            db_title = db_conn.get_title(new_doi)\n",
    "            if db_title == None:\n",
    "                print(\"Not in DB:\", new_doi, new_title)\n",
    "            else:\n",
    "                print(\"Already in DB:\", new_doi, \"for:\", new_title, db_title)\n",
    "                working_file[art_num]['ignore'] = '6'\n",
    "    print(\"without DOI:\", i)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 6\n",
    "    db_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full json files for remaining articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ce3aac7d914472b27e8f00f18621b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a8bca01e4347749ecfb48516dc157b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 4:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            data, file_name = get_cr_json_object(article_doi)\n",
    "            if data != {}:\n",
    "                working_file[art_num]['file'] = file_name\n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if CR json files contain funder details for UKCH grants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fa08a1e94048e59f5d447103018525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a169e2940b46bf8952b2fd7fea46bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 2 0)-Textured tungsten trioxide nanostructure with enhanced photoelectrochemical activity 10.1016/j.jcat.2020.06.012\n",
      "2 A detailed speciation of iron on FCC catalysts based on an integrated use of advanced characterisation methods and thermodynamic equilibrium simulation 10.1016/j.apcata.2020.117597\n",
      "3 Anylogic Simulation Research on Passenger Evacuation System of Urban Transportation Hub 10.5220/0010002800050011\n",
      "4 Beyond surface redox and oxygen mobility at pd-polar ceria (100) interface: Underlying principle for strong metal-support interactions in green catalysis 10.1016/j.apcatb.2020.118843\n",
      "5 Carbon dioxide decomposition through gas exchange in barium calcium iron niobates 10.1016/j.cattod.2020.05.024\n",
      "6 Computational prediction of muon stopping sites: A novel take on the unperturbed electrostatic potential method 10.1063/5.0012381\n",
      "7 Decarbonising Kenya's domestic & industry Sectors through bioenergy: An assessment of biomass resource potential & GHG performances 10.1016/j.biombioe.2020.105757\n",
      "8 Designer metalloenzymes for synthetic biology: Enzyme hybrids for catalysis 10.1016/j.cbpa.2020.06.004\n",
      "9 Enhancing activity and durability of Pd nanoparticle electrocatalyst by ceria undercoating on carbon support 10.1016/j.jcat.2020.02.010\n",
      "10 Feasibility Study of Combining Direct Air Capture of CO2 and Methanation at Isothermal Conditions with Dual Function Materials 10.1016/j.apcatb.2020.119416\n",
      "11 Ni-Fe-Al mixed oxide for combined dry reforming and decomposition of methane with CO2 utilization 10.1016/j.cattod.2020.02.030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 4:\n",
    "    i = 1\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            data, file_name = get_cr_json_object(article_doi)\n",
    "            print(i, article_title, article_doi)\n",
    "            #print(data.keys())\n",
    "            epsrc_keys = ['EP/R026645/1', 'EP/K014668/1', 'EP/K014714/1', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                          'EP/M013219/1', 'EP/R027129/1', 'EP/K014854/1', 'EP/K014706/2']\n",
    "            confirmed_in_cr = []\n",
    "            if 'funder' in data.keys():\n",
    "                for a_funder in data['funder']:\n",
    "                    for an_award in a_funder['award']:\n",
    "                        if an_award in epsrc_keys:\n",
    "                            print(\"Found\", an_award)\n",
    "                            confirmed_in_cr.append(an_award)\n",
    "                working_file[art_num]['award_in_cr'] = ', '.join(confirmed_in_cr)\n",
    "            i += 1\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full HTML files for remaining articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f616e235c5e047c2bb1557620a312062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    }
   ],
   "source": [
    "#nr_wf = \"pop_searches/PoPCites20201017_wf.csv\"\n",
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "if current_pass >= 6:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_id = working_file[art_num]['Num']\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr'].strip().lower()\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            article_type =working_file[art_num]['type']\n",
    "            html_content = file_name = None\n",
    "            if valid_doi(article_doi):\n",
    "                html_content, file_name = get_pub_html_doi(article_doi)\n",
    "            else:\n",
    "                #try with url\n",
    "                html_content = None\n",
    "                #identifier = \"id\" + str((1000000 + int(article_id)))[1,6] + article_type \n",
    "                #html_content, file_name = get_pub_html_doi(article_url, identifier)\n",
    "            if html_content != None:\n",
    "                working_file[art_num]['html_file'] = file_name\n",
    "                \n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get HTML page from DOI and verify if it contains UKCH acknowledgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d871e1754fc94a3daa7306caf8ef0c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fc2775c9d84ae49113ca03b6b5506a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://doi.org/10.1016/j.jcat.2020.06.012\n",
      "Title:  0 2 0)-Textured tungsten trioxide nanostructure with enhanced photoelectrochemical activity\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1016/j.apcata.2020.117597\n",
      "Title:  A detailed speciation of iron on FCC catalysts based on an integrated use of advanced characterisation methods and thermodynamic equilibrium simulation\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.5220/0010002800050011\n",
      "Title:  Anylogic Simulation Research on Passenger Evacuation System of Urban Transportation Hub\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1016/j.apcatb.2020.118843\n",
      "Title:  Beyond surface redox and oxygen mobility at pd-polar ceria (100) interface: Underlying principle for strong metal-support interactions in green catalysis\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1016/j.cattod.2020.05.024\n",
      "Title:  Carbon dioxide decomposition through gas exchange in barium calcium iron niobates\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1063/5.0012381\n",
      "Title:  Computational prediction of muon stopping sites: A novel take on the unperturbed electrostatic potential method\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1016/j.biombioe.2020.105757\n",
      "Title:  Decarbonising Kenya's domestic & industry Sectors through bioenergy: An assessment of biomass resource potential & GHG performances\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1016/j.cbpa.2020.06.004\n",
      "Title:  Designer metalloenzymes for synthetic biology: Enzyme hybrids for catalysis\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1016/j.jcat.2020.02.010\n",
      "Title:  Enhancing activity and durability of Pd nanoparticle electrocatalyst by ceria undercoating on carbon support\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1016/j.apcatb.2020.119416\n",
      "Title:  Feasibility Study of Combining Direct Air Capture of CO2 and Methanation at Isothermal Conditions with Dual Function Materials\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "https://doi.org/10.1016/j.cattod.2020.02.030\n",
      "Title:  Ni-Fe-Al mixed oxide for combined dry reforming and decomposition of methane with CO2 utilization\n",
      "***************************************************************\n",
      "Options:\n",
      "\ta) add ack text\n",
      "\tb) mark as not relevant\n",
      "\tc) go to next\n",
      "selection:\n",
      "b\n",
      "going to next\n",
      "\n",
      "pop_searches/PoPCites20201017CR_wf.csv\n"
     ]
    }
   ],
   "source": [
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "if current_pass >= 4:\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if (working_file[art_num]['ignore']=='0' and not 'ack_fragment' in working_file[art_num].keys()) or \\\n",
    "           (working_file[art_num]['ignore']=='0' and working_file[art_num]['ack_fragment'] == \"\"):\n",
    "            article_id = working_file[art_num]['Num']\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            request_str = \"https://doi.org/\" + article_doi \n",
    "            if valid_doi(article_doi):\n",
    "                request_str = \"https://doi.org/\" + article_doi \n",
    "                print(request_str)\n",
    "                #display(HTML('<h1>Hello, world!</h1>'))\n",
    "                #%%html\n",
    "                #<iframe src=request_str  width=\"600\" height=\"400\"></iframe>\n",
    "                IFrame(request_str, width=700, height=350)\n",
    "                inspected = False\n",
    "                while not inspected:\n",
    "                    #new_title = working_file[art_num]['Title']\n",
    "                    print('Title: ', article_title)\n",
    "                    print('***************************************************************')\n",
    "                    print(\"Options:\\n\\ta) add ack text\\n\\tb) mark as not relevant\\n\\tc) go to next\")\n",
    "                    print(\"selection:\")\n",
    "                    usr_select = input()\n",
    "                    if usr_select == 'b':\n",
    "                        #working_file[art_num]['ignore']=3 # visual inspection\n",
    "                        inspected = True\n",
    "                        working_file[art_num]['ignore'] = '9'\n",
    "                        working_file[art_num]['send_to_corinne'] = 'no'\n",
    "                        working_file[art_num]['reason_send'] = \"not acknowledged, no UKCH authors\"\n",
    "                        print(\"going to next\")\n",
    "                    elif usr_select == 'c':\n",
    "                        #working_file[art_num]['ignore']=3 # visual inspection\n",
    "                        inspected = True\n",
    "                        print(\"going to next\")\n",
    "                    elif usr_select == 'a':\n",
    "                        inspected = True\n",
    "                        ack_text = \"\"\n",
    "                        while ack_text == \"\":\n",
    "                            print(\"Enter ack text: \")\n",
    "                            ack_text = input()\n",
    "                            working_file[art_num]['ack_fragment'] = ack_text\n",
    "                            working_file[art_num]['send_to_corinne'] = 'yes'\n",
    "                            working_file[art_num]['reason_send'] = \"confirmed in acknowledgements\"\n",
    "            else:\n",
    "                print(article_doi, \"is not a valid DOI\")\n",
    "    csvh.write_csv_data(working_file, nr_wf)  \n",
    "    print(nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get bib data from CR to send for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nr_wf = \"pop_searches/PoPCites20201017_wf.csv\"\n",
    "working_file, wf_fields, current_pass = get_working_file(nr_wf)\n",
    "article_title = \"\"\n",
    "article_doi = \"\"\n",
    "article_url = \"\"\n",
    "data = None    \n",
    "try:\n",
    "    if current_pass >= 6:\n",
    "        for art_num in tqdm_notebook(working_file):\n",
    "            if working_file[art_num]['send_to_corinne'] == 'yes':\n",
    "                article_title = working_file[art_num]['Title']\n",
    "                article_doi = working_file[art_num]['DOIcr']\n",
    "                article_url =working_file[art_num]['ArticleURL']\n",
    "                if valid_doi(article_doi):\n",
    "                    data, file_name = get_cr_json_object(article_doi)\n",
    "                    # get authors\n",
    "                    working_file[art_num]['cr_authors'] = get_cr_author_list(data)\n",
    "                    # get article year\n",
    "                    working_file[art_num]['cr_year'] = get_cr_year_published(data)\n",
    "                    working_file[art_num]['cr_title'] = data['title']\n",
    "                    working_file[art_num]['cr_journal'] = data['container-title']\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "except:\n",
    "    print(article_title, article_doi, article_url)\n",
    "    print(data)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#***************************************************************************************************************\n",
    "# Wait do not run this yet\n",
    "#***************************************************************************************************************\n",
    "if current_pass >= 6:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            print(\"Analysing:\", article_title, article_doi, article_url)\n",
    "            # try to retrive html page for article using link from crossref first\n",
    "            # and if not try url from pop\n",
    "            # find reference to uk catalysis hub in html text\n",
    "            # if found mark as relevant\n",
    "            found = \"\"\n",
    "            referents = [\"uk catalysis hub\", \"uk catalysis\", \"catalysis hub\",\n",
    "                 'EP/R026645/1', 'resources', 'EP/K014668/1', 'EPSRC', 'EP/K014714/1',\n",
    "                 'Hub','provided', 'grant', 'biocatalysis', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                 'support', 'membership', 'EP/M013219/1', 'UK', 'kindly', 'Catalysis',\n",
    "                 'funded', 'EP/R027129/1', 'Consortium', 'thanked', 'EP/K014854/1', 'EP/K014706/2']\n",
    "            found = urlh.findFromDOI(article_title, article_doi, referents)\n",
    "            working_file[art_num]['checked_doi'] = 1\n",
    "            working_file[art_num]['ack_doi'] = found\n",
    "            found = urlh.findFromURI(article_title, article_url, referents)\n",
    "            working_file[art_num]['checked_url'] = 1\n",
    "            working_file[art_num]['ack_url'] = found\n",
    "            print(\"Ack:\", found)\n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_text = '10.1039/d0cy00036a'\n",
    "\n",
    "url_text = \"https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.808495\"\n",
    "\n",
    "html_content, file_name = get_pub_html_doi(doi_text)\n",
    "\n",
    "print(file_name)#, html_content)\n",
    "\n",
    "import re\n",
    "\n",
    "#print(len(doi_text))\n",
    "# CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "# CR DOIs re1\n",
    "# /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "\n",
    "cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "\n",
    "compare = re.match(cr_re_01, doi_text, re.IGNORECASE)\n",
    "\n",
    "print(compare)\n",
    "print(compare.start())\n",
    "print(compare.end())\n",
    "print(compare.group())\n",
    "\n",
    "if compare != None and doi_text == compare.group():\n",
    "    print(\"This is a DOI: \", doi_text)\n",
    "else:\n",
    "    print(\"This is not a DOI: \", doi_text)\n",
    "\n",
    "compare = re.match(cr_re_01, url_text, re.IGNORECASE)\n",
    "    \n",
    "print(url_text, valid_doi(url_text))\n",
    "print(doi_text, valid_doi(doi_text))\n",
    "\n",
    "# url_text = \"https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.808495\"\n",
    "# id = id000069_thesis\n",
    "entry_id = 'id000069_thesis'\n",
    "\n",
    "html_content, file_name = get_pub_html_url(url_text, entry_id)\n",
    "print(file_name, html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_text = '10.1039/d0cy00036a'\n",
    "print(doi_text, valid_doi(doi_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsrc_keys = ['EP/R026645/1', 'EP/K014668/1', 'EP/K014714/1', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                          'EP/M013219/1', 'EP/R027129/1', 'EP/K014854/1', 'EP/K014706/2']\n",
    "', '.join(epsrc_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
