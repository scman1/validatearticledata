{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of references to UK Catalysis Hub \n",
    "A list of articles is obtainded from publish or perish. This list will contain a titles and some IDs whic need to be verified. \n",
    "\n",
    "The criteria for adding a publication to the database are: \n",
    "a) has an explicit acknowledgement of UK Catalysis Hub\n",
    "b) mentions one of the UK Catalysis Hub grants\n",
    "c) has two or more authors with affiliation to UK Catalysis Hub\n",
    "d) acknowledges support from a scientist affiliated to UK Catalysis Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "#library for handling json files\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cr_json_object(cr_doi):\n",
    "  crjd = None\n",
    "  doi_file = 'json_files/' + cr_doi.replace('/','_').lower() + '.json'\n",
    "  if not Path(doi_file).is_file():\n",
    "    crjd = cr_api.getBibData(cr_doi)\n",
    "    with open(doi_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(crjd, f, ensure_ascii=False, indent=4)\n",
    "  else:\n",
    "    jf = open(doi_file, 'r')\n",
    "    crjd = json.load(jf)\n",
    "  return crjd, doi_file\n",
    "\n",
    "def get_titles(str_pub_title, db_name = \"prev_search.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'prev_pop_searches'\n",
    "    fields_required = \"Num, Title\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles\n",
    "\n",
    "def get_titles_and_dois(str_pub_title, db_name = \"app_db.sqlite3\"):\n",
    "    print(db_name)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, title, doi\"\n",
    "    filter_str = \"Title like '\"+str_pub_title[0]+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the file with the results of the PoP search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the name of the input file:\n",
      "pop_searches/PoPCites20201017.csv\n"
     ]
    }
   ],
   "source": [
    "# input file with path: pop_searches/PoPCites20201017.csv\n",
    "new_results_file = \"\"\n",
    "while not Path(new_results_file).is_file():\n",
    "    print('Please enter the name of the input file:')\n",
    "    new_results_file = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the db file with previous results of the PoP search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the name of the previous results file:\n",
      "db_files/prev_search.sqlite3\n"
     ]
    }
   ],
   "source": [
    "# previous results db file with path: db_files/prev_search.sqlite3\n",
    "\n",
    "previous_db = \"\"\n",
    "while not Path(previous_db).is_file():\n",
    "    print('Please enter the name of the previous results file:')\n",
    "    previous_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the name of the previous results file:\n",
      "db_files/app_db.sqlite3\n"
     ]
    }
   ],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of the previous results file:')\n",
    "    ukchapp_db = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the name of the output file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying if the articles listed in: \n",
      "\t PoPCites20201017.csv\n",
      "where included in previous searches: \n",
      "\t prev_search.sqlite3\n",
      "The results will bt saved in: \n",
      "\t pop_searches/PoPCites20201017_wf.csv\n"
     ]
    }
   ],
   "source": [
    "nr_wf = new_results_file[:-4]+\"_wf.csv\"\n",
    "print(\"Verifying if the articles listed in: \\n\\t\", Path(new_results_file).name)\n",
    "print(\"where included in previous searches: \\n\\t\", Path(previous_db).name)\n",
    "\n",
    "print(\"The results will bt saved in: \\n\\t\", nr_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe06cd3012d460d8e72e62fcdf09c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 7\n"
     ]
    }
   ],
   "source": [
    "working_file = wf_fields = None\n",
    "current_pass = 0\n",
    "if Path(nr_wf).is_file():\n",
    "    working_file, wf_fields = csvh.get_csv_data(nr_wf,'Num')\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if current_pass < int(working_file[art_num]['ignore']):\n",
    "            current_pass = int(working_file[art_num]['ignore'])\n",
    "print(\"Current pass:\", current_pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if already processed titles are included\n",
    "Read data and verify if results in file have already been included in previous searches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_pass == 0:\n",
    "    current_initial = \"\"\n",
    "    db_titles = []\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        new_title = working_file[art_num]['Title'].lower()\n",
    "        working_file[art_num]['ignore'] = 0 \n",
    "        working_file[art_num]['previous'] = 0 \n",
    "        working_file[art_num]['similarity'] = 0.0\n",
    "        if current_initial == \"\" or current_initial != new_title[0]:\n",
    "            print(\"new intital \", new_title[0])\n",
    "            current_initial = new_title[0]\n",
    "            db_titles = get_titles(current_initial, previous_db)\n",
    "            \n",
    "        for prev_pair in db_titles:\n",
    "            prev_num = prev_pair[0]\n",
    "            used_title = prev_pair[1].lower()\n",
    "            # if titles match exactly or simialarity > 0.8 ignore\n",
    "            title_similarity = txtc.similar(new_title, used_title)\n",
    "            if title_similarity > 0.80:\n",
    "                #print(art_num, 'Title:', new_title, \"already processed\", prev_num, used_title)\n",
    "                working_file[art_num]['ignore'] = 1\n",
    "                working_file[art_num]['previous'] = prev_num\n",
    "                working_file[art_num]['similarity'] = title_similarity\n",
    "                break\n",
    "\n",
    "    csvh.write_csv_data(working_file, nr_wf)  \n",
    "    print(nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Titles in app\n",
    "Verify if the title is in the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that titles are not in the app_db (if they are  also get DOI)\n",
    "if current_pass == 1: \n",
    "    db_titles = []\n",
    "    current_initial = \"\"\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            new_title = working_file[art_num]['Title'].lower()\n",
    "            if current_initial == \"\" or current_initial != new_title[0]:\n",
    "                print(\"new intital \", new_title[0])\n",
    "                current_initial = new_title[0]\n",
    "                db_titles = get_titles_and_dois(current_initial, ukchapp_db)\n",
    "            for art_in_db in db_titles:\n",
    "                prev_num = art_in_db[0]\n",
    "                used_title = art_in_db[1].lower()\n",
    "                # if titles match exactly or simialarity > 0.8 ignore\n",
    "                title_similarity = txtc.similar(new_title, used_title)\n",
    "                if title_similarity > 0.80:\n",
    "                    #print(art_num, 'Title:', new_title, \"already processed\", prev_num, used_title)\n",
    "                    working_file[art_num]['ignore'] = 2\n",
    "                    working_file[art_num]['previous'] = prev_num\n",
    "                    working_file[art_num]['similarity'] = title_similarity\n",
    "                    working_file[art_num]['DOIcr'] = art_in_db[2]\n",
    "                    break                \n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Title Wording\n",
    "Using the workds in previous catalysis hub papers check if the title is likely to be a cat hub title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_pass < 3:\n",
    "    # pass 2\n",
    "    # check titles for likelihood of being catalysis articles using keywords from titles in current DB \n",
    "    print(\"Get word list from DB\")\n",
    "    x = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    db_titles = x.get_value_list('articles','title')\n",
    "    title_words = set()\n",
    "    ignore_words=set(['the','of','to','and','a','in','is','it', 'their', 'so', 'as'])\n",
    "    average = 0\n",
    "    words_sum = 0.0\n",
    "    for title in db_titles:\n",
    "        one_title = set(title.lower().split())\n",
    "        one_title = one_title - ignore_words\n",
    "        title_words = title_words.union(one_title)\n",
    "        words_sum += len(one_title) \n",
    "        \n",
    "    average = words_sum /len(db_titles)\n",
    "    print(\"Average words per title:\", average)\n",
    "    title_words = title_words - ignore_words\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if 0 == int(working_file[art_num]['ignore']):\n",
    "            art_title = working_file[art_num]['Title']\n",
    "            art_words = set(art_title.lower().split())\n",
    "            occurrences = len(title_words.intersection(art_words))\n",
    "            working_file[art_num]['keywords']=occurrences\n",
    "            if occurrences == 0:\n",
    "                print(\"occurrences:\", occurrences, \"in title:\", art_title)\n",
    "                working_file[art_num]['ignore']=3\n",
    "            else:\n",
    "                print(\"occurrences:\", occurrences, \"in title:\", art_title)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    x.close()\n",
    "    current_pass = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_pass == 3:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            inspected = False\n",
    "            while not inspected:\n",
    "                new_title = working_file[art_num]['Title']\n",
    "                keywords = working_file[art_num]['keywords']\n",
    "                #print (keywords, new_title)\n",
    "                if keywords < 3 and not (\"cataly\" in new_title.lower()):\n",
    "                # ignore  it because it does not contains cataly in title\n",
    "                    working_file[art_num]['ignore']=4 # visual inspection\n",
    "                    inspected = True\n",
    "                else:\n",
    "                    inspected = True\n",
    "    print(\"To Process:\", i, \"Pass:\", current_pass)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DOIs for Articles\n",
    "The remaining titles need to be further analysed. Recovering their DOIs helps to obtain abstracts and acknowledgement statements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6b007174884830954debce6325b47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 7\n"
     ]
    }
   ],
   "source": [
    "working_file = wf_fields = None\n",
    "current_pass = 0\n",
    "if Path(nr_wf).is_file():\n",
    "    working_file, wf_fields = csvh.get_csv_data(nr_wf,'Num')\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if current_pass < int(working_file[art_num]['ignore']):\n",
    "            current_pass = int(working_file[art_num]['ignore'])\n",
    "print(\"Current pass:\", current_pass)\n",
    "if current_pass == 4:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0' and working_file[art_num]['DOIcr']==\"\":\n",
    "            new_title = working_file[art_num]['Title']\n",
    "            new_doi = cr_api.getDOIForTitle(new_title)\n",
    "            if new_doi == \"\":\n",
    "                #print(\"Missing DOI:\", new_title)\n",
    "                working_file[art_num]['ignore'] = '5'\n",
    "                i +=1\n",
    "            else:\n",
    "                #print(\"DOI found:\", new_doi, \"for:\", new_title)\n",
    "                working_file[art_num]['DOIcr'] = new_doi\n",
    "                working_file[art_num]['ignore'] = '0'\n",
    "    print(\"without DOI:\", i)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify DOIs in DB\n",
    "Verify that articles do not exist in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c671b6adfa14d75b399b54df2594e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 7\n"
     ]
    }
   ],
   "source": [
    "working_file = wf_fields = None\n",
    "current_pass = 0\n",
    "if Path(nr_wf).is_file():\n",
    "    working_file, wf_fields = csvh.get_csv_data(nr_wf,'Num')\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if current_pass < int(working_file[art_num]['ignore']):\n",
    "            current_pass = int(working_file[art_num]['ignore'])\n",
    "print(\"Current pass:\", current_pass)\n",
    "if current_pass == 5:\n",
    "    i = 0\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            new_title = working_file[art_num]['Title']\n",
    "            new_doi = working_file[art_num]['DOIcr'].strip()\n",
    "            db_title = db_conn.get_title(new_doi)\n",
    "            if db_title == None:\n",
    "                print(\"Not in DB:\", new_doi, new_title)\n",
    "            else:\n",
    "                print(\"Already in DB:\", new_doi, \"for:\", new_title, db_title)\n",
    "                working_file[art_num]['ignore'] = '6'\n",
    "    print(\"without DOI:\", i)\n",
    "    csvh.write_csv_data(working_file, nr_wf)\n",
    "    current_pass = 6\n",
    "    dbh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full json files for remaining articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7569eb19fc9458f8fa9f81fb08e51ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current pass: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a167e9ba9e5644ee8fae622f15e6e2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "working_file = wf_fields = None\n",
    "current_pass = 0\n",
    "if Path(nr_wf).is_file():\n",
    "    working_file, wf_fields = csvh.get_csv_data(nr_wf,'Num')\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if current_pass < int(working_file[art_num]['ignore']):\n",
    "            current_pass = int(working_file[art_num]['ignore'])\n",
    "            \n",
    "print(\"Current pass:\", current_pass)\n",
    "\n",
    "if current_pass >= 6:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            #print(\"Getting Schema:\", article_title, article_doi, article_url)\n",
    "            # try to retrive html page for article using link from crossref first\n",
    "            # and if not try url from pop\n",
    "            # find reference to uk catalysis hub in html text\n",
    "            # if found mark as relevant\n",
    "            data, file_name = get_cr_json_object(article_doi)\n",
    "            if data != {}:\n",
    "                working_file[art_num]['file'] = file_name\n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#***************************************************************************************************************\n",
    "# Wait do not run this yet\n",
    "#***************************************************************************************************************\n",
    "if current_pass >= 6:\n",
    "    i = 0\n",
    "    for art_num in tqdm_notebook(working_file):\n",
    "        if working_file[art_num]['ignore']=='0':\n",
    "            article_title = working_file[art_num]['Title']\n",
    "            article_doi = working_file[art_num]['DOIcr']\n",
    "            article_url =working_file[art_num]['ArticleURL']\n",
    "            print(\"Analysing:\", article_title, article_doi, article_url)\n",
    "            # try to retrive html page for article using link from crossref first\n",
    "            # and if not try url from pop\n",
    "            # find reference to uk catalysis hub in html text\n",
    "            # if found mark as relevant\n",
    "            found = \"\"\n",
    "            referents = [\"uk catalysis hub\", \"uk catalysis\", \"catalysis hub\",\n",
    "                 'EP/R026645/1', 'resources', 'EP/K014668/1', 'EPSRC', 'EP/K014714/1',\n",
    "                 'Hub','provided', 'grant', 'biocatalysis', 'EP/R026815/1', 'EP/R026939/1',\n",
    "                 'support', 'membership', 'EP/M013219/1', 'UK', 'kindly', 'Catalysis',\n",
    "                 'funded', 'EP/R027129/1', 'Consortium', 'thanked', 'EP/K014854/1', 'EP/K014706/2']\n",
    "            found = urlh.findFromDOI(article_title, article_doi, referents)\n",
    "            working_file[art_num]['checked_doi'] = 1\n",
    "            working_file[art_num]['ack_doi'] = found\n",
    "            found = urlh.findFromURI(article_title, article_url, referents)\n",
    "            working_file[art_num]['checked_url'] = 1\n",
    "            working_file[art_num]['ack_url'] = found\n",
    "            print(\"Ack:\", found)\n",
    "    csvh.write_csv_data(working_file, nr_wf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
