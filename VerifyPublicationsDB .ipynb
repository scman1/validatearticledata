{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check fo duplicate articles and data objects\n",
    "A list of publications is obtainded from the app database. This list will contain a titles, IDs and DOIs which need to be explored to look for duplicates. \n",
    "The steps of the process are: \n",
    " 1. get a Title, DOI, and URL for each publication\n",
    " 2. revise each element of the list for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "from tqdm import tqdm\n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n",
    "# library for handling http requests\n",
    "import requests\n",
    "# import custom functions (common to various notebooks)\n",
    "import processing_functions as pr_fns\n",
    "\n",
    "# datetime parsing\n",
    "from datetime import datetime\n",
    "\n",
    "current_step = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if there are duplicates in articles\n",
    "\n",
    "1. Open the current publication list from the appdb\n",
    "2. read each entry and check if there are duplicates in doi, url or title\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem articles table does not have pdf_file column\n"
     ]
    }
   ],
   "source": [
    "# deal with no pdf_file column\n",
    "def get_pubs_list(db_path):\n",
    "    pubs_list = None\n",
    "    try:\n",
    "        pubs_list = pr_fns.get_pub_data(db_path)\n",
    "    except Exception as inst:\n",
    "        if 'pdf_file' in inst.args[0]:\n",
    "            print('problem articles table does not have pdf_file column')\n",
    "            pass\n",
    "    try:\n",
    "        if pubs_list == None:\n",
    "            pubs_list = pr_fns.get_pub_app_data(db_path)\n",
    "    except Exception as inst:\n",
    "        print(type(inst))\n",
    "        print(inst.args)\n",
    "        print(inst)\n",
    "        print('another problem')    \n",
    "    return pubs_list\n",
    "\n",
    "# 1 current app DB\n",
    "db_name = 'development'\n",
    "#ukchapp_db = \"db_files/\"+db_name+\".sqlite3\"\n",
    "ukchapp_db = \"../mcc_data/\"+db_name+\".sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "    \n",
    "#  get publication data from the ukch app\n",
    "app_pubs = get_pubs_list(ukchapp_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate articles in DB\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# 2 read each entry and check if there are duplicates in doi, url or title\n",
    "dup_list={}\n",
    "dup_count = 0\n",
    "if current_step == 1 and app_pubs != None:  \n",
    "    dups = []\n",
    "    for idx, a_pub in enumerate(tqdm_notebook(app_pubs)):\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        # verfy if dois are duplicated\n",
    "        if pub_doi != None:\n",
    "            for i_indx in range(idx+1, len(app_pubs)):\n",
    "                #print(pub_doi, app_pubs[i_indx][2])\n",
    "                if app_pubs[i_indx][2]!=None and pub_doi.strip().lower() ==  app_pubs[i_indx][2].strip().lower():\n",
    "                    print(\"DOI\", pub_doi, \"duplicated at:\", i_indx, app_pubs[i_indx], app_pubs[idx] )\n",
    "                    a_dup = {'pub_comp': app_pubs[idx], 'pub_dup': app_pubs[i_indx],\"dup_at\":'DOI'}\n",
    "                    dup_count+=1\n",
    "                    dup_list[dup_count] = a_dup\n",
    "        # verify if urls are all unique\n",
    "        if pub_url != None:\n",
    "            for i_indx in range(idx+1, len(app_pubs)):\n",
    "                if app_pubs[i_indx][3]!=None and pub_url.strip().lower() ==  app_pubs[i_indx][3].strip().lower():\n",
    "                    print(\"URL\", pub_url, \"duplicated at:\", i_indx, app_pubs[i_indx], app_pubs[idx] )    \n",
    "                    a_dup = {'pub_comp': app_pubs[idx], 'pub_dup': app_pubs[i_indx],\"dup_at\":'URL'}\n",
    "                    dup_count+=1\n",
    "                    dup_list[dup_count] = a_dup\n",
    "        # verify if titles are all unique\n",
    "        if pub_title != None:\n",
    "            for i_indx in range(idx+1, len(app_pubs)):\n",
    "                similarity = txtc.similar(pub_title.strip().lower(), app_pubs[i_indx][1].strip().lower())\n",
    "                #print(similarity)\n",
    "                if app_pubs[i_indx][1]!=None and pub_title.strip().lower() ==  app_pubs[i_indx][1].strip().lower():\n",
    "                    print(\"Title\", pub_title, \"duplicated at:\", i_indx,app_pubs[i_indx][1], app_pubs[idx][1])     \n",
    "                    print(\"Similarity:\", similarity)\n",
    "                    a_dup = {'pub_comp': app_pubs[idx], 'pub_dup': app_pubs[i_indx],\"dup_at\":'Title'}\n",
    "                    dup_count+=1\n",
    "                    dup_list[dup_count] = a_dup\n",
    "                #elif similarity > 0.8:\n",
    "                #    print(similarity, \"Title:\\n\\t\", pub_title, \"\\n\\t- similar at:\\n\\t\", i_indx,app_pubs[i_indx][1]) \n",
    "if len(dup_list) > 0:\n",
    "        csvh.write_csv_data(dup_list, 'dup_'+db_name+'.csv')\n",
    "else:\n",
    "    print (\"No duplicate articles in DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if there are duplicates in data objects\n",
    "\n",
    "1. get the current data objects list from the appdb\n",
    "2. read each entry and check if there are duplicates in doi, url or title\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate DOs in DB\n"
     ]
    }
   ],
   "source": [
    "# 1 current app DB\n",
    "dup_list={}\n",
    "db_name = 'development'\n",
    "#ukchapp_db = \"db_files/\"+db_name+\".sqlite3\"\n",
    "ukchapp_db = \"../mcc_data/\"+db_name+\".sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "    \n",
    "#  get datasets list from the ukch app\n",
    "app_datasetes = pr_fns.get_dataset_data(ukchapp_db)\n",
    "\n",
    "# 2 read each entry and check if there are duplicates in doi, url or title\n",
    "if current_step == 2:  \n",
    "    dup_list={}\n",
    "    dup_count = 0\n",
    "    for idx, a_ds in enumerate(tqdm_notebook(app_datasetes)):\n",
    "        ds_id = a_ds[0]\n",
    "        ds_doi = a_ds[1]\n",
    "        ds_url = a_ds[2]\n",
    "        ds_name = a_ds[3]\n",
    "        #print (ds_id, ds_doi, ds_url, ds_name)\n",
    "        # verfy if dois are duplicated\n",
    "        if ds_doi != None and ds_doi != '':\n",
    "            for i_indx in range(idx+1, len(app_datasetes)):\n",
    "                #print(pub_doi, app_pubs[i_indx][1])\n",
    "                if app_datasetes[i_indx][1]!=None and ds_doi.strip().lower() ==  app_datasetes[i_indx][1].strip().lower():\n",
    "                    print(\"DOI\", ds_doi, \"duplicated at:\", i_indx, app_datasetes[i_indx], app_datasetes[idx] )\n",
    "                    a_dup = {'pub_comp': app_datasetes[idx], 'pub_dup': app_datasetes[i_indx],\"dup_at\":'DOI'}\n",
    "                    dup_count+=1\n",
    "                    dup_list[dup_count] = a_dup\n",
    "        # verify if urls are all unique\n",
    "        if ds_url != None:\n",
    "            for i_indx in range(idx+1, len(app_datasetes)):\n",
    "                #print(app_datasetes[i_indx])\n",
    "                if app_datasetes[i_indx][2]!=None and ds_url.strip().lower() ==  app_datasetes[i_indx][2].strip().lower():\n",
    "                    print(\"URL\", ds_url, \"duplicated at:\", i_indx, app_datasetes[i_indx], app_datasetes[idx] )  \n",
    "                    a_dup = {'pub_comp': app_datasetes[idx], 'pub_dup': app_datasetes[i_indx],\"dup_at\":'URL'}\n",
    "                    dup_count+=1\n",
    "                    dup_list[dup_count] = a_dup\n",
    "        # verify if titles are all unique\n",
    "        if ds_name != None:\n",
    "            for i_indx in range(idx+1, len(app_datasetes)):\n",
    "                similarity = txtc.similar(ds_name.strip().lower(), app_datasetes[i_indx][3].strip().lower())\n",
    "                #print(similarity)\n",
    "                if app_datasetes[i_indx][1]!=None and ds_name.strip().lower() ==  app_datasetes[i_indx][3].strip().lower():\n",
    "                    print(\"Title\", ds_name, \"duplicated at:\", i_indx,app_datasetes[i_indx][3], app_datasetes[idx][3])     \n",
    "                    print(\"Similarity:\", similarity)\n",
    "                    a_dup = {'pub_comp': app_datasetes[idx], 'pub_dup': app_datasetes[i_indx],\"dup_at\":'Title'}\n",
    "                    dup_count+=1\n",
    "                    dup_list[dup_count] = a_dup\n",
    "                #elif similarity > 0.8:\n",
    "                #    print(similarity, \"Title:\\n\\t\", ds_name, \"\\n\\t- similar at:\\n\\t\", i_indx,app_datasetes[i_indx][3]) \n",
    "                #    break\n",
    "if len(dup_list) > 0:\n",
    "    csvh.write_csv_data(dup_list, 'dup_do_'+db_name+'.csv')\n",
    "else:\n",
    "    print (\"No duplicate DOs in DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify authors\n",
    "\n",
    "1. verify that there are no authors with no articles in the DB\n",
    "2. verify that authors are unique remove close matches (need to check spellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_like_str(a_string):\n",
    "    like_str = \"%\" + re.sub(r'[^a-zA-Z\\s:]', '%', a_string) + \"%\"\n",
    "    return like_str\n",
    "\n",
    "def get_all_authors():\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'authors'\n",
    "    s_fields = 'id, given_name, last_name, orcid'\n",
    "    s_where = 'isap IS NULL' # 1 displayed authors - 0/NULL the rest\n",
    "    authors_list = db_conn.get_values(s_table, s_fields, s_where)\n",
    "    return authors_list\n",
    "\n",
    "def get_null_authors():\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'authors'\n",
    "    s_fields = 'id, given_name, last_name, orcid'\n",
    "    s_where = 'id NOT IN (SELECT article_authors.author_id FROM article_authors)'\n",
    "    authors_list = db_conn.get_values(s_table, s_fields, s_where)\n",
    "    return authors_list\n",
    "\n",
    "def get_similar_authors(name,surname,orcid):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'authors'\n",
    "    s_fields = 'id, given_name, last_name, orcid'\n",
    "    like_surname = make_like_str(surname)\n",
    "    surname.replace(\"'\",\"''\")\n",
    "    s_where = \"(orcid = '%s' AND last_name = '%s')\"%(orcid, surname)\n",
    "    s_where += \"OR(given_name = '%s' AND last_name = '%s')\"%(name, surname)\n",
    "    s_where += \"OR(last_name = '%s')\"%(surname)\n",
    "    s_where += \"OR(last_name LIKE '%s')\"%(like_surname)\n",
    "    authors_list = db_conn.get_values(s_table, s_fields, s_where)\n",
    "    return authors_list\n",
    "\n",
    "def count_linked(author_id):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'article_authors'\n",
    "    s_fields = 'id, author_id'\n",
    "    s_where = \"(author_id = %s)\"%(author_id)\n",
    "    aa_list = db_conn.get_values(s_table, s_fields, s_where)\n",
    "    return len(aa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def save_ok_list(values_list, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for an_id in values_list:\n",
    "            f.write(str(an_id)+'\\n')\n",
    "\n",
    "def open_ok_list(file_name):\n",
    "    with open(file_name) as f:\n",
    "        lines = f.readlines()\n",
    "    from_file = []\n",
    "    for a_line in lines:\n",
    "        from_file.append(int(a_line.replace('\\n','')))\n",
    "    return from_file\n",
    "\n",
    "def add_to_ok_list(a_value, file_name):\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(str(a_value)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_single_word(a_word, another_word):\n",
    "    single_word = False\n",
    "    in_word = another_word.lower().find(a_word.lower())\n",
    "    if in_word >= 0:\n",
    "        single_word = True\n",
    "        if in_word > 0 and  another_word[in_word-1].isalpha():\n",
    "            sinlge_word = False\n",
    "        if in_word + len(a_word) < len(another_word)-1 and another_word[in_word + len(a_word)].isalpha():\n",
    "            single_word = False\n",
    "    return single_word\n",
    "\n",
    "def prune_similar_surnames(the_similars, a_surname):\n",
    "    pruned_list = []\n",
    "    for a_simi in the_similars:\n",
    "        if a_simi[2] == a_surname or is_single_word(a_surname,a_simi[2]) :\n",
    "            pruned_list.append(a_simi)\n",
    "    return pruned_list\n",
    "\n",
    "def get_initials(given_names):\n",
    "    initials_1 =  [a_letter for a_letter in given_names if a_letter.isupper() ] \n",
    "    names = given_names.split()\n",
    "    initials_2 = [a_name[0] for a_name in given_names]\n",
    "    ri_1 = \". \".join(initials_1)+\".\"\n",
    "    ri_2 = \" \".join(initials_1)\n",
    "    return ri_1, ri_2\n",
    "    \n",
    "def prune_similar_names(the_similars, a_name):\n",
    "    pruned_list = []\n",
    "    dot_initials, initials = get_initials(a_name)\n",
    "    for a_simi in the_similars:\n",
    "        if a_simi[1] == a_name or is_single_word(a_name,a_simi[1]) :\n",
    "            pruned_list.append(a_simi)\n",
    "        elif initials == a_simi[1] or dot_initials == a_simi[1]:\n",
    "            pruned_list.append(a_simi)    \n",
    "    return pruned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_author_value(a_id, a_column, a_value):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    db_conn.set_value_table('authors', a_id, a_column, a_value)\n",
    "\n",
    "\n",
    "def update_author(old_author, a_id, a_name, a_surname, a_orcid):\n",
    "    if a_id != old_author[0]:\n",
    "        return # do not update different authors        \n",
    "    if a_name != old_author[1]:\n",
    "        set_author_value(a_id, 'given_name', a_name)\n",
    "    if a_surname != old_author[2]:\n",
    "        set_author_value(a_id, 'last_name', a_surname)\n",
    "    if a_orcid != old_author[3]:\n",
    "        set_author_value(a_id, 'given_name', a_orcid)\n",
    "\n",
    "def update_article_authors(new_id, old_id):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_where = \"author_id = %s\" % (old_id)\n",
    "    aa_ids = db_conn.get_values('article_authors', 'id', s_where)\n",
    "    print(aa_ids)\n",
    "    for an_id in aa_ids:\n",
    "        db_conn.set_value_table('article_authors', an_id[0], 'author_id', new_id)\n",
    "\n",
    "def delete_author(a_id):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    db_conn.connection.execute(\"DELETE FROM authors WHERE id = %s\" % (a_id ))\n",
    "    db_conn.connection.commit()\n",
    "    \n",
    "def merge_authors(an_author, auth_similars):\n",
    "    auth_id = an_author[0]\n",
    "    auth_name = an_author[1]\n",
    "    auth_surname = an_author[2]\n",
    "    auth_orcid = an_author[3]\n",
    "\n",
    "    for a_result in auth_similars:\n",
    "        if auth_id > a_result[0]:\n",
    "            auth_id = a_result[0]\n",
    "        if auth_name != a_result[1] and len(auth_name) < len(a_result[1]):\n",
    "            print(\"Which name \\n\\t 1)\", auth_name, \"\\n\\t 2)\", a_result[1])\n",
    "            opt_name = input()\n",
    "            if opt_name == '2': auth_name = a_result[1] \n",
    "        if auth_surname != a_result[2] and len(auth_surname) < len(a_result[2]):\n",
    "            print(\"Which surname \\n\\t 1)\", auth_surname, \"\\n\\t 2)\", a_result[2])\n",
    "            opt_name = input()\n",
    "            if opt_name == '2': auth_surname = a_result[2] \n",
    "        if auth_orcid != a_result[3]:\n",
    "            print(\"Which ORCID \\n\\t 1)\", auth_orcid, \"\\n\\t 2)\", a_result[3])\n",
    "            opt_name = input()\n",
    "            if opt_name == '2': auth_orcid = a_result[3]\n",
    "\n",
    "    if auth_id != an_author[0] or auth_name != an_author[1] or auth_surname != an_author[2] or auth_orcid != an_author[3]:\n",
    "        print (\"Will update\", an_author, \"to\", auth_id , auth_name , auth_surname,auth_orcid )\n",
    "\n",
    "    for a_result in auth_similars:\n",
    "        if auth_id != a_result[0]:\n",
    "            print(\"will update all author articles from:\", a_result[0], \"to:\", auth_id)\n",
    "            print(\"will delete author:\", a_result[0])\n",
    "    print(\"Continue?\\n\\t 1) proceed \\n\\t 2) cancel\")\n",
    "    opt_go = input()\n",
    "    if opt_go == '1':\n",
    "        update_author(an_author, auth_id, auth_name , auth_surname,auth_orcid)\n",
    "        for a_result in auth_similars:\n",
    "            if auth_id != a_result[0]:\n",
    "                update_article_authors(auth_id,a_result[0])\n",
    "                delete_author(a_result[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check authors with no articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# verify that there are no authors with no articles in the DB\n",
    "no_artaut_authors = get_null_authors()\n",
    "\n",
    "delete_these=[]\n",
    "for an_author in no_artaut_authors:\n",
    "    dup_id = an_author[0]\n",
    "    dup_name = an_author[1]\n",
    "    dup_surname = an_author[2]\n",
    "    dup_orcid = an_author[3] if an_author[3]!=None else \"NULL\"\n",
    "    print(dup_id, dup_name, dup_surname, dup_orcid)\n",
    "    similars = get_similar_authors(dup_name, dup_surname, dup_orcid)\n",
    "    print('There are %s similar authors in DB'%(len(similars)))\n",
    "    for idx, a_simil in enumerate(similars):\n",
    "        art_count = count_linked(a_simil[0])\n",
    "        if art_count == 0:\n",
    "            print(idx, a_simil,\"Links:\", art_count, \"DELETE\")\n",
    "            delete_these.append(a_simil[0])\n",
    "        else:\n",
    "            print(idx, a_simil,\"Links:\", art_count, \"CHECK\")\n",
    "\n",
    "print(delete_these)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review duplicate authors (by name and last name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: 34\n",
      "[1, 147, 158, 166, 179, 202, 395, 550, 557, 572, 664, 921, 956, 1036, 1086, 1172, 1269, 1939, 1969, 1974, 2010, 2400, 2721, 2738, 2855, 2870, 2965, 3012, 3104, 3181, 3231, 3248, 3344, 3375]\n"
     ]
    }
   ],
   "source": [
    "# Manually review probable duplicate authors \n",
    "all_authors = get_all_authors()\n",
    "revise_these=[]\n",
    "\n",
    "safe_list = open_ok_list('safe_list.txt')\n",
    "pacer_idx = 0\n",
    "for an_author in all_authors:   \n",
    "    dup_id = an_author[0]\n",
    "    dup_name = an_author[1]\n",
    "    dup_surname = an_author[2]\n",
    "    dup_orcid = an_author[3] if an_author[3]!=None else \"NULL\"\n",
    "    if not(int(dup_id) in safe_list):\n",
    "        if \"'\" in dup_surname: dup_surname = dup_surname.replace(\"'\",\"''\")\n",
    "        if (\"’\") in dup_surname: dup_surname = dup_surname.replace(\"’\",\"''\")\n",
    "        all_similars = get_similar_authors(dup_name, dup_surname, dup_orcid)\n",
    "        similars = prune_similar_surnames(all_similars, dup_surname)\n",
    "        similars = prune_similar_names(similars, dup_name)\n",
    "        if len(similars) > 1 and  len(dup_surname)>3 :\n",
    "            print(\"*************************************************\")\n",
    "            print(\"Author:\", dup_id, dup_name, dup_surname, dup_orcid)\n",
    "            print('There are %s similar authors in DB'%(len(similars)))\n",
    "\n",
    "            for idx, a_simil in enumerate(similars):\n",
    "                art_count = count_linked(a_simil[0])\n",
    "                print(idx, a_simil,\"Links:\", art_count)\n",
    "            print (\"Options:\\n\\t (1) Ignore \\n\\t (2) Merge\\n\\t (3) next\")\n",
    "            sel_action = input()\n",
    "            if sel_action == '1':\n",
    "                safe_list = sorted(list(set(safe_list + [sublist[0] for sublist in similars])))\n",
    "            if sel_action == '2':\n",
    "                merge_authors(an_author, similars)\n",
    "            pacer_idx+=1\n",
    "            clear_output()\n",
    "            if pacer_idx == 10:\n",
    "                break\n",
    "print(\"OK:\", len(safe_list))\n",
    "print(safe_list)\n",
    "save_ok_list(safe_list, 'safe_list.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify affiliations and author affiliations agains crossref affiliations\n",
    "1 get group of crossref affiliations\n",
    "2 get assigned affiliation\n",
    "3 verify if OK if not show and ask for action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing lists\n",
      "Article Author:  1\n",
      "[(1, 'Chemistry─School of Natural and Environmental Sciences, Newcastle University, Newcastle upon Tyne, NE1 7RU, U.K.', 1, 1, '2022-08-23 08:21:07.822289', '2022-08-28 18:40:22.195960'), (2, 'Centre for Energy, Newcastle University, Newcastle upon Tyne, NE1 7RU, U.K.', 1, 2, '2022-08-23 08:21:07.832527', '2022-08-28 18:40:22.218108')]\n",
      "Chemistry─School of Natural and Environmental Sciences, Newcastle University, Newcastle upon Tyne, NE1 7RU, U.K. True\n",
      "Centre for Energy, Newcastle University, Newcastle upon Tyne, NE1 7RU, U.K. True\n",
      "verify one liners\n",
      "Assigned ID: 1 Recoverd ID: 1\n",
      "Assigned ID: 2 Recoverd ID: 2\n",
      "Article Author:  2\n",
      "[(3, 'Department of Chemistry, University of Bath, Bath, BA2 7AY, U.K.', 2, 7, '2022-08-23 08:21:07.882489', '2022-08-28 18:44:26.111714'), (4, 'Department of Materials, University of Oxford, Oxford, OX1 3PH, U.K.', 2, 8, '2022-08-23 08:21:07.894823', '2022-08-28 18:44:26.131624')]\n",
      "Department of Chemistry, University of Bath, Bath, BA2 7AY, U.K. True\n",
      "Department of Materials, University of Oxford, Oxford, OX1 3PH, U.K. True\n",
      "verify one liners\n",
      "Assigned ID: 47 Recoverd ID: 47\n",
      "Assigned ID: 3 Recoverd ID: 3\n",
      "Article Author:  17\n",
      "[(5, \"Department of Physics King's College London  London WC2R 2LS UK\", 17, 2203, '2022-08-24 11:30:48.943400', '2022-08-24 11:30:48.943400')]\n",
      "Department of Physics King's College London  London WC2R 2LS UK True\n",
      "verify one liners\n",
      "Assigned ID: 368 Recoverd ID: 368\n",
      "Article Author:  18\n",
      "[(6, \"Department of Physics King's College London  London WC2R 2LS UK\", 18, 2204, '2022-08-24 11:30:48.992754', '2022-08-24 11:30:48.992754')]\n",
      "Department of Physics King's College London  London WC2R 2LS UK True\n",
      "verify one liners\n",
      "Assigned ID: 368 Recoverd ID: 368\n",
      "Article Author:  19\n",
      "[(7, \"Institute of Pharmaceutical Science King's College London  London SE1 9NH UK\", 19, 105, '2022-08-24 11:30:49.053015', '2022-08-28 19:56:46.461617')]\n",
      "Institute of Pharmaceutical Science King's College London  London SE1 9NH UK True\n",
      "verify one liners\n",
      "Assigned ID: 468 Recoverd ID: 468\n",
      "Article Author:  20\n",
      "[(8, \"Department of Physics King's College London  London WC2R 2LS UK\", 20, 2205, '2022-08-24 11:30:49.086865', '2022-08-24 11:30:49.086865')]\n",
      "Department of Physics King's College London  London WC2R 2LS UK True\n",
      "verify one liners\n",
      "Assigned ID: 368 Recoverd ID: 368\n",
      "Article Author:  21\n",
      "[(9, 'Department of Chemical Sciences, University of Huddersfield, Queensgate, Huddersfield, HD1 3DH, UK', 21, 129, '2022-08-24 11:33:32.953661', '2022-08-28 20:27:50.786784')]\n",
      "Department of Chemical Sciences, University of Huddersfield, Queensgate, Huddersfield, HD1 3DH, UK True\n",
      "verify one liners\n",
      "Assigned ID: 366 Recoverd ID: 366\n",
      "Article Author:  22\n",
      "[(10, 'Department of Chemistry, University of Manchester, Manchester M13 9PL, UK', 22, 131, '2022-08-24 11:33:32.979484', '2022-08-28 20:27:50.895247')]\n",
      "Department of Chemistry, University of Manchester, Manchester M13 9PL, UK True\n",
      "verify one liners\n",
      "Assigned ID: 112 Recoverd ID: 112\n",
      "Article Author:  23\n",
      "[(11, 'Department of Chemical Sciences, University of Huddersfield, Queensgate, Huddersfield, HD1 3DH, UK', 23, 164, '2022-08-24 11:33:32.995237', '2022-08-28 20:27:52.667266')]\n",
      "Department of Chemical Sciences, University of Huddersfield, Queensgate, Huddersfield, HD1 3DH, UK True\n",
      "verify one liners\n",
      "Assigned ID: 366 Recoverd ID: 366\n",
      "Article Author:  24\n",
      "[(12, 'Department of Chemical Sciences, University of Huddersfield, Queensgate, Huddersfield, HD1 3DH, UK', 24, 167, '2022-08-24 11:33:33.011333', '2022-08-28 20:27:52.818401'), (13, 'Department of Chemistry, University of Manchester, Manchester M13 9PL, UK', 24, 168, '2022-08-24 11:33:33.016739', '2022-08-28 20:27:52.843187')]\n",
      "Department of Chemical Sciences, University of Huddersfield, Queensgate, Huddersfield, HD1 3DH, UK True\n",
      "Department of Chemistry, University of Manchester, Manchester M13 9PL, UK True\n",
      "verify one liners\n",
      "Assigned ID: 366 Recoverd ID: 366\n",
      "Assigned ID: 112 Recoverd ID: 112\n",
      "Article Author:  25\n",
      "[(14, 'National Institute for Materials Science (NIMS), 1-2-1 Sengen, Tsukuba, Ibaraki 305-0047, Japan', 25, 169, '2022-08-24 11:33:33.038367', '2022-08-28 20:27:52.878611')]\n",
      "National Institute for Materials Science (NIMS), 1-2-1 Sengen, Tsukuba, Ibaraki 305-0047, Japan True\n",
      "verify one liners\n",
      "Assigned ID: 812 Recoverd ID: 812\n",
      "Article Author:  26\n",
      "[(15, 'Department of Chemical Sciences, University of Huddersfield, Queensgate, Huddersfield, HD1 3DH, UK', 26, 170, '2022-08-24 11:33:33.068463', '2022-08-28 20:27:52.912599')]\n",
      "Department of Chemical Sciences, University of Huddersfield, Queensgate, Huddersfield, HD1 3DH, UK True\n",
      "verify one liners\n",
      "Assigned ID: 366 Recoverd ID: 366\n",
      "Article Author:  27\n",
      "[(16, 'IFIMUP, Department of Physics and Astronomy, Faculty of Science, University of Porto, 4169-007, Porto, Portugal', 27, 2206, '2022-08-24 11:33:33.091103', '2022-08-24 11:33:33.091103')]\n",
      "IFIMUP, Department of Physics and Astronomy, Faculty of Science, University of Porto, 4169-007, Porto, Portugal True\n",
      "verify one liners\n",
      "Assigned ID: 439 Recoverd ID: 439\n",
      "Article Author:  28\n",
      "[(17, 'AWE Aldermaston, Reading, RG7 4PR, UK', 28, 2207, '2022-08-24 11:33:33.118858', '2022-08-24 11:33:33.118858')]\n",
      "AWE Aldermaston, Reading, RG7 4PR, UK True\n",
      "verify one liners\n",
      "Assigned ID: 440 Recoverd ID: 440\n",
      "Article Author:  29\n",
      "[(18, 'AWE Aldermaston, Reading, RG7 4PR, UK', 29, 2209, '2022-08-24 11:33:33.146458', '2022-08-24 11:33:33.146458')]\n",
      "AWE Aldermaston, Reading, RG7 4PR, UK True\n",
      "verify one liners\n",
      "Assigned ID: 440 Recoverd ID: 440\n",
      "Article Author:  30\n",
      "[(19, 'Department of Chemistry, University of Bath, Claverton Down, Bath, BA2 7AY, UK', 30, 186, '2022-08-24 11:33:33.195122', '2022-08-28 20:34:18.443779')]\n",
      "Department of Chemistry, University of Bath, Claverton Down, Bath, BA2 7AY, UK True\n",
      "verify one liners\n",
      "Assigned ID: 47 Recoverd ID: 47\n",
      "Article Author:  31\n",
      "[(20, 'Department of Chemical Sciences, University of Huddersfield, Queensgate, Huddersfield, HD1 3DH, UK', 31, 218, '2022-08-24 11:33:33.228040', '2022-08-28 20:34:20.086328')]\n",
      "Department of Chemical Sciences, University of Huddersfield, Queensgate, Huddersfield, HD1 3DH, UK True\n",
      "verify one liners\n",
      "Assigned ID: 366 Recoverd ID: 366\n",
      "Article Author:  32\n",
      "[(21, 'Centre for Sustainable and Circular Technologies, Department of Chemistry, University of Bath, Claverton Down, Bath BA2 7AY, U.K.', 32, 231, '2022-08-24 11:33:34.502280', '2022-08-28 20:34:20.789456')]\n",
      "Centre for Sustainable and Circular Technologies, Department of Chemistry, University of Bath, Claverton Down, Bath BA2 7AY, U.K. True\n",
      "verify one liners\n",
      "Assigned ID: 113 Recoverd ID: 113\n",
      "Article Author:  33\n",
      "[(22, 'Centre for Sustainable and Circular Technologies, Department of Chemistry, University of Bath, Claverton Down, Bath BA2 7AY, U.K.', 33, 232, '2022-08-24 11:33:34.535898', '2022-08-28 20:34:20.824568')]\n",
      "Centre for Sustainable and Circular Technologies, Department of Chemistry, University of Bath, Claverton Down, Bath BA2 7AY, U.K. True\n",
      "verify one liners\n",
      "Assigned ID: 113 Recoverd ID: 113\n",
      "Article Author:  34\n",
      "[(23, 'Department of Chemical Engineering, University of Bath, Claverton Down, Bath BA2 7AY, U.K.', 34, 233, '2022-08-24 11:33:34.562317', '2022-08-28 20:34:20.855890')]\n",
      "Department of Chemical Engineering, University of Bath, Claverton Down, Bath BA2 7AY, U.K. True\n",
      "verify one liners\n",
      "Assigned ID: 95 Recoverd ID: 95\n",
      "Article Author:  35\n",
      "[(24, 'Centre for Sustainable and Circular Technologies, Department of Chemistry, University of Bath, Claverton Down, Bath BA2 7AY, U.K.', 35, 187, '2022-08-24 11:33:34.581377', '2022-08-28 20:34:18.476078')]\n",
      "Centre for Sustainable and Circular Technologies, Department of Chemistry, University of Bath, Claverton Down, Bath BA2 7AY, U.K. True\n",
      "verify one liners\n",
      "Assigned ID: 113 Recoverd ID: 113\n",
      "Article Author:  76\n",
      "[(25, 'Department of Chemical Sciences, Indian Institute of Science Education and Research (IISER) Mohali, Sector 81, Knowledge City, Manauli 140306, India', 76, 2208, '2022-08-24 11:33:37.861021', '2022-08-24 11:33:37.861021')]\n",
      "Department of Chemical Sciences, Indian Institute of Science Education and Research (IISER) Mohali, Sector 81, Knowledge City, Manauli 140306, India True\n",
      "verify one liners\n",
      "Assigned ID: 441 Recoverd ID: 441\n",
      "Article Author:  77\n",
      "[(26, 'School of Science and Technology, Nottingham Trent University, Clifton Lane, NG11 8NS Nottingham, United Kingdom', 77, 2210, '2022-08-24 11:33:37.888649', '2022-08-24 11:33:37.888649')]\n",
      "School of Science and Technology, Nottingham Trent University, Clifton Lane, NG11 8NS Nottingham, United Kingdom True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify one liners\n",
      "Assigned ID: 442 Recoverd ID: 442\n",
      "Article Author:  78\n",
      "[(27, 'School of Science and Technology, Nottingham Trent University, Clifton Lane, NG11 8NS Nottingham, United Kingdom', 78, 2211, '2022-08-24 11:33:37.913368', '2022-08-24 11:33:37.913368')]\n",
      "School of Science and Technology, Nottingham Trent University, Clifton Lane, NG11 8NS Nottingham, United Kingdom True\n",
      "verify one liners\n",
      "Assigned ID: 442 Recoverd ID: 442\n",
      "Article Author:  79\n",
      "[(28, 'Department of Chemical Sciences, Indian Institute of Science Education and Research (IISER) Mohali, Sector 81, Knowledge City, Manauli 140306, India', 79, 3157, '2022-08-24 11:33:37.955240', '2022-08-24 11:33:37.955240')]\n",
      "Department of Chemical Sciences, Indian Institute of Science Education and Research (IISER) Mohali, Sector 81, Knowledge City, Manauli 140306, India True\n",
      "verify one liners\n",
      "Assigned ID: 441 Recoverd ID: 441\n",
      "Article Author:  80\n",
      "[(29, 'School of Science and Technology', 80, 3516, '2022-08-24 11:50:08.873479', '2022-08-30 14:01:35.627024'), (30, 'Nottingham Trent University', 80, 3516, '2022-08-24 11:50:08.886550', '2022-08-30 14:01:35.640246'), (31, 'Nottingham', 80, 3516, '2022-08-24 11:50:08.902845', '2022-08-30 14:01:35.661514'), (32, 'UK', 80, 3516, '2022-08-24 11:50:08.914753', '2022-08-30 14:01:35.677513')]\n",
      "School of Science and Technology False\n",
      "Nottingham Trent University False\n",
      "Nottingham False\n",
      "UK False\n",
      "verify multiline affi\n"
     ]
    }
   ],
   "source": [
    "import craffiparser\n",
    "\n",
    "def get_parser(db_):\n",
    "    cr_parse = craffiparser.crp(db_)\n",
    "    cr_parse.start_lists()\n",
    "    return cr_parse\n",
    "\n",
    "def get_cr_affis_article_author_ids(db_name):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    a_table = 'cr_affiliations'\n",
    "    a_column = 'article_author_id'\n",
    "    cr_affis_article_author_ids = db_conn.get_value_list(a_table, a_column)\n",
    "    return cr_affis_article_author_ids\n",
    "\n",
    "def get_cr_lines_for_article_author_ids(db_name, art_author_id):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    s_table = 'cr_affiliations'\n",
    "    s_fields = '*'\n",
    "    s_where = \"article_author_id = %s\"%(art_author_id)\n",
    "    authors_list = db_conn.get_values(s_table, s_fields, s_where)\n",
    "    return authors_list\n",
    "\n",
    "def get_affiliation_id(db_name, parsed_affi):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    s_table = 'affiliations'\n",
    "    s_field = 'id'\n",
    "    for k,v in parsed_affi.items():\n",
    "        if \"'\" in v :parsed_affi[k]=v.replace(\"'\",\"''\")\n",
    "    \n",
    "    list_where = [ k +\" = '\"+ v +\"'\" for k,v in parsed_affi.items() if k != 'address']\n",
    "    s_where = \" AND \".join(list_where) \n",
    "    s_where = s_where.replace(\"= ''\", \"IS NULL\")\n",
    "    #print (s_where)\n",
    "    affi_list = db_conn.get_values(s_table, s_field, s_where)\n",
    "    affi_id = None\n",
    "    if affi_list !=[]:\n",
    "        affi_id = affi_list[0][0]\n",
    "    return affi_id\n",
    "\n",
    "def get_auth_affi_affiliation_id(db_name, aut_affi_id):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    s_table = 'author_affiliations'\n",
    "    s_field = 'affiliation_id'\n",
    "    s_where = \" id = %i\" %(aut_affi_id)\n",
    "    #print (s_where)\n",
    "    affi_list = db_conn.get_values(s_table, s_field, s_where)\n",
    "    if affi_list !=[]:\n",
    "        affi_list = list(set([an_id[0] for an_id in affi_list]))\n",
    "    return affi_list\n",
    "\n",
    "def is_one_line_affi(cr_parser, str_affi):\n",
    "    is_one_liner = False\n",
    "    parsed_affi = cr_parser.split_single(str_affi)\n",
    "    parsed_no_blanks = {k:v for k,v in parsed_affi.items() if v != ''}\n",
    "    if len(parsed_no_blanks) > 1:\n",
    "        is_one_liner = True\n",
    "    return is_one_liner\n",
    "\n",
    "def check_assigned_affi_ol(db_name, cr_parser, cr_affi):\n",
    "    assigned_ok = False\n",
    "    if cr_affi[3] != None:\n",
    "        parsed_affi = cr_parser.split_single(cr_affi[1])\n",
    "        parsed_no_blanks = {k:v for k,v in parsed_affi.items() if v != ''}\n",
    "        affi_id = get_affiliation_id(db_name, parsed_affi)\n",
    "        aut_affi = get_auth_affi_affiliation_id(db_name, cr_affi[3])\n",
    "        if aut_affi[0] == affi_id:\n",
    "            print('Assigned ID:', aut_affi[0], \"Recoverd ID:\", affi_id)\n",
    "            assigned_ok = True\n",
    "    return assigned_ok\n",
    "\n",
    "def parse_multiline(db_name, cr_parser, cr_affis):\n",
    "    print('multiline affiliation')\n",
    "    print('try to join multiline')\n",
    "    affi_key = list(parsed_no_blanks.keys())[0]\n",
    "    if parsed_multiline =={}:\n",
    "        parsed_multiline = parsed_affi\n",
    "    elif parsed_multiline[affi_key] ==\"\":\n",
    "        parsed_multiline[affi_key] = parsed_no_blanks[affi_key]\n",
    "    elif parsed_multiline[affi_key] !=\"\" and affi_key == 'address':\n",
    "        parsed_multiline[affi_key] += \", \" + parsed_no_blanks[affi_key]\n",
    "    elif len (auth_cr_affis) == len(multiline_no_blanks):# or parsed_multiline[affi_key] !=\"\":\n",
    "        # check if OK, and start new collector \n",
    "        affi_id = get_parsed_affiliation_id(parsed_multiline)\n",
    "        aut_affi = get_auth_affi_affiliation_id(current_affi)\n",
    "        if affi_id in aut_affi:\n",
    "            # assigned id matches calculated affi_id\n",
    "            print(\"assigned id:\", aut_affi, \"matches calculated:\", affi_id)\n",
    "            affis_ok = True\n",
    "            parsed_multiline\n",
    "        else:\n",
    "            print(\"Author:\", art_aut_id, \"Collected: \", len (auth_cr_affis))\n",
    "            print(\"assigned id:\", aut_affi, \"does not match calculated:\", affi_id)\n",
    "            affis_ok = False\n",
    "            break\n",
    "    parsed_multiline = parsed_affi\n",
    "\n",
    "# Problems:\n",
    "#   Mismatch in assigned affiliation\n",
    "#   Affiliation not assigned\n",
    "\n",
    "# 1 Get list of article_author_ids from CR_affi\n",
    "# 2 For each article_author_id:\n",
    "#   1 Get CR_affi lines\n",
    "#   2 verify CR_affi lines\n",
    "#     1 check if \n",
    "#        a) one affiliation per cr_affi \n",
    "#           parse each single affiliation\n",
    "#           check if assigned affiliation is OK (assigned ID matches calculated ID)\n",
    "#        b) multiple lines form an affiliation (2+)\n",
    "#           parse each multi-line affiliation\n",
    "#           check if assigned affiliation is OK\n",
    "\n",
    "affi_parser = get_parser(ukchapp_db)\n",
    "\n",
    "list_art_aut_ids = get_cr_affis_article_author_ids(ukchapp_db)\n",
    "\n",
    "for art_aut_id in list_art_aut_ids:\n",
    "    print ('Article Author: ', art_aut_id)\n",
    "    cr_lines = get_cr_lines_for_article_author_ids(ukchapp_db, art_aut_id)\n",
    "    print(cr_lines)\n",
    "    all_one_liners = True\n",
    "    for a_cr_line in cr_lines:\n",
    "        one_line_affi = is_one_line_affi(affi_parser, a_cr_line[1])\n",
    "        print( a_cr_line[1], one_line_affi)\n",
    "        if not one_line_affi:\n",
    "            all_one_liners = False\n",
    "    if all_one_liners:\n",
    "        print('verify one liners')\n",
    "        for a_cr_line in cr_lines:\n",
    "            assigned_ok = check_assigned_affi_ol(ukchapp_db, affi_parser, a_cr_line)\n",
    "            if not assigned_ok:\n",
    "                print(\"Problems with \", a_cr_line)\n",
    "    else:\n",
    "        print('verify multiline affi')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1834, 'School of Chemistry', 1210, 469, '2022-08-24 11:52:10.755049', '2022-08-28 21:07:58.823088'), (1835, 'University of Leeds', 1210, 469, '2022-08-24 11:52:10.762317', '2022-08-28 21:07:58.833093'), (1836, 'Leeds LS2 9JT', 1210, 469, '2022-08-24 11:52:10.772644', '2022-08-28 21:07:58.841728'), (1837, 'UK', 1210, 469, '2022-08-24 11:52:10.780377', '2022-08-28 21:07:58.858861'), (1838, 'School of Chemistry', 1210, -1, '2022-08-24 11:52:10.788217', '2022-08-24 11:52:10.788217')]\n"
     ]
    }
   ],
   "source": [
    "cr_lines = get_cr_lines_for_article_author_ids(ukchapp_db, 1210)\n",
    "print(cr_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import craffiparser\n",
    "\n",
    "def get_cr_affiliations(db_name):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    cr_affis = db_conn.get_full_table('cr_affiliations')\n",
    "    return cr_affis\n",
    "\n",
    "def get_parsed_affiliation_id(parsed_affi):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'affiliations'\n",
    "    s_field = 'id'\n",
    "    for k,v in parsed_affi.items():\n",
    "        if \"'\" in v :parsed_affi[k]=v.replace(\"'\",\"''\")\n",
    "    \n",
    "    list_where = [ k +\" = '\"+ v +\"'\" for k,v in parsed_affi.items() if k != 'address']\n",
    "    s_where = \" AND \".join(list_where) \n",
    "    s_where = s_where.replace(\"= ''\", \"IS NULL\")\n",
    "    #print (s_where)\n",
    "    affi_list = db_conn.get_values(s_table, s_field, s_where)\n",
    "    affi_id = None\n",
    "    if affi_list !=[]:\n",
    "        affi_id = affi_list[0][0]\n",
    "    return affi_id\n",
    "\n",
    "def get_auth_affi_affiliation_id(aut_affi_id):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'author_affiliations'\n",
    "    s_field = 'affiliation_id'\n",
    "    s_where = \" id = %i\" %(aut_affi_id)\n",
    "    #print (s_where)\n",
    "    affi_list = db_conn.get_values(s_table, s_field, s_where)\n",
    "    if affi_list !=[]:\n",
    "        affi_list = list(set([an_id[0] for an_id in affi_list]))\n",
    "    return affi_list\n",
    "\n",
    "def set_auth_affi_affiliation_id(aut_affi_id, affi_id):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'author_affiliations'\n",
    "    s_field = 'affiliation_id'\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    db_conn.set_value_table(s_table, aut_affi_id,  s_field , affi_id)\n",
    "\n",
    "\n",
    "def start_parser(db_):\n",
    "    cr_parse = craffiparser.crp(db_)\n",
    "    cr_parse.start_lists()\n",
    "    return cr_parse\n",
    "\n",
    "def check_art_author_affi(cr_parse, art_aut_id, auth_cr_affis): \n",
    "    error_code = 0\n",
    "    current_affi = 0\n",
    "    parsed_multiline ={}\n",
    "    for an_affi in auth_cr_affis:\n",
    "        if current_affi == 0 and an_affi[3]!=None:\n",
    "            current_affi = an_affi[3]\n",
    "        elif current_affi!= an_affi[3] and an_affi[3]!=None:\n",
    "            current_affi = an_affi[3]\n",
    "        elif an_affi[3]==None:\n",
    "            print(\"Affiliation not assigned\")\n",
    "            print(an_affi)\n",
    "            error_code = 3\n",
    "            break\n",
    "        parsed_affi = cr_parse.split_single(an_affi[1])\n",
    "        parsed_no_blanks = {k:v for k,v in parsed_affi.items() if v != ''}\n",
    "        if len(parsed_no_blanks)>1:\n",
    "            affi_id = get_parsed_affiliation_id(parsed_affi)\n",
    "            aut_affi = get_auth_affi_affiliation_id(current_affi)\n",
    "            #print(affi_id)\n",
    "            if affi_id in aut_affi:\n",
    "                #print(\"assigned id:\", aut_affi, \"matches calculated:\", affi_id)\n",
    "                error_code = 0\n",
    "            else:\n",
    "                print(\"assigned id:\", aut_affi, \"does not match calculated:\", affi_id)\n",
    "                if affi_id == None:\n",
    "                    error_code = 2\n",
    "                else:\n",
    "                    error_code = 1\n",
    "                break\n",
    "        if len(parsed_no_blanks)==1:\n",
    "            error_code = 4\n",
    "            print(\"Could be multiline\", parsed_no_blanks, error_code)\n",
    "    return error_code\n",
    "\n",
    "def add_affiliation(affi_values):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    add_update_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    affiliation_new = affi_values\n",
    "    del affiliation_new['address']\n",
    "    if 'address' in affiliation_new.keys():\n",
    "        del affiliation_new['address']\n",
    "    if 'num' in affiliation_new.keys():\n",
    "        del affiliation_new['num']\n",
    "    affiliation_new['created_at'] = add_update_time\n",
    "    affiliation_new['updated_at'] = add_update_time\n",
    "    affiliation_id = db_conn.put_values_table(\"affiliations\", affiliation_new.keys(), affiliation_new.values())\n",
    "    return affiliation_id\n",
    "\n",
    "def add_address(addr_str, affi_id, ctry_str):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    add_update_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    affi_address = {'add_01':addr_str,\"affiliation_id\":affi_id,'country':ctry_str,\n",
    "                    'created_at': add_update_time, 'updated_at':add_update_time}\n",
    "    add_id = db_conn.put_values_table(\"addresses\", affi_address.keys(), affi_address.values())\n",
    "    return add_id\n",
    "\n",
    "def institution_manual_entry(affiliation_values):\n",
    "    print(\"*********** enter institution ************************\")\n",
    "    print(\"Provide institution for:\", affiliation_values)            \n",
    "    ins_str = input()\n",
    "    affiliation_values['institution'] = ins_str\n",
    "    return affiliation_values\n",
    "\n",
    "\n",
    "def custom_split(cr_parse, affiliation_values):\n",
    "    # Look for institution name should be in the address element\n",
    "    print (\"Select how to parse \",  affiliation_values)\n",
    "    option_how = 0\n",
    "    while not option_how in [1,2,3]:\n",
    "        print('options: ')\n",
    "        print('1: parse address element')\n",
    "        print('2: enter manually')\n",
    "        print('3: do nothing ')\n",
    "        print('selection:')\n",
    "        str_opt = input()\n",
    "        if str_opt != '':\n",
    "            option_how = int(str_opt)\n",
    "        else:\n",
    "            option_how = 3\n",
    "    if option_how == 1:\n",
    "        affiliation_values = cr_parse.manual_split_address(affiliation_values)\n",
    "    elif option_how == 2:\n",
    "        affiliation_values = institution_manual_entry(affiliation_values)\n",
    "    return affiliation_values\n",
    "\n",
    "\n",
    "def add_missing(cr_parse, auth_cr_affis):\n",
    "    current_affi = 0\n",
    "    for an_affi in auth_cr_affis:\n",
    "        if current_affi == 0 and an_affi[3]!=None:\n",
    "            current_affi = an_affi[3]\n",
    "        elif current_affi!= an_affi[3] and an_affi[3]!=None:\n",
    "            current_affi = an_affi[3]\n",
    "        elif an_affi[3]==None:\n",
    "            print(\"Affiliation not assigned\")\n",
    "            print(an_affi)\n",
    "            error_code = 3\n",
    "            break\n",
    "        parsed_affi = cr_parse.split_single(an_affi[1])\n",
    "        print(\"* Affi Values:\", parsed_affi)\n",
    "        affiliation_id = address_id = 0\n",
    "        # determine if parsing needs help (institution found?)   \n",
    "        if parsed_affi['institution'] in [\"\", None]:\n",
    "            parsed_affi = custom_split(cr_parse, parsed_affi)\n",
    "        else: \n",
    "            print('could not process string:', auth_cr_affis)\n",
    "            print('results obtained:', parsed_affi)\n",
    "            \n",
    "        address_string = parsed_affi['address']\n",
    "        affiliation_id = add_affiliation(parsed_affi)\n",
    "        # renew lists as new affi is added\n",
    "        cr_parse.start_lists()\n",
    "        address_id = add_address(address_string, affiliation_id, parsed_affi['country'])\n",
    "        \n",
    "        print ('* Added affiliation and address', affiliation_id, address_id)    \n",
    "        print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "\n",
    "def correct_mistmatch_affi(cr_parse, auth_cr_affis):\n",
    "    current_affi = 0\n",
    "    for an_affi in auth_cr_affis:\n",
    "        if current_affi == 0 and an_affi[3]!=None:\n",
    "            current_affi = an_affi[3]\n",
    "        elif current_affi!= an_affi[3] and an_affi[3]!=None:\n",
    "            current_affi = an_affi[3]\n",
    "        elif an_affi[3]==None:\n",
    "            print(\"Affiliation not assigned\")\n",
    "            print(an_affi)\n",
    "            error_code = 3\n",
    "            break\n",
    "        parsed_affi = cr_parse.split_single(an_affi[1])\n",
    "        parsed_no_blanks = {k:v for k,v in parsed_affi.items() if v != ''}\n",
    "        if len(parsed_no_blanks)>1:\n",
    "            affi_id = get_parsed_affiliation_id(parsed_affi)\n",
    "            aut_affi = get_auth_affi_affiliation_id(current_affi)\n",
    "            #print(affi_id)\n",
    "            if not affi_id in aut_affi and len(aut_affi) == 1:\n",
    "                print(\"assigned id:\", aut_affi, \"does not match calculated:\", affi_id)\n",
    "                set_auth_affi_affiliation_id(current_affi, affi_id)\n",
    "                input()\n",
    "            else:\n",
    "                print(\"assigned id:\", aut_affi, \"matches calculated:\", affi_id)\n",
    "                input()\n",
    "\n",
    "affi_error={1:\"Missmatch assignement\", 2:\"Affiliation not found\", \n",
    "            3:\"Affiliation not assigned\", 4:\"Multiline affiliation\"}\n",
    "parser_obj = start_parser(ukchapp_db)\n",
    "\n",
    "cr_affi_data = get_cr_affiliations(ukchapp_db)\n",
    "print(\"Verifying\", len(cr_affi_data), \"Afiliations\")\n",
    "current_art_author = 0\n",
    "collected_current = []\n",
    "problem_assignements=[]\n",
    "\n",
    "safe_list = open_ok_list('ok_affi_list.txt')\n",
    "for a_cr_line in cr_affi_data:\n",
    "    if current_art_author == 0:\n",
    "        current_art_author = int(a_cr_line[2])\n",
    "    if current_art_author != int(a_cr_line[2]) and not current_art_author in safe_list:\n",
    "        # change of author verify if saved affis are OK\n",
    "        verify_result = check_art_author_affi (parser_obj, current_art_author, collected_current)\n",
    "        if not verify_result in [0,4]:\n",
    "            clear_output()\n",
    "            print(\"problems with author affi \", collected_current[0][3], \"**\", affi_error[verify_result])\n",
    "            problem_assignements.append(current_art_author)\n",
    "            if a_cr_line[3] != None: aut_affi = get_auth_affi_affiliation_id(a_cr_line[3])  \n",
    "            else: aut_affi = 0\n",
    "            print(\"****************** PARSED **************************\")\n",
    "            print(collected_current)\n",
    "            print(\"****************** ASSIGNED **************************\")\n",
    "            print(aut_affi)\n",
    "            print(\"Options:\\n\\t1) Fix\\n\\t2) Ignore\\n\\t3) End\" )\n",
    "            option_af =\"\"\n",
    "            option_af=input()\n",
    "            if option_af == '1':\n",
    "                if verify_result in [2]:\n",
    "                    add_missing(parser_obj, collected_current)\n",
    "                if verify_result == 1:\n",
    "                    correct_mistmatch_affi(parser_obj, collected_current)\n",
    "            if option_af=='2':\n",
    "                safe_list.append(current_art_author)\n",
    "            elif option_af=='3':\n",
    "                break\n",
    "        elif verify_result == 0:\n",
    "            safe_list.append(current_art_author)\n",
    "        current_art_author = int(a_cr_line[2])\n",
    "        collected_current = [a_cr_line]\n",
    "    else:\n",
    "        # same author keep collecting affi names\n",
    "        collected_current.append(a_cr_line)\n",
    "    \n",
    "print(\"OK:\", len(safe_list))\n",
    "print(safe_list)\n",
    "save_ok_list(safe_list, 'ok_affi_list.txt')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option_af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "# Check later\n",
    "\n",
    "import craffiparser\n",
    "\n",
    "def get_cr_affiliations(db_name):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    cr_affis = db_conn.get_full_table('cr_affiliations')\n",
    "    return cr_affis\n",
    "\n",
    "def get_parsed_affiliation_id(parsed_affi):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'affiliations'\n",
    "    s_field = 'id'\n",
    "    for k,v in parsed_affi.items():\n",
    "        if \"'\" in v :parsed_affi[k]=v.replace(\"'\",\"''\")\n",
    "    \n",
    "    list_where = [ k +\" = '\"+ v +\"'\" for k,v in parsed_affi.items() if k != 'address']\n",
    "    s_where = \" AND \".join(list_where) \n",
    "    s_where = s_where.replace(\"= ''\", \"IS NULL\")\n",
    "    #print (s_where)\n",
    "    affi_list = db_conn.get_values(s_table, s_field, s_where)\n",
    "    affi_id = None\n",
    "    if affi_list !=[]:\n",
    "        affi_id = affi_list[0][0]\n",
    "    return affi_id\n",
    "\n",
    "def get_auth_affi_affiliation_id(aut_affi_id):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'author_affiliations'\n",
    "    s_field = 'affiliation_id'\n",
    "    s_where = \" id = %i\" %(aut_affi_id)\n",
    "    #print (s_where)\n",
    "    affi_list = db_conn.get_values(s_table, s_field, s_where)\n",
    "    if affi_list !=[]:\n",
    "        affi_list = list(set([an_id[0] for an_id in affi_list]))\n",
    "    return affi_list\n",
    "\n",
    "def set_auth_affi_affiliation_id(aut_affi_id, affi_id):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'author_affiliations'\n",
    "    s_field = 'affiliation_id'\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    db_conn.set_value_table(s_table, aut_affi_id,  s_field , affi_id)\n",
    "\n",
    "\n",
    "def start_parser(db_):\n",
    "    cr_parse = craffiparser.crp(db_)\n",
    "    cr_parse.start_lists()\n",
    "    return cr_parse\n",
    "\n",
    "def check_art_author_affi(cr_parse, art_aut_id, auth_cr_affis): \n",
    "    error_code = 0\n",
    "    current_affi = 0\n",
    "    parsed_multiline ={}\n",
    "    for an_affi in auth_cr_affis:\n",
    "        if current_affi == 0 and an_affi[3]!=None:\n",
    "            current_affi = an_affi[3]\n",
    "        elif current_affi!= an_affi[3] and an_affi[3]!=None:\n",
    "            current_affi = an_affi[3]\n",
    "        elif an_affi[3]==None:\n",
    "            print(\"Affiliation not assigned\")\n",
    "            print(an_affi)\n",
    "            error_code = 3\n",
    "            break\n",
    "        parsed_affi = cr_parse.split_single(an_affi[1])\n",
    "        parsed_no_blanks = {k:v for k,v in parsed_affi.items() if v != ''}\n",
    "        \n",
    "        if len(parsed_no_blanks)>1:\n",
    "            affi_id = get_parsed_affiliation_id(parsed_affi)\n",
    "            aut_affi = get_auth_affi_affiliation_id(current_affi)\n",
    "            #print(affi_id)\n",
    "            if affi_id in aut_affi:\n",
    "                #print(\"assigned id:\", aut_affi, \"matches calculated:\", affi_id)\n",
    "                error_code = 0\n",
    "            else:\n",
    "                #clear_output()\n",
    "       #         print(\"Author:\", art_aut_id, \"Collected: \", len (auth_cr_affis))\n",
    "                print(\"assigned id:\", aut_affi, \"does not match calculated:\", affi_id)\n",
    "       #         print(\"****************** PARSED **************************\")\n",
    "       #         print(parsed_affi)\n",
    "       #         print(\"****************** ASSIGNED **************************\")\n",
    "       #         print(an_affi)\n",
    "       #         print(\"1) fix 2) Ignore\")\n",
    "       #         option_af=input()\n",
    "                \n",
    "       #         affis_ok = False\n",
    "                if affi_id == None:\n",
    "                    error_code = 2\n",
    "                else:\n",
    "                    error_code = 1\n",
    "            \n",
    "                break\n",
    "        if len(parsed_no_blanks)==1:\n",
    "        #    print('multiline affiliation')\n",
    "        #    print('try to join multiline')\n",
    "        #    affi_key = list(parsed_no_blanks.keys())[0]\n",
    "        #    if parsed_multiline =={}:\n",
    "        #        parsed_multiline = parsed_affi\n",
    "        #    elif parsed_multiline[affi_key] ==\"\":\n",
    "        #        parsed_multiline[affi_key] = parsed_no_blanks[affi_key]\n",
    "        #    elif parsed_multiline[affi_key] !=\"\" and affi_key == 'address':\n",
    "        #        parsed_multiline[affi_key] += \", \" + parsed_no_blanks[affi_key]\n",
    "        #    elif len (auth_cr_affis) == len(multiline_no_blanks):# or parsed_multiline[affi_key] !=\"\":\n",
    "        #        # check if OK, and start new collector \n",
    "        #        affi_id = get_parsed_affiliation_id(parsed_multiline)\n",
    "        #        aut_affi = get_auth_affi_affiliation_id(current_affi)\n",
    "        #        if affi_id in aut_affi:\n",
    "        #            # assigned id matches calculated affi_id\n",
    "        #            print(\"assigned id:\", aut_affi, \"matches calculated:\", affi_id)\n",
    "        #            affis_ok = True\n",
    "        #            parsed_multiline\n",
    "        #        else:\n",
    "        #            print(\"Author:\", art_aut_id, \"Collected: \", len (auth_cr_affis))\n",
    "        #            print(\"assigned id:\", aut_affi, \"does not match calculated:\", affi_id)\n",
    "        #            affis_ok = False\n",
    "        #            break\n",
    "        #    parsed_multiline = parsed_affi\n",
    "        #    multiline_no_blanks = {k:v for k,v in parsed_multiline.items() if v != ''}    \n",
    "        #    if len (auth_cr_affis) == len(multiline_no_blanks):\n",
    "        #        affi_id = get_parsed_affiliation_id(parsed_multiline)\n",
    "        #        aut_affi = get_auth_affi_affiliation_id(current_affi)\n",
    "        #        if affi_id in aut_affi:\n",
    "        #            # assigned id matches calculated affi_id\n",
    "        #            print(\"assigned id:\", aut_affi, \"matches calculated:\", affi_id)\n",
    "        #            affis_ok = True\n",
    "        #            parsed_multiline\n",
    "        #        else:\n",
    "        #            print(\"Author:\", art_aut_id, \"Collected: \", len (auth_cr_affis))\n",
    "        #            print(\"assigned id:\", aut_affi, \"does not match calculated:\", affi_id)\n",
    "        #            affis_ok = False\n",
    "        #            break\n",
    "            print(\"Could be multiline\", parsed_no_blanks, error_code)\n",
    "    return error_code\n",
    "\n",
    "def add_affiliation(affi_values):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    add_update_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    affiliation_new = affi_values\n",
    "    del affiliation_new['address']\n",
    "    if 'address' in affiliation_new.keys():\n",
    "        del affiliation_new['address']\n",
    "    if 'num' in affiliation_new.keys():\n",
    "        del affiliation_new['num']\n",
    "    affiliation_new['created_at'] = add_update_time\n",
    "    affiliation_new['updated_at'] = add_update_time\n",
    "    affiliation_id = db_conn.put_values_table(\"affiliations\", affiliation_new.keys(), affiliation_new.values())\n",
    "    return affiliation_id\n",
    "\n",
    "def add_address(addr_str, affi_id, ctry_str):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    add_update_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    affi_address = {'add_01':addr_str,\"affiliation_id\":affi_id,'country':ctry_str,\n",
    "                    'created_at': add_update_time, 'updated_at':add_update_time}\n",
    "    add_id = db_conn.put_values_table(\"addresses\", affi_address.keys(), affi_address.values())\n",
    "    return add_id\n",
    "\n",
    "def institution_manual_entry(affiliation_values):\n",
    "    print(\"*********** enter institution ************************\")\n",
    "    print(\"Provide institution for:\", affiliation_values)            \n",
    "    ins_str = input()\n",
    "    affiliation_values['institution'] = ins_str\n",
    "    return affiliation_values\n",
    "\n",
    "\n",
    "def custom_split(cr_parse, affiliation_values):\n",
    "    # Look for institution name should be in the address element\n",
    "    print (\"Select how to parse \",  affiliation_values)\n",
    "    option_how = 0\n",
    "    while not option_how in [1,2,3]:\n",
    "        print('options: ')\n",
    "        print('1: parse address element')\n",
    "        print('2: enter manually')\n",
    "        print('3: do nothing ')\n",
    "        print('selection:')\n",
    "        str_opt = input()\n",
    "        if str_opt != '':\n",
    "            option_how = int(str_opt)\n",
    "        else:\n",
    "            option_how = 3\n",
    "    if option_how == 1:\n",
    "        affiliation_values = cr_parse.manual_split_address(affiliation_values)\n",
    "    elif option_how == 2:\n",
    "        affiliation_values = institution_manual_entry(affiliation_values)\n",
    "    return affiliation_values\n",
    "\n",
    "\n",
    "def add_missing(cr_parse, auth_cr_affis):\n",
    "    current_affi = 0\n",
    "    for an_affi in auth_cr_affis:\n",
    "        if current_affi == 0 and an_affi[3]!=None:\n",
    "            current_affi = an_affi[3]\n",
    "        elif current_affi!= an_affi[3] and an_affi[3]!=None:\n",
    "            current_affi = an_affi[3]\n",
    "        elif an_affi[3]==None:\n",
    "            print(\"Affiliation not assigned\")\n",
    "            print(an_affi)\n",
    "            error_code = 3\n",
    "            break\n",
    "        parsed_affi = cr_parse.split_single(an_affi[1])\n",
    "        print(\"* Affi Values:\", parsed_affi)\n",
    "        affiliation_id = address_id = 0\n",
    "        # determine if parsing needs help (institution found?)   \n",
    "        if parsed_affi['institution'] in [\"\", None]:\n",
    "            parsed_affi = custom_split(cr_parse, parsed_affi)\n",
    "        else: \n",
    "            print('could not process string:', auth_cr_affis)\n",
    "            print('results obtained:', parsed_affi)\n",
    "            \n",
    "        address_string = parsed_affi['address']\n",
    "        affiliation_id = add_affiliation(parsed_affi)\n",
    "        # renew lists as new affi is added\n",
    "        cr_parse.start_lists()\n",
    "        address_id = add_address(address_string, affiliation_id, parsed_affi['country'])\n",
    "        \n",
    "        print ('* Added affiliation and address', affiliation_id, address_id)    \n",
    "        print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "\n",
    "def correct_mistmatch_affi(cr_parse, auth_cr_affis):\n",
    "    current_affi = 0\n",
    "    for an_affi in auth_cr_affis:\n",
    "        if current_affi == 0 and an_affi[3]!=None:\n",
    "            current_affi = an_affi[3]\n",
    "        elif current_affi!= an_affi[3] and an_affi[3]!=None:\n",
    "            current_affi = an_affi[3]\n",
    "        elif an_affi[3]==None:\n",
    "            print(\"Affiliation not assigned\")\n",
    "            print(an_affi)\n",
    "            error_code = 3\n",
    "            break\n",
    "        parsed_affi = cr_parse.split_single(an_affi[1])\n",
    "        parsed_no_blanks = {k:v for k,v in parsed_affi.items() if v != ''}\n",
    "        if len(parsed_no_blanks)>1:\n",
    "            affi_id = get_parsed_affiliation_id(parsed_affi)\n",
    "            aut_affi = get_auth_affi_affiliation_id(current_affi)\n",
    "            #print(affi_id)\n",
    "            if not affi_id in aut_affi and len(aut_affi) == 1:\n",
    "                print(\"assigned id:\", aut_affi, \"does not match calculated:\", affi_id)\n",
    "                set_auth_affi_affiliation_id(current_affi, affi_id)\n",
    "                input()\n",
    "            else:\n",
    "                print(\"assigned id:\", aut_affi, \"matches calculated:\", affi_id)\n",
    "                input()\n",
    "\n",
    "affi_error={1:\"Missmatch assignement\", 2:\"Affiliation not found\", \n",
    "            3:\"Affiliation not assigned\", 4:\"Multiline affiliation\"}\n",
    "parser_obj = start_parser(ukchapp_db)\n",
    "\n",
    "cr_affi_data = get_cr_affiliations(ukchapp_db)\n",
    "print(\"Verifying\", len(cr_affi_data), \"Afiliations\")\n",
    "current_art_author = 0\n",
    "collected_current = []\n",
    "problem_assignements=[]\n",
    "\n",
    "safe_list = open_ok_list('ok_affi_list.txt')\n",
    "for a_cr_line in cr_affi_data:\n",
    "    if current_art_author == 0:\n",
    "        current_art_author = int(a_cr_line[2])\n",
    "    if current_art_author != int(a_cr_line[2]):\n",
    "        # change of author verify if saved affis are OK\n",
    "        verify_result = check_art_author_affi (parser_obj, current_art_author, collected_current)\n",
    "        if verify_result !=0:\n",
    "            clear_output()\n",
    "            print(\"problems with author affi \", collected_current[0][3], \"**\", affi_error[verify_result])\n",
    "            problem_assignements.append(current_art_author)\n",
    "            if a_cr_line[3] != None: aut_affi = get_auth_affi_affiliation_id(a_cr_line[3])  \n",
    "            else: aut_affi = 0\n",
    "            print(\"****************** PARSED **************************\")\n",
    "            print(collected_current)\n",
    "            print(\"****************** ASSIGNED **************************\")\n",
    "            print(aut_affi)\n",
    "            print(\"Options:\\n\\t1) Fix\\n\\t2) Ignore\\n\\t3) End\" )\n",
    "            option_af=input()\n",
    "            if option_af == '1':\n",
    "                if verify_result in [2]:\n",
    "                    add_missing(parser_obj, collected_current)\n",
    "                if verify_result == 1:\n",
    "                    correct_mistmatch_affi(parser_obj, collected_current)\n",
    "            if option_af=='2':\n",
    "                add_to_ok_list(current_art_author, 'ok_affi_list.txt')\n",
    "            elif option_af=='3':\n",
    "                break\n",
    "        else:\n",
    "            add_to_ok_list(current_art_author, 'ok_affi_list.txt')\n",
    "        current_art_author = int(a_cr_line[2])\n",
    "        collected_current = [a_cr_line]\n",
    "    else:\n",
    "        # same author keep collecting affi names\n",
    "        collected_current.append(a_cr_line)\n",
    "    \n",
    "    #if current_art_author > 26:\n",
    "    #    break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that pdf files exist \n",
    "\n",
    "Use the data on the articles table to verify if file are stored in the corresponding folder\n",
    "We also check that the files in the folder are all accounted for (have a corersponding record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if current_step == 2:\n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "    dups = []\n",
    "    for idx, a_pub in enumerate(tqdm_notebook(app_pubs)):\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        for i_indx in range(idx,len(app_pubs)):\n",
    "            if not (pub_doi is None) and pub_doi.strip().lower() ==  app_pubs[i_indx][2]:\n",
    "                print(pub_doi, \"duplicated at:\", i_indx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukchapp_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if current_step == 2:\n",
    "    for infile in tqdm_notebook(Path(\"pdf_files\").glob('*.pdf')):\n",
    "        file_found = False\n",
    "        for a_pub in app_pubs:\n",
    "            if infile.name == a_pub[4]:\n",
    "                file_found = True\n",
    "                break\n",
    "        if not file_found:\n",
    "            print(\"Not in DB:\", infile.name, \"DB Name\", a_pub[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get missing pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use regular expression to check if a given string\n",
    "# is a valid DOI, using pattern from CR\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    if cr_doi == None:\n",
    "        return False\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# get publication data from the ukch app\n",
    "db_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "if current_step == 2:\n",
    "    for a_pub in tqdm_notebook(db_pubs):\n",
    "        if a_pub[0] > 616:\n",
    "            pub_id = a_pub[0]\n",
    "            pub_title = a_pub[1]\n",
    "            pub_doi = a_pub[2]\n",
    "            pub_url = a_pub[3]\n",
    "            pub_pdf = a_pub[4]\n",
    "            pub_html = a_pub[5]\n",
    "            if pub_pdf == None:\n",
    "                not_in_url = True\n",
    "                print(\"ID: \", pub_id, \"Publication: \",pub_title,\n",
    "                      \"\\n\\tDOI: \", pub_doi, \" URL: \", pub_url)\n",
    "                if \"pdf\" in pub_url:\n",
    "                    print (\"\\tTry to get the pdf from URL: \", pub_url)\n",
    "                    try:\n",
    "                        response = requests.get(pub_url)\n",
    "                        content_type = response.headers['content-type']\n",
    "                        if not 'text' in content_type:\n",
    "                            #print(response.headers)\n",
    "                            cd= response.headers['content-disposition']\n",
    "                            #print(cd)\n",
    "                            fname = re.findall(\"filename=(.+)\", cd)[0]\n",
    "                            #print(fname)\n",
    "                            if not Path('pdf_files/' + pdf_file).is_file():\n",
    "                                with open('pdf_files/'+ fname +'.pdf', 'wb') as f:\n",
    "                                    f.write(response.content)\n",
    "                            else:\n",
    "                                set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            not_in_url = False\n",
    "                    except:\n",
    "                        print(\"ID: \", pub_id, \"\\nPublication: \",pub_title, \n",
    "                               \"\\nDOI: \", pub_doi, \"\\nDOI: \", pub_url) \n",
    "                if not_in_url:\n",
    "                    print(\"\\tTry to see if json file has link to pdf: \")\n",
    "                    if valid_doi(pub_doi):\n",
    "                        crjd, doi_file = pr_fns.get_cr_json_object(pub_doi)\n",
    "                        got_pdf = False\n",
    "                        if \"link\" in crjd.keys():\n",
    "                            for a_link in crjd[\"link\"]:\n",
    "                                if \"\\tURL\" in a_link.keys() and (\"pdf\" in a_link[\"URL\"] or \"pdf\" in a_link[\"content-type\"]):\n",
    "                                    cr_url = a_link[\"URL\"]\n",
    "                                    #print(\"URL: \", cr_url)\n",
    "                                    pdf_file = get_pdf_from_url(cr_url)\n",
    "                                    # if the name corresponds to a existing file, assign value to db_record\n",
    "                                    if Path('pdf_files/' + pdf_file).is_file():\n",
    "                                        print(\"\\tFile name:\", pdf_file)\n",
    "                                        set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                                        got_pdf = True\n",
    "                                    else:\n",
    "                                        print(\"\\tcould not get file from\", cr_url)\n",
    "                        else: \n",
    "                            print(\"\\tno links in json\", pub_doi)\n",
    "                    if not got_pdf and \"elsevier\" in pub_url:\n",
    "                        print(\"\\tTrying elsevier doi:\" )\n",
    "                        pdf_file = pr_fns.get_elsevier_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    elif not got_pdf and \"wiley\" in pub_url:\n",
    "                        print(\"\\tTrying wiley doi:\" )\n",
    "                        pdf_file = pr_fns.get_wiley_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    elif not got_pdf and \"pubs.acs\" in pub_url:\n",
    "                        print(\"\\tTrying acs doi:\" )\n",
    "                        pdf_file = pr_fns.get_acs_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    if not got_pdf:\n",
    "                        print(\"\\tTry doi:  https://doi.org/\" + pub_doi)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pdfminer to get metadata from pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer import high_level as pdfmnr_hl\n",
    "\n",
    "# functions for PDFminer\n",
    "\n",
    "def get_pdf_text(pdf_file):\n",
    "    return pdfmnr_hl.extract_text(pdf_file)\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_ref_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if pr_fns.is_data_stmt(sentence.lower()):\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1,idx,idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_all_data_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if 'data' in sentence.lower() or 'inform' in sentence.lower():\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1, idx, idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        if not full_sentence in return_group:\n",
    "            return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the http strings from references to data\n",
    "def get_http_ref(sentence):\n",
    "    http_frag = \"\"\n",
    "    if 'http' in sentence.lower():\n",
    "        idx_http = sentence.lower().index('http')\n",
    "        http_frag = sentence[idx_http:]\n",
    "        space_in_ref = True\n",
    "        while \" \" in http_frag:\n",
    "            space_idx = http_frag.rfind(\" \")\n",
    "            http_frag = http_frag[:space_idx]\n",
    "        if(http_frag[-1:]==\".\"):\n",
    "            http_frag = http_frag[:-1]\n",
    "    return http_frag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_step == 2:\n",
    "    # get publication data from the ukch app\n",
    "    db_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "    # get the list of dois already mined for data \n",
    "    input_file = 'pub_data_add202012.csv'\n",
    "    id_field = 'num'\n",
    "    processed, headings = csvh.get_csv_data(input_file, id_field)\n",
    "    for id_num in processed:\n",
    "        current_title = processed[id_num]['doi']\n",
    "    processed[1]['num']\n",
    "\n",
    "    processed_dois = []\n",
    "    for entry in processed:\n",
    "        if not processed[entry]['doi'] in processed_dois:\n",
    "            processed_dois.append( processed[entry]['doi'])\n",
    "\n",
    "    data_records = {}\n",
    "    data_mentions = {}\n",
    "    ref_count = mention_count = 0\n",
    "    for a_pub in tqdm_notebook(db_pubs):\n",
    "        data_refs = []\n",
    "        data_sents = []\n",
    "        if a_pub[0] > 616:\n",
    "            pub_id = a_pub[0]\n",
    "            pub_title = a_pub[1]\n",
    "            pub_doi = a_pub[2]\n",
    "            pub_url = a_pub[3]\n",
    "            pub_pdf = a_pub[4]\n",
    "            pub_html = a_pub[5]\n",
    "            if pub_pdf == 'None':\n",
    "                print(\"*************************\")\n",
    "                print(\"Missing PDF for:\", pub_doi)\n",
    "                print(\"*************************\")\n",
    "            else:\n",
    "                pdf_file = \"pdf_files/\" + pub_pdf\n",
    "                if not Path(pdf_file).is_file():\n",
    "                    print(\"*************************\")\n",
    "                    print(\"Missing file for:\", pdf_file, \"for\", pub_doi)\n",
    "                    print(\"*************************\")\n",
    "                else: \n",
    "                    print(\"PDF filename\", pdf_file)\n",
    "                    pdf_text = get_pdf_text(pdf_file)\n",
    "                    ref_sentences = get_ref_sentences(pdf_text)\n",
    "                    data_sentences = get_all_data_sentences(pdf_text)\n",
    "                    for r_sentence in ref_sentences:\n",
    "                        dt_link = get_http_ref(r_sentence)\n",
    "                        if 'supplem' in r_sentence.lower():\n",
    "                            data_refs.append({'type':'supplementary',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                        else:\n",
    "                            data_refs.append({'type':'supporting',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                    for d_sentence in data_sentences:\n",
    "                        dt_link = get_http_ref(d_sentence)\n",
    "                        if 'supplem' in d_sentence.lower():\n",
    "                            data_sents.append({'type':'supplementary',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "                        else:\n",
    "                            data_sents.append({'type':'supporting',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "            if data_refs != []:\n",
    "                for data_ref in data_refs:\n",
    "                    data_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                    data_record.update(data_ref)\n",
    "                    data_records[ref_count] = data_record\n",
    "                    ref_count += 1\n",
    "            if data_sents != []:\n",
    "                for data_sent in data_sents:\n",
    "                    sentence_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                    sentence_record.update(data_sent)\n",
    "                    data_mentions[mention_count] = sentence_record\n",
    "                    mention_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the results to a csv file to be checked to verify if data mentions are actually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if len(data_records) > 0:\n",
    "#    csvh.write_csv_data(data_records, 'pdf_data.csv')\n",
    "if current_step == 2:    \n",
    "    if len(data_mentions) > 0:\n",
    "        csvh.write_csv_data(data_mentions, 'pdf_mentions202110.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify if the mentions of data or information actually can be linked to data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "if current_step == 3:\n",
    "    print(ukchapp_db)\n",
    "    print(len(app_pubs))\n",
    "    # Open results file\n",
    "    data_mentions, dm_headers = csvh.get_csv_data('pdf_mentions202110.csv')\n",
    "    print(dm_headers)\n",
    "    art_id = ''\n",
    "    for dm in data_mentions:\n",
    "        if data_mentions[dm]['action']=='':\n",
    "            clear_output()\n",
    "            print (\"*******************************************\")\n",
    "            print (\"Article id  :\", data_mentions[dm]['id'])\n",
    "            print (\"DOI         :\", data_mentions[dm]['doi'])\n",
    "            print (\"Type        :\", data_mentions[dm]['type'], '\\tLine:', dm)\n",
    "            print (\"Description :\\n\\t\", data_mentions[dm]['desc'])\n",
    "            print (\"data_url :\", data_mentions[dm]['data_url'])\n",
    "            print (\"*******************************************\")\n",
    "            decide_action = False\n",
    "            while not decide_action:\n",
    "                print('Action:')\n",
    "                print('\\ta) review')\n",
    "                print('\\tb) none')\n",
    "                print('\\tSelect a or b:')\n",
    "                lts = input()\n",
    "                if lts == \"a\":\n",
    "                    data_mentions[dm]['action'] = 'review'\n",
    "                    decide_action = True\n",
    "                elif lts == \"b\":\n",
    "                    data_mentions[dm]['action'] = 'none'\n",
    "                    decide_action = True\n",
    "        art__id = data_mentions[dm]['id']\n",
    "        if dm > 1700:\n",
    "            break\n",
    "    if len(data_mentions) > 0:\n",
    "       csvh.write_csv_data(data_mentions, 'pdf_mentions202110.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the output after each loop cycle\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# display editable spreadsheet\n",
    "import ipysheet\n",
    "\n",
    "\n",
    "# show gds parameters in a spreadsheet on jupyter\n",
    "def show_gds(gds_group):\n",
    "    gds_list = gds_to_list(gds_group)\n",
    "    #print(gds_list)\n",
    "    #add 10 more rows in case we need more parameters\n",
    "    for i in range(10):\n",
    "        gds_list.append([(len(gds_list)-1)+1,None,None,None,None])\n",
    "    a_sheet = ipysheet.sheet(rows=len(gds_list), columns=len(gds_list[0]))\n",
    "    ipysheet.cell_range(gds_list)\n",
    "    display(a_sheet)\n",
    "    return a_sheet\n",
    "\n",
    "if current_step == 3:\n",
    "    print(ukchapp_db)\n",
    "    print(len(app_pubs))\n",
    "    # Open results file\n",
    "    data_mentions, dm_headers = csvh.get_csv_data('pdf_mentions202110.csv')\n",
    "    print(dm_headers)\n",
    "    art_id = ''\n",
    "    terminate = False\n",
    "    additional_rows = {}\n",
    "    for dm in data_mentions:\n",
    "        if data_mentions[dm]['action']=='review':\n",
    "            clear_output()\n",
    "            print (\"*******************************************\")\n",
    "            print (\"Article id  :\", data_mentions[dm]['id'])\n",
    "            print (\"DOI         :\", data_mentions[dm]['doi'])\n",
    "            print (\"Type        :\", data_mentions[dm]['type'], '\\tLine:', dm)\n",
    "            print (\"Description :\\n\\t\", data_mentions[dm]['desc'])\n",
    "            print (\"data_url :\", data_mentions[dm]['data_url'])\n",
    "            print (\"*******************************************\")\n",
    "            decide_action = False\n",
    "            while not decide_action:\n",
    "                print('Action:')\n",
    "                print('\\ta) review: https://doi.org/'+data_mentions[dm]['doi'])\n",
    "                print('\\ts) add new row')\n",
    "                print('\\td) next')\n",
    "                print('\\tf) terminate')\n",
    "                print('\\tSelect a, s, d, f:')\n",
    "                lts = input()\n",
    "                if lts == \"a\":\n",
    "                    data_mentions[dm]['action'] = 'reviewed'\n",
    "                    print ('https://doi.org/'+data_mentions[dm]['doi'])\n",
    "                    print ('link:',data_mentions[dm]['link'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['link'] = add_this\n",
    "                    print ('issue:',data_mentions[dm]['issue'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['issue'] = add_this\n",
    "                    print ('name:',data_mentions[dm]['name'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['name'] = add_this\n",
    "                    print ('file:',data_mentions[dm]['file'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['file'] = add_this\n",
    "                if lts == \"s\":\n",
    "                    #add a new row\n",
    "                    new_idx = len(data_mentions) + len(additional_rows) + 1\n",
    "                    additional_rows[new_idx] = data_mentions[dm]\n",
    "                    print ('link:',additional_rows[new_idx]['link'])\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['link'] = add_this\n",
    "                    print ('issue:',additional_rows[new_idx]['issue'])\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['issue'] = add_this\n",
    "                    print ('name:',additional_rows[new_idx]['name'])\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['name'] = add_this\n",
    "                    print ('file:',additional_rows[new_idx]['file'])\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['file'] = add_this\n",
    "                elif lts == \"d\":\n",
    "                    if data_mentions[dm]['action'] != 'reviewed':\n",
    "                        data_mentions[dm]['action'] = 'none'\n",
    "                    decide_action = True\n",
    "                elif lts == 'f':\n",
    "                    decide_action = True\n",
    "                    terminate = True\n",
    "        art__id = data_mentions[dm]['id']\n",
    "        if dm > 1700 or terminate:\n",
    "            break\n",
    "    if len(additional_rows)> 0 :\n",
    "        for nr in additional_rows:\n",
    "            for a_field in dm_headers:\n",
    "                data_mentions[nr][a_field] = additional_rows[nr][a_field]\n",
    "    if len(data_mentions) > 0:\n",
    "       csvh.write_csv_data(data_mentions, 'pdf_mentions202110.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_mentions = {}\n",
    "for dm in data_mentions:\n",
    "    if data_mentions[dm]['action'] in ['add', 'reviewed']:\n",
    "        filter_mentions[dm]={}\n",
    "        for a_field in dm_headers:\n",
    "            filter_mentions[dm][a_field] = data_mentions[dm][a_field]\n",
    "print('filtered mentions:', len(filter_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_do_id_list =[]\n",
    "for fm in filter_mentions:\n",
    "    art_id = int(filter_mentions[fm][\"id\"])\n",
    "    if not art_id in new_do_id_list:\n",
    "        new_do_id_list.append(art_id)\n",
    "\n",
    "# currend app DB\n",
    "ukchapp_db = \"db_files/app_db20211005.sqlite3\"\n",
    "\n",
    "no_data_pubs = pr_fns.get_pub_app_no_data(ukchapp_db)\n",
    "#print(len(ids_w_data))\n",
    "print(len(no_data_pubs))\n",
    "print(new_do_id_list, len(new_do_id_list))\n",
    "filter_mentions\n",
    "\n",
    "\n",
    "int_idx = 0\n",
    "revised_list = {}\n",
    "if Path(\"./html_revised202111.csv\").is_file():\n",
    "    revised_list, rl_headers = csvh.get_csv_data('html_revised202111.csv')\n",
    "    int_idx = len(revised_list)\n",
    "    \n",
    "already_revised =[]\n",
    "for fm in revised_list:\n",
    "    art_id = int(revised_list[fm][\"id\"])\n",
    "    if not art_id in already_revised:\n",
    "        already_revised.append(art_id)\n",
    "    \n",
    "for ndp in no_data_pubs:\n",
    "    if not ndp[0] in new_do_id_list and ndp[0] > 616 and not ndp[0] in already_revised:\n",
    "        int_idx += 1\n",
    "        pub_id = ndp[0]\n",
    "        pub_title = ndp[1]\n",
    "        pub_doi = ndp[2]\n",
    "        pub_url = ndp[3]\n",
    "        data_record = {'id':pub_id, 'doi':pub_doi, 'title':pub_title} \n",
    "        print ('id',pub_id, '\\n', pub_title)\n",
    "        decide_action = False\n",
    "        terminate = False\n",
    "        while not decide_action:\n",
    "            print('Action:')\n",
    "            print(pub_url)\n",
    "            print(\"https://doi.org/\"+pub_doi)\n",
    "            print('\\ta) no data' )\n",
    "            print('\\ts) review')\n",
    "            print('\\td) next')\n",
    "            print('\\tf) terminate')\n",
    "            print('\\tSelect a, s, d, f:')\n",
    "            lts = input()\n",
    "            if lts == \"a\":\n",
    "                data_record['action'] = 'no data'\n",
    "                data_record['issue'] = \"no data availability or supplementary data mentioned in html or pdf versions or article\"\n",
    "                revised_list[int_idx] = data_record\n",
    "                decide_action = True\n",
    "            if lts == \"s\":\n",
    "                data_record['action'] = 'review'\n",
    "                print ('issue:',data_mentions[dm]['issue'])\n",
    "                add_this = input()\n",
    "                data_record['issue'] = add_this\n",
    "                revised_list[int_idx] = data_record\n",
    "                decide_action = True\n",
    "            if lts == \"d\":\n",
    "                decide_action = True\n",
    "            elif lts == 'f':\n",
    "                decide_action = True\n",
    "                terminate = True\n",
    "        if terminate:\n",
    "            break\n",
    "\n",
    "if len(revised_list) > 0:\n",
    "    csvh.write_csv_data(revised_list, 'html_revised202111.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(revised_list) > 0:\n",
    "    csvh.write_csv_data(revised_list, 'html_revised202111.csv')\n",
    "revised_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for ChemDataExtractor\n",
    "# not used for mining data references (suplementary/raw) or to get pdf metadata\n",
    "from chemdataextractor import Document\n",
    "\n",
    "# A function for getting a list of files from the directory\n",
    "# This will be modified to get the list from a csv file\n",
    "def get_files_list (source_dir):\n",
    "    i_counter = 0\n",
    "    files_list = []\n",
    "    for filepath in sorted(source_dir.glob('*.pdf')):\n",
    "        i_counter += 1\n",
    "        files_list.append(filepath)\n",
    "    return files_list\n",
    "\n",
    "def cde_read_pdfs(a_file):\n",
    "    pdf_f = open(a_file, 'rb')\n",
    "    doc = Document.from_file(pdf_f)\n",
    "    return doc\n",
    "\n",
    "def find_doi(element_text):\n",
    "    cr_re_01 = '10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.search(cr_re_01, element_text, re.IGNORECASE)\n",
    "    if compare != None:\n",
    "        return compare.group()\n",
    "    return \"\"\n",
    "\n",
    "def get_db_id(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"id\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_db_title(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"title\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_close_dois(str_name, db_name = \"prev_search.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, doi, title, pdf_file\"\n",
    "    filter_str = \"doi like '%\"+str_name+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"db_files/app_db2.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "ukchapp_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names and links for references in data mentions\n",
    "data_mentions, dm_fields = csvh.get_csv_data('pdf_mentions_filtered_02.csv', 'num')\n",
    "\n",
    "for dm in data_mentions:\n",
    "    print(\"https://doi.org/\" + data_mentions[dm]['doi'])\n",
    "    ref_name = data_mentions[dm]['ref_name']\n",
    "    while ref_name == \"\":\n",
    "        print('Please enter the name of data object:')\n",
    "        ref_name = input()\n",
    "    ref_link = data_mentions[dm]['ref_link']\n",
    "    while ref_link == \"\":\n",
    "        print('Please enter the data object link:')\n",
    "        ref_link = input()\n",
    "    data_mentions[dm]['ref_name'] = ref_name\n",
    "    data_mentions[dm]['ref_link'] = ref_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getmembers, isfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pdfminer.high_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
