{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check fo duplicate articles and data objects\n",
    "A list of publications is obtainded from the app database. This list will contain a titles, IDs and DOIs which need to be explored to look for duplicates. \n",
    "The steps of the process are: \n",
    " 1. get a Title, DOI, and URL for each publication\n",
    " 2. revise each element of the list for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# library containign functions that read and write to csv files\n",
    "import lib.handle_csv as csvh\n",
    "# library for connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "# library for handling text matchings\n",
    "import lib.text_comp as txtc\n",
    "# library for getting data from crossref\n",
    "import lib.crossref_api as cr_api\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "# add aprogress bar\n",
    "from tqdm import tqdm_notebook \n",
    "from tqdm import tqdm\n",
    "#library for handling json files\n",
    "import json\n",
    "# library for using regular expressions\n",
    "import re\n",
    "# library for handling http requests\n",
    "import requests\n",
    "# import custom functions (common to various notebooks)\n",
    "import processing_functions as pr_fns\n",
    "\n",
    "# datetime parsing\n",
    "from datetime import datetime\n",
    "\n",
    "current_step = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if there are duplicates in articles\n",
    "\n",
    "1. Open the current publication list from the appdb\n",
    "2. read each entry and check if there are duplicates in doi, url or title\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with no pdf_file column\n",
    "def get_pubs_list(db_path):\n",
    "    pubs_list = None\n",
    "    try:\n",
    "        pubs_list = pr_fns.get_pub_data(db_path)\n",
    "    except Exception as inst:\n",
    "        if 'pdf_file' in inst.args[0]:\n",
    "            print('problem articles table does not have pdf_file column')\n",
    "            pass\n",
    "    try:\n",
    "        if pubs_list == None:\n",
    "            pubs_list = pr_fns.get_pub_app_data(db_path)\n",
    "    except Exception as inst:\n",
    "        print(type(inst))\n",
    "        print(inst.args)\n",
    "        print(inst)\n",
    "        print('another problem')    \n",
    "    return pubs_list\n",
    "\n",
    "# 1 current app DB\n",
    "db_name = 'development'\n",
    "#ukchapp_db = \"db_files/\"+db_name+\".sqlite3\"\n",
    "ukchapp_db = \"../mcc_data/\"+db_name+\".sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "    \n",
    "#  get publication data from the ukch app\n",
    "app_pubs = get_pubs_list(ukchapp_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "# 2 read each entry and check if there are duplicates in doi, url or title\n",
    "dup_list={}\n",
    "dup_count = 0\n",
    "if current_step == 1 and app_pubs != None:  \n",
    "    dups = []\n",
    "    for idx, a_pub in enumerate(tqdm_notebook(app_pubs)):\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        # verfy if dois are duplicated\n",
    "        if pub_doi != None:\n",
    "            for i_indx in range(idx+1, len(app_pubs)):\n",
    "                #print(pub_doi, app_pubs[i_indx][2])\n",
    "                if app_pubs[i_indx][2]!=None and pub_doi.strip().lower() ==  app_pubs[i_indx][2].strip().lower():\n",
    "                    print(\"DOI\", pub_doi, \"duplicated at:\", i_indx, app_pubs[i_indx], app_pubs[idx] )\n",
    "                    a_dup = {'pub_comp': app_pubs[idx], 'pub_dup': app_pubs[i_indx],\"dup_at\":'DOI'}\n",
    "                    dup_count+=1\n",
    "                    dup_list[dup_count] = a_dup\n",
    "        # verify if urls are all unique\n",
    "        if pub_url != None:\n",
    "            for i_indx in range(idx+1, len(app_pubs)):\n",
    "                if app_pubs[i_indx][3]!=None and pub_url.strip().lower() ==  app_pubs[i_indx][3].strip().lower():\n",
    "                    print(\"URL\", pub_url, \"duplicated at:\", i_indx, app_pubs[i_indx], app_pubs[idx] )    \n",
    "                    a_dup = {'pub_comp': app_pubs[idx], 'pub_dup': app_pubs[i_indx],\"dup_at\":'URL'}\n",
    "                    dup_count+=1\n",
    "                    dup_list[dup_count] = a_dup\n",
    "        # verify if titles are all unique\n",
    "        if pub_title != None:\n",
    "            for i_indx in range(idx+1, len(app_pubs)):\n",
    "                similarity = txtc.similar(pub_title.strip().lower(), app_pubs[i_indx][1].strip().lower())\n",
    "                #print(similarity)\n",
    "                if app_pubs[i_indx][1]!=None and pub_title.strip().lower() ==  app_pubs[i_indx][1].strip().lower():\n",
    "                    print(\"Title\", pub_title, \"duplicated at:\", i_indx,app_pubs[i_indx][1], app_pubs[idx][1])     \n",
    "                    print(\"Similarity:\", similarity)\n",
    "                    a_dup = {'pub_comp': app_pubs[idx], 'pub_dup': app_pubs[i_indx],\"dup_at\":'Title'}\n",
    "                    dup_count+=1\n",
    "                    dup_list[dup_count] = a_dup\n",
    "                #elif similarity > 0.8:\n",
    "                #    print(similarity, \"Title:\\n\\t\", pub_title, \"\\n\\t- similar at:\\n\\t\", i_indx,app_pubs[i_indx][1]) \n",
    "if len(dup_list) > 0:\n",
    "        csvh.write_csv_data(dup_list, 'dup_'+db_name+'.csv')\n",
    "else:\n",
    "    print (\"No duplicate articles in DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if there are duplicates in data objects\n",
    "\n",
    "1. get the current data objects list from the appdb\n",
    "2. read each entry and check if there are duplicates in doi, url or title\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1 current app DB\n",
    "dup_list={}\n",
    "db_name = 'development'\n",
    "#ukchapp_db = \"db_files/\"+db_name+\".sqlite3\"\n",
    "ukchapp_db = \"../mcc_data/\"+db_name+\".sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "    \n",
    "#  get datasets list from the ukch app\n",
    "app_datasetes = pr_fns.get_dataset_data(ukchapp_db)\n",
    "\n",
    "# 2 read each entry and check if there are duplicates in doi, url or title\n",
    "if current_step == 2:  \n",
    "    dup_list={}\n",
    "    dup_count = 0\n",
    "    for idx, a_ds in enumerate(tqdm_notebook(app_datasetes)):\n",
    "        ds_id = a_ds[0]\n",
    "        ds_doi = a_ds[1]\n",
    "        ds_url = a_ds[2]\n",
    "        ds_name = a_ds[3]\n",
    "        #print (ds_id, ds_doi, ds_url, ds_name)\n",
    "        # verfy if dois are duplicated\n",
    "        if ds_doi != None and ds_doi != '':\n",
    "            for i_indx in range(idx+1, len(app_datasetes)):\n",
    "                #print(pub_doi, app_pubs[i_indx][1])\n",
    "                if app_datasetes[i_indx][1]!=None and ds_doi.strip().lower() ==  app_datasetes[i_indx][1].strip().lower():\n",
    "                    print(\"DOI\", ds_doi, \"duplicated at:\", i_indx, app_datasetes[i_indx], app_datasetes[idx] )\n",
    "                    a_dup = {'pub_comp': app_datasetes[idx], 'pub_dup': app_datasetes[i_indx],\"dup_at\":'DOI'}\n",
    "                    dup_count+=1\n",
    "                    dup_list[dup_count] = a_dup\n",
    "        # verify if urls are all unique\n",
    "        if ds_url != None:\n",
    "            for i_indx in range(idx+1, len(app_datasetes)):\n",
    "                #print(app_datasetes[i_indx])\n",
    "                if app_datasetes[i_indx][2]!=None and ds_url.strip().lower() ==  app_datasetes[i_indx][2].strip().lower():\n",
    "                    print(\"URL\", ds_url, \"duplicated at:\", i_indx, app_datasetes[i_indx], app_datasetes[idx] )  \n",
    "                    a_dup = {'pub_comp': app_datasetes[idx], 'pub_dup': app_datasetes[i_indx],\"dup_at\":'URL'}\n",
    "                    dup_count+=1\n",
    "                    dup_list[dup_count] = a_dup\n",
    "        # verify if titles are all unique\n",
    "        if ds_name != None:\n",
    "            for i_indx in range(idx+1, len(app_datasetes)):\n",
    "                similarity = txtc.similar(ds_name.strip().lower(), app_datasetes[i_indx][3].strip().lower())\n",
    "                #print(similarity)\n",
    "                if app_datasetes[i_indx][1]!=None and ds_name.strip().lower() ==  app_datasetes[i_indx][3].strip().lower():\n",
    "                    print(\"Title\", ds_name, \"duplicated at:\", i_indx,app_datasetes[i_indx][3], app_datasetes[idx][3])     \n",
    "                    print(\"Similarity:\", similarity)\n",
    "                    a_dup = {'pub_comp': app_datasetes[idx], 'pub_dup': app_datasetes[i_indx],\"dup_at\":'Title'}\n",
    "                    dup_count+=1\n",
    "                    dup_list[dup_count] = a_dup\n",
    "                #elif similarity > 0.8:\n",
    "                #    print(similarity, \"Title:\\n\\t\", ds_name, \"\\n\\t- similar at:\\n\\t\", i_indx,app_datasetes[i_indx][3]) \n",
    "                #    break\n",
    "if len(dup_list) > 0:\n",
    "    csvh.write_csv_data(dup_list, 'dup_do_'+db_name+'.csv')\n",
    "else:\n",
    "    print (\"No duplicate DOs in DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify authors\n",
    "\n",
    "1. verify that there are no authors with no articles in the DB\n",
    "2. verify that authors are unique remove close matches (need to check spellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_like_str(a_string):\n",
    "    like_str = \"%\" + re.sub(r'[^a-zA-Z\\s:]', '%', a_string) + \"%\"\n",
    "    return like_str\n",
    "\n",
    "def get_all_authors():\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'authors'\n",
    "    s_fields = 'id, given_name, last_name, orcid'\n",
    "    s_where = 'isap IS NULL' # 1 displayed authors - 0/NULL the rest\n",
    "    authors_list = db_conn.get_values(s_table, s_fields, s_where)\n",
    "    return authors_list\n",
    "\n",
    "def get_null_authors():\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'authors'\n",
    "    s_fields = 'id, given_name, last_name, orcid'\n",
    "    s_where = 'id NOT IN (SELECT article_authors.author_id FROM article_authors)'\n",
    "    authors_list = db_conn.get_values(s_table, s_fields, s_where)\n",
    "    return authors_list\n",
    "\n",
    "def get_similar_authors(name,surname,orcid):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'authors'\n",
    "    s_fields = 'id, given_name, last_name, orcid'\n",
    "    like_surname = make_like_str(surname)\n",
    "    surname.replace(\"'\",\"''\")\n",
    "    s_where = \"(orcid = '%s' AND last_name = '%s')\"%(orcid, surname)\n",
    "    s_where += \"OR(given_name = '%s' AND last_name = '%s')\"%(name, surname)\n",
    "    s_where += \"OR(last_name = '%s')\"%(surname)\n",
    "    s_where += \"OR(last_name LIKE '%s')\"%(like_surname)\n",
    "    authors_list = db_conn.get_values(s_table, s_fields, s_where)\n",
    "    return authors_list\n",
    "\n",
    "def count_linked(author_id):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_table = 'article_authors'\n",
    "    s_fields = 'id, author_id'\n",
    "    s_where = \"(author_id = %s)\"%(author_id)\n",
    "    aa_list = db_conn.get_values(s_table, s_fields, s_where)\n",
    "    return len(aa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def save_ok_list(values_list, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for an_id in values_list:\n",
    "            f.write(str(an_id)+'\\n')\n",
    "\n",
    "def open_ok_list(file_name):\n",
    "    with open(file_name) as f:\n",
    "        lines = f.readlines()\n",
    "    from_file = []\n",
    "    for a_line in lines:\n",
    "        from_file.append(int(a_line.replace('\\n','')))\n",
    "    return from_file\n",
    "\n",
    "def add_to_ok_list(a_value, file_name):\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(str(a_value)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_single_word(a_word, another_word):\n",
    "    single_word = False\n",
    "    in_word = another_word.lower().find(a_word.lower())\n",
    "    if in_word >= 0:\n",
    "        single_word = True\n",
    "        if in_word > 0 and  another_word[in_word-1].isalpha():\n",
    "            sinlge_word = False\n",
    "        if in_word + len(a_word) < len(another_word)-1 and another_word[in_word + len(a_word)].isalpha():\n",
    "            single_word = False\n",
    "    return single_word\n",
    "\n",
    "def prune_similar_surnames(the_similars, a_surname):\n",
    "    pruned_list = []\n",
    "    for a_simi in the_similars:\n",
    "        if a_simi[2] == a_surname or is_single_word(a_surname,a_simi[2]) :\n",
    "            pruned_list.append(a_simi)\n",
    "    return pruned_list\n",
    "\n",
    "def get_initials(given_names):\n",
    "    initials_1 =  [a_letter for a_letter in given_names if a_letter.isupper() ] \n",
    "    names = given_names.split()\n",
    "    initials_2 = [a_name[0] for a_name in given_names]\n",
    "    ri_1 = \". \".join(initials_1)+\".\"\n",
    "    ri_2 = \" \".join(initials_1)\n",
    "    return ri_1, ri_2\n",
    "    \n",
    "def prune_similar_names(the_similars, a_name):\n",
    "    pruned_list = []\n",
    "    dot_initials, initials = get_initials(a_name)\n",
    "    for a_simi in the_similars:\n",
    "        if a_simi[1] == a_name or is_single_word(a_name,a_simi[1]) :\n",
    "            pruned_list.append(a_simi)\n",
    "        elif initials == a_simi[1] or dot_initials == a_simi[1]:\n",
    "            pruned_list.append(a_simi)    \n",
    "    return pruned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_author_value(a_id, a_column, a_value):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    db_conn.set_value_table('authors', a_id, a_column, a_value)\n",
    "\n",
    "\n",
    "def update_author(old_author, a_id, a_name, a_surname, a_orcid):\n",
    "    if a_id != old_author[0]:\n",
    "        return # do not update different authors        \n",
    "    if a_name != old_author[1]:\n",
    "        set_author_value(a_id, 'given_name', a_name)\n",
    "    if a_surname != old_author[2]:\n",
    "        set_author_value(a_id, 'last_name', a_surname)\n",
    "    if a_orcid != old_author[3]:\n",
    "        set_author_value(a_id, 'given_name', a_orcid)\n",
    "\n",
    "def update_article_authors(new_id, old_id):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    s_where = \"author_id = %s\" % (old_id)\n",
    "    aa_ids = db_conn.get_values('article_authors', 'id', s_where)\n",
    "    print(aa_ids)\n",
    "    for an_id in aa_ids:\n",
    "        db_conn.set_value_table('article_authors', an_id[0], 'author_id', new_id)\n",
    "\n",
    "def delete_author(a_id):\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    db_conn.connection.execute(\"DELETE FROM authors WHERE id = %s\" % (a_id ))\n",
    "    db_conn.connection.commit()\n",
    "    \n",
    "def merge_authors(an_author, auth_similars):\n",
    "    auth_id = an_author[0]\n",
    "    auth_name = an_author[1]\n",
    "    auth_surname = an_author[2]\n",
    "    auth_orcid = an_author[3]\n",
    "\n",
    "    for a_result in auth_similars:\n",
    "        if auth_id > a_result[0]:\n",
    "            auth_id = a_result[0]\n",
    "        if auth_name != a_result[1] and len(auth_name) < len(a_result[1]):\n",
    "            print(\"Which name \\n\\t 1)\", auth_name, \"\\n\\t 2)\", a_result[1])\n",
    "            opt_name = input()\n",
    "            if opt_name == '2': auth_name = a_result[1] \n",
    "        if auth_surname != a_result[2] and len(auth_surname) < len(a_result[2]):\n",
    "            print(\"Which surname \\n\\t 1)\", auth_surname, \"\\n\\t 2)\", a_result[2])\n",
    "            opt_name = input()\n",
    "            if opt_name == '2': auth_surname = a_result[2] \n",
    "        if auth_orcid != a_result[3]:\n",
    "            print(\"Which ORCID \\n\\t 1)\", auth_orcid, \"\\n\\t 2)\", a_result[3])\n",
    "            opt_name = input()\n",
    "            if opt_name == '2': auth_orcid = a_result[3]\n",
    "\n",
    "    if auth_id != an_author[0] or auth_name != an_author[1] or auth_surname != an_author[2] or auth_orcid != an_author[3]:\n",
    "        print (\"Will update\", an_author, \"to\", auth_id , auth_name , auth_surname,auth_orcid )\n",
    "\n",
    "    for a_result in auth_similars:\n",
    "        if auth_id != a_result[0]:\n",
    "            print(\"will update all author articles from:\", a_result[0], \"to:\", auth_id)\n",
    "            print(\"will delete author:\", a_result[0])\n",
    "    print(\"Continue?\\n\\t 1) proceed \\n\\t 2) cancel\")\n",
    "    opt_go = input()\n",
    "    if opt_go == '1':\n",
    "        update_author(an_author, auth_id, auth_name , auth_surname,auth_orcid)\n",
    "        for a_result in auth_similars:\n",
    "            if auth_id != a_result[0]:\n",
    "                update_article_authors(auth_id,a_result[0])\n",
    "                delete_author(a_result[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check authors with no articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that there are no authors with no articles in the DB\n",
    "no_artaut_authors = get_null_authors()\n",
    "\n",
    "delete_these=[]\n",
    "for an_author in no_artaut_authors:\n",
    "    dup_id = an_author[0]\n",
    "    dup_name = an_author[1]\n",
    "    dup_surname = an_author[2]\n",
    "    dup_orcid = an_author[3] if an_author[3]!=None else \"NULL\"\n",
    "    print(dup_id, dup_name, dup_surname, dup_orcid)\n",
    "    similars = get_similar_authors(dup_name, dup_surname, dup_orcid)\n",
    "    print('There are %s similar authors in DB'%(len(similars)))\n",
    "    for idx, a_simil in enumerate(similars):\n",
    "        art_count = count_linked(a_simil[0])\n",
    "        if art_count == 0:\n",
    "            print(idx, a_simil,\"Links:\", art_count, \"DELETE\")\n",
    "            delete_these.append(a_simil[0])\n",
    "        else:\n",
    "            print(idx, a_simil,\"Links:\", art_count, \"CHECK\")\n",
    "\n",
    "print(delete_these)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review duplicate authors (by name and last name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually review probable duplicate authors \n",
    "all_authors = get_all_authors()\n",
    "revise_these=[]\n",
    "\n",
    "safe_list = open_ok_list('safe_list.txt')\n",
    "pacer_idx = 0\n",
    "for an_author in all_authors:   \n",
    "    dup_id = an_author[0]\n",
    "    dup_name = an_author[1]\n",
    "    dup_surname = an_author[2]\n",
    "    dup_orcid = an_author[3] if an_author[3]!=None else \"NULL\"\n",
    "    if not(int(dup_id) in safe_list):\n",
    "        if \"'\" in dup_surname: dup_surname = dup_surname.replace(\"'\",\"''\")\n",
    "        if (\"’\") in dup_surname: dup_surname = dup_surname.replace(\"’\",\"''\")\n",
    "        all_similars = get_similar_authors(dup_name, dup_surname, dup_orcid)\n",
    "        similars = prune_similar_surnames(all_similars, dup_surname)\n",
    "        similars = prune_similar_names(similars, dup_name)\n",
    "        if len(similars) > 1 and  len(dup_surname)>3 :\n",
    "            print(\"*************************************************\")\n",
    "            print(\"Author:\", dup_id, dup_name, dup_surname, dup_orcid)\n",
    "            print('There are %s similar authors in DB'%(len(similars)))\n",
    "\n",
    "            for idx, a_simil in enumerate(similars):\n",
    "                art_count = count_linked(a_simil[0])\n",
    "                print(idx, a_simil,\"Links:\", art_count)\n",
    "            print (\"Options:\\n\\t (1) Ignore \\n\\t (2) Merge\\n\\t (3) next\")\n",
    "            sel_action = input()\n",
    "            if sel_action == '1':\n",
    "                safe_list = sorted(list(set(safe_list + [sublist[0] for sublist in similars])))\n",
    "            if sel_action == '2':\n",
    "                merge_authors(an_author, similars)\n",
    "            pacer_idx+=1\n",
    "            clear_output()\n",
    "            if pacer_idx == 10:\n",
    "                break\n",
    "print(\"OK:\", len(safe_list))\n",
    "print(safe_list)\n",
    "save_ok_list(safe_list, 'safe_list.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify affiliations and author affiliations agains crossref affiliations\n",
    "1 get group of crossref affiliations\n",
    "2 get assigned affiliation\n",
    "3 verify if OK if not show and ask for action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import craffiparser\n",
    "\n",
    "def get_parser(db_):\n",
    "    cr_parse = craffiparser.crp(db_)\n",
    "    cr_parse.start_lists()\n",
    "    return cr_parse\n",
    "\n",
    "def get_affiliations():\n",
    "\n",
    "def get_cr_affis_article_author_ids(db_name):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    a_table = 'cr_affiliations'\n",
    "    a_column = 'article_author_id'\n",
    "    cr_affis_article_author_ids = db_conn.get_value_list(a_table, a_column)\n",
    "    return cr_affis_article_author_ids\n",
    "\n",
    "def get_cr_lines_for_article_author_ids(db_name, art_author_id):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    s_table = 'cr_affiliations'\n",
    "    s_fields = '*'\n",
    "    s_where = \"article_author_id = %s\"%(art_author_id)\n",
    "    authors_list = db_conn.get_values(s_table, s_fields, s_where)\n",
    "    return authors_list\n",
    "\n",
    "def get_affiliation_id(db_name, parsed_affi):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    s_table = 'affiliations'\n",
    "    s_field = 'id'\n",
    "    for k,v in parsed_affi.items():\n",
    "        if \"'\" in v :parsed_affi[k]=v.replace(\"'\",\"''\")\n",
    "    list_where = [ k +\" = '\"+ v +\"'\" for k,v in parsed_affi.items() if k != 'address']\n",
    "    s_where = \" AND \".join(list_where) \n",
    "    s_where = s_where.replace(\"= ''\", \"IS NULL\")\n",
    "    print (s_where)\n",
    "    affi_list = db_conn.get_values(s_table, s_field, s_where)\n",
    "    affi_id = None\n",
    "    if affi_list !=[]:\n",
    "        affi_id = affi_list[0][0]\n",
    "    return affi_id\n",
    "\n",
    "# could correct the close affiliation to get all the ones with \n",
    "# same institution and compare closest match\n",
    "def get_close_affiliation_id(db_name, parsed_affi):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    s_table = 'affiliations'\n",
    "    s_field = 'id'\n",
    "    for k,v in parsed_affi.items():\n",
    "        if \"'\" in v :parsed_affi[k]=v.replace(\"'\",\"''\")\n",
    "    list_where = [ k +\" = '\"+ v +\"'\" for k,v in parsed_affi.items() if k != 'address']\n",
    "    s_where = \" AND \".join(list_where) \n",
    "    s_where = s_where.replace(\"= ''\", \"IS NULL\")\n",
    "    #print (s_where)\n",
    "    affi_list = db_conn.get_values(s_table, s_field, s_where)\n",
    "    affi_id = None\n",
    "    if affi_list !=[]:\n",
    "        affi_id = affi_list[0][0]\n",
    "    return affi_id\n",
    "\n",
    "#get the id of affiliation assigned to an author affiliation record\n",
    "def get_auth_affi_affiliation_id(db_name, aut_affi_id):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    s_table = 'author_affiliations'\n",
    "    s_field = 'affiliation_id'\n",
    "    s_where = \" id = %i\" %(aut_affi_id)\n",
    "    #print (s_where)\n",
    "    affi_list = db_conn.get_values(s_table, s_field, s_where)\n",
    "    if affi_list !=[]:\n",
    "        affi_list = list(set([an_id[0] for an_id in affi_list]))\n",
    "    return affi_list\n",
    "\n",
    "#get the ids the author affiliation records for a given author\n",
    "def get_auth_affi_id_for_author(db_name, art_aut_id):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    s_table = 'author_affiliations'\n",
    "    s_field = 'id'\n",
    "    s_where = \" article_author_id = %i\" %(art_aut_id)\n",
    "    #print (s_where)\n",
    "    affi_list = db_conn.get_values(s_table, s_field, s_where)\n",
    "    if affi_list !=[]:\n",
    "        affi_list = list(set([an_id[0] for an_id in affi_list]))\n",
    "    return affi_list\n",
    "\n",
    "def is_one_line_affi(cr_parser, str_affi):\n",
    "    is_one_liner = False\n",
    "    parsed_affi = cr_parser.split_single(str_affi)\n",
    "    parsed_no_blanks = {k:v for k,v in parsed_affi.items() if v != ''}\n",
    "    if len(parsed_no_blanks) > 1:\n",
    "        is_one_liner = True\n",
    "    return is_one_liner\n",
    "\n",
    "def check_assigned_affi_ol(db_name, cr_parser, cr_affi):\n",
    "    assigned_ok = False\n",
    "    if cr_affi[3] != None:\n",
    "        parsed_affi = cr_parser.split_single(cr_affi[1])\n",
    "        parsed_no_blanks = {k:v for k,v in parsed_affi.items() if v != ''} \n",
    "        affi_id = get_affiliation_id(db_name, parsed_affi)\n",
    "        if affi_id == None:\n",
    "            affi_id = get_close_affiliation_id(db_name, parsed_no_blanks)\n",
    "        ##############################################################################\n",
    "        # if there is no close affiliation should ask if add, assign or ignore\n",
    "        # in the case of orphan lines it is ignore\n",
    "        print(cr_affi)\n",
    "        assigned_affi_id = get_auth_affi_affiliation_id(db_name, cr_affi[3])[0]\n",
    "        \n",
    "        print('Assigned ID:', assigned_affi_id, \"Recoverd ID:\", affi_id)\n",
    "        \n",
    "        if assigned_affi_id == affi_id:\n",
    "            assigned_ok = True\n",
    "    return assigned_ok\n",
    "\n",
    "def check_assigned_affi_ml(db_name, cr_parser, cr_affi_lines, art_aut_id):\n",
    "    assigned_ok = True\n",
    "    just_affi_lines = [x[1] for x in cr_affi_lines]\n",
    "    parsed_affis = cr_parser.parse_multiline(just_affi_lines)\n",
    "    # all affiliations belong to same article author\n",
    "    aut_affis = get_auth_affi_id_for_author(db_name, art_aut_id)\n",
    "    assigned_affis = []\n",
    "    for an_aut_affi_id in aut_affis:\n",
    "        assigned_affis.append(get_auth_affi_affiliation_id(db_name, an_aut_affi_id)[0])\n",
    " \n",
    "    for one_parsed in parsed_affis:\n",
    "        affi_id = get_affiliation_id(db_name, one_parsed)\n",
    "        if affi_id == None:\n",
    "            parsed_no_blanks = {k:v for k,v in one_parsed.items() if v != ''}\n",
    "            affi_id = get_close_affiliation_id(db_name, parsed_no_blanks)\n",
    "        # if there is no close affiliation should ask if add, assign or ignore\n",
    "        # in the case of orphan lines it is ignore\n",
    "        \n",
    "        if not affi_id in assigned_affis:\n",
    "            print('Assigned ID:', affi_id, \"not in recoverd IDs list:\", assigned_affis)\n",
    "            assigned_ok = False\n",
    "        else:\n",
    "            print('Assigned ID:', affi_id, \"in recoverd IDs list:\", assigned_affis)\n",
    "    return assigned_ok\n",
    "\n",
    "##############################################################################\n",
    "# FIX AFFILIATION ISSUES\n",
    "# Likely problems:\n",
    "#   a) only one assigned to two affiliations\n",
    "#      Fixes:\n",
    "#        - add missing author affiliation\n",
    "#        - correct exiting author affiliation \n",
    "#   b) Mismatch in assigned affiliation\n",
    "#      Fixes:\n",
    "#        - correct exiting author affiliation \n",
    "#   c) Affiliation not assigned\n",
    "#      Fixes:\n",
    "#        - try to assign from existing\n",
    "#        - if no existing one, ask if new should be added\n",
    "\n",
    "def correct_oneline(db_name, cr_parser, cr_affis):\n",
    "    # get a list of parsed affis with the ids of the corresponding cr_records\n",
    "    parsed_affis  =[]\n",
    "    for a_cr_affi in cr_affis:\n",
    "        parsed_affis += cr_parser.parse_and_map_single(a_cr_affi)\n",
    "    print(parsed_affis)\n",
    "    # all belong to same article author\n",
    "    art_author_id = cr_affis[0][2]\n",
    "    \n",
    "    print (\"verifying affiliations for article author\", art_author_id)\n",
    "    \n",
    "    art_auth_affis = get_auth_affi_id_for_author(db_name, art_author_id)\n",
    "    \n",
    "    print (\"Article author affiliations:\", len(art_auth_affis), art_auth_affis )\n",
    "    \n",
    "    print (\"Parsed article author affiliations:\", len(parsed_affis) )\n",
    "\n",
    "    for affi_idx, parsed_affi in enumerate(parsed_affis):\n",
    "        print('processing', parsed_affi)\n",
    "        affi_vals = parsed_affi[0]\n",
    "        cr_affi_ids = parsed_affi[1]\n",
    "        correct_this = 0\n",
    "        if affi_idx < len(art_auth_affis):\n",
    "            correct_this = art_auth_affis[affi_idx]#\n",
    "        affi_id = get_affiliation_id(db_name, affi_vals)\n",
    "        if affi_id == None:\n",
    "            parsed_no_blanks = {k:v for k,v in affi_vals.items() if v != ''}\n",
    "            affi_id = get_close_affiliation_id(db_name, parsed_no_blanks)\n",
    "        if correct_this != 0:\n",
    "            # the affiliation does not exist but something was assigned to author affi\n",
    "            if affi_id == None:\n",
    "                print('{0:*^80}'.format('Affi does not exist'))\n",
    "                print(affi_vals)\n",
    "                affi_id = add_new_affiliation(db_name, affi_vals)\n",
    "            # if the affiliation exists    \n",
    "            if affi_id != None:\n",
    "                print('{0:*^80}'.format('Update Author Affiliatio'))\n",
    "                print('Update ID:', correct_this, 'with values:', affi_vals )\n",
    "                update_author_affiliation(db_name, correct_this, affi_id, affi_vals)\n",
    "                update_cr_aai(db_name, cr_affi_ids[0], correct_this)\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            if affi_id != None :\n",
    "                print(\"Add author affiliation for author: \", art_author_id, 'with affi:', affi_vals) \n",
    "                new_affi_id = add_author_affiliation(db_name, art_aut_id, affi_id, affi_vals)\n",
    "                #update cr_affis (assign author_affi_id)\n",
    "                for cr_id in cr_affi_ids:\n",
    "                    update_cr_aai(db_name, cr_id, new_affi_id)\n",
    "\n",
    "def correct_multiline(db_name, cr_parser, cr_affis):\n",
    "    # get a list of parsed affis with the ids of the corresponding cr_records\n",
    "    parsed_affis = cr_parser.parse_and_map_multiline(cr_affis)\n",
    "    print(parsed_affis)\n",
    "    # all belong to same article author\n",
    "    art_author_id = cr_affis[0][2]\n",
    "    \n",
    "    print (\"verifying affiliations for article author\", art_author_id)\n",
    "    \n",
    "    art_auth_affis = get_auth_affi_id_for_author(db_name, art_author_id)\n",
    "    \n",
    "    print (\"Article author affiliations:\", len(art_auth_affis), art_auth_affis )\n",
    "    \n",
    "    print (\"Parsed article author affiliations:\", len(parsed_affis) )\n",
    "    \n",
    "    if len(parsed_affis) > len(art_auth_affis):\n",
    "        missing_author_affi = True\n",
    "\n",
    "    for affi_idx, parsed_affi in enumerate(parsed_affis):\n",
    "        affi_vals = parsed_affi[0]\n",
    "        cr_affi_ids = parsed_affi[1]\n",
    "        correct_this = 0\n",
    "        if affi_idx < len(art_auth_affis):\n",
    "            correct_this = art_auth_affis[affi_idx]\n",
    "\n",
    "        affi_id = get_affiliation_id(db_name, affi_vals)\n",
    "        if affi_id == None:\n",
    "            parsed_no_blanks = {k:v for k,v in affi_vals.items() if v != ''}\n",
    "            affi_id = get_close_affiliation_id(db_name, parsed_no_blanks)\n",
    "        if correct_this != 0:\n",
    "            # if the affiliation exists\n",
    "            if affi_id != None:\n",
    "                print('Update author_affiliation:', correct_this, 'with affi:', affi_vals )\n",
    "                update_author_affiliation(db_name, correct_this, affi_id, affi_vals)\n",
    "                for cr_id in cr_affi_ids:\n",
    "                    update_cr_aai(db_name, cr_id, affi_id)\n",
    "            else:\n",
    "                print('Affi does not exist')\n",
    "                print(affi_vals)\n",
    "                \n",
    "        else:\n",
    "            if affi_id != None:\n",
    "                print(\"Add author affiliation for author: \", art_author_id, 'with affi:', affi_vals) \n",
    "                new_affi_id = add_author_affiliation(db_name, art_aut_id, affi_id, affi_vals)\n",
    "                #update cr_affis (assign author_affi_id)\n",
    "                for cr_id in cr_affi_ids:\n",
    "                    update_cr_aai(db_name, cr_id, new_affi_id)\n",
    "                \n",
    "def make_author_affiliation(art_aut_id, affi_values, addr_values):\n",
    "    # get smallest unit\n",
    "    smallest_unit = \"\" \n",
    "    #id Institution> Faculty > School > Department > Work_group + address + Country\n",
    "    if affi_values[4] != None and  len(affi_values[4]) > 0: #'work_group'\n",
    "        smallest_unit = affi_values[4]\n",
    "    elif affi_values[2] != None and len(affi_values[2]) > 0 and smallest_unit == \"\": #'department'\n",
    "        smallest_unit = affi_values[2]\n",
    "    elif affi_values[9] != None and  len(affi_values[9]) > 0 and smallest_unit == \"\": #'school'\n",
    "        smallest_unit = affi_values[9]\n",
    "    elif affi_values[3] != None and len(affi_values[3]) > 0 and smallest_unit == \"\": #'faculty'\n",
    "        smallest_unit = affi_values[3]\n",
    "       \n",
    "    ret_art_auth_affi = {}\n",
    "    ret_art_auth_affi['article_author_id'] = art_aut_id\n",
    "    if len(smallest_unit) > 0:\n",
    "        ret_art_auth_affi['name'] = smallest_unit + \", \"+  affi_values[1] #'institution'\n",
    "    else:\n",
    "        ret_art_auth_affi['name'] = affi_values[1]\n",
    "    ret_art_auth_affi['short_name'] = affi_values[1]\n",
    "    add_01 = \"\"\n",
    "    if affi_values[3] != None and affi_values[3]  != \"\" and affi_values[3] != smallest_unit:\n",
    "        add_01 = affi_values[3] \n",
    "    if affi_values[9] != None and affi_values[9] != \"\" and affi_values[9] != smallest_unit:\n",
    "        if add_01 != \"\":\n",
    "               add_01 += \", \"+ affi_values[9]\n",
    "        else:\n",
    "               add_01 += affi_values[9]\n",
    "    if affi_values[2] != None and affi_values[2] != \"\" and affi_values[2] != smallest_unit:\n",
    "        if add_01 != \"\":\n",
    "               add_01 += \", \"+ affi_values[2]\n",
    "        else:\n",
    "               add_01 += affi_values[2]\n",
    "    if add_01 != \"\":\n",
    "        ret_art_auth_affi['add_01'] = add_01\n",
    "        ret_art_auth_affi['add_02'] = addr_values[1] \n",
    "        ret_art_auth_affi['add_03'] = addr_values[2]\n",
    "        ret_art_auth_affi['add_04'] = addr_values[3]\n",
    "        ret_art_auth_affi['add_05'] = addr_values[4]\n",
    "    else:\n",
    "        ret_art_auth_affi['add_01'] = addr_values[1]\n",
    "        ret_art_auth_affi['add_02'] = addr_values[2] \n",
    "        ret_art_auth_affi['add_03'] = addr_values[3]\n",
    "        ret_art_auth_affi['add_04'] = addr_values[4]\n",
    "        \n",
    "    ret_art_auth_affi['country'] = addr_values[5]\n",
    "    ret_art_auth_affi['affiliation_id'] = affi_values[0]\n",
    "    ret_art_auth_affi['created_at'] = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    ret_art_auth_affi['updated_at'] = ret_art_auth_affi['created_at'] \n",
    "    return ret_art_auth_affi                \n",
    "\n",
    "def build_address_row(affi, affi_vals):\n",
    "    address_row = [0,None,None,None,None,None]\n",
    "    if 'address' in affi_vals:\n",
    "        address_row[1] = affi_vals['address']\n",
    "    if 'country' in affi_vals:\n",
    "        address_row[5] = affi_vals['country']\n",
    "    else:\n",
    "        address_row[5] = affi[5]\n",
    "    return address_row\n",
    "\n",
    "def add_author_affiliation(db_name, art_aut_id, affi_id, affi_values):\n",
    "    print(\"Creating \", art_aut_id, affi_id, affi_values)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    affiliation_row = list(db_conn.get_row(\"affiliations\", affi_id))[0]\n",
    "    address_row = build_address_row(affiliation_row, affi_values)\n",
    "    print(\"Affiliation values\", affiliation_row)\n",
    "    print(\"Address values\", address_row )\n",
    "    new_auth_affi = make_author_affiliation(art_aut_id, affiliation_row, address_row)\n",
    "    print('Adding:', new_auth_affi)\n",
    "    new_aa_id = db_conn.put_values_table(\"author_affiliations\", new_auth_affi.keys(), new_auth_affi.values())\n",
    "    return new_aa_id\n",
    "\n",
    "def is_affi_ok(an_affi):\n",
    "    affi_ok = True\n",
    "    # has institution and institution is not blank\n",
    "    if an_affi['institution'] == '' or an_affi['institution'] == None:\n",
    "        print('Affiliation Error: Missing institution')\n",
    "        affi_ok = False\n",
    "    if an_affi['country'] == '' or an_affi['country'] == None:\n",
    "        print('Affiliation Error: country missing')\n",
    "        affi_ok = False\n",
    "    if an_affi['sector'] == '' or an_affi['sector'] == None:\n",
    "        print('Affiliation Error: sector missing')\n",
    "        affi_ok = False\n",
    "    return affi_ok\n",
    "\n",
    "\n",
    "def add_new_affiliation(db_name, affi_values):\n",
    "    if not is_affi_ok(affi_values)\n",
    "        return 0;\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    add_update_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    affiliation_new = affi_values\n",
    "    del affiliation_new['address']\n",
    "    if 'address' in affiliation_new.keys():\n",
    "        del affiliation_new['address']\n",
    "    if 'num' in affiliation_new.keys():\n",
    "        del affiliation_new['num']\n",
    "    affiliation_new['created_at'] = add_update_time\n",
    "    affiliation_new['updated_at'] = add_update_time\n",
    "    affiliation_id = db_conn.put_values_table(\"affiliations\", affiliation_new.keys(), affiliation_new.values())\n",
    "    return affiliation_id\n",
    "\n",
    "def update_author_affiliation(db_name, aut_affi_id, affi_id, affi_values):\n",
    "    print(\"Updating\", aut_affi_id, affi_values)\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    affiliation_row = list(db_conn.get_row(\"affiliations\", affi_id))[0]\n",
    "    address_row = [0,None,None,None,None,None]\n",
    "    if 'address' in affi_values:\n",
    "        address_row[1] = affi_values['address']\n",
    "    if 'country' in affi_values:\n",
    "        address_row[5] = affi_values['country']\n",
    "    else:\n",
    "        address_row[5] = affiliation_row[5]\n",
    "   \n",
    "    auth_affi = make_author_affiliation(0, affiliation_row, address_row)\n",
    "    update_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    print(auth_affi)\n",
    "    for affi_col in auth_affi:\n",
    "        if not affi_col in [\"article_author_id\", \"created_at\"]:\n",
    "            new_value = auth_affi[affi_col]\n",
    "            if isinstance(new_value, str):# new_value != None and not isinstance(new_value, int):\n",
    "                if \"'\" in new_value: new_value = new_value.replace(\"'\",\"''\")\n",
    "                if \"’\" in new_value: new_value = new_value.replace(\"’\",\"''\")\n",
    "            print(\"updating aut_affi_id:\", aut_affi_id, \"column:\", affi_col, \"value:\", new_value)\n",
    "            db_conn.set_value_table('author_affiliations', aut_affi_id,  affi_col, new_value)\n",
    "            \n",
    "\n",
    "def update_cr_aai(db_name, cr_affi_id, auth_affi_id):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    s_table = 'cr_affiliations'\n",
    "    s_field = 'author_affiliation_id'\n",
    "    db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "    db_conn.set_value_table(s_table, cr_affi_id,  s_field , auth_affi_id)\n",
    "            \n",
    "            \n",
    "#########################################################################\n",
    "# VERIFY ARTICLE AUTHOR AFFILITIONS VS CR_AFFILIATIONS\n",
    "# 0 Verify integrity of affiliations\n",
    "# 1 Get list of article_author_ids from CR_affi\n",
    "# 2 For each article_author_id:\n",
    "#   1 Get CR_affi lines\n",
    "#   2 verify CR_affi lines\n",
    "#     1 check if \n",
    "#        a) one affiliation per cr_affi \n",
    "#           parse each single affiliation\n",
    "#           check if assigned affiliation is OK (assigned ID matches calculated ID)\n",
    "#        b) multiple lines form an affiliation (2+)\n",
    "#           parse each multi-line affiliation\n",
    "#           check if assigned affiliation is OK\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "affi_parser = get_parser(ukchapp_db)\n",
    "\n",
    "list_art_aut_ids = get_cr_affis_article_author_ids(ukchapp_db)\n",
    "\n",
    "already_ok = open_ok_list('ok_affi_list.txt')\n",
    "\n",
    "for art_aut_id in list_art_aut_ids:\n",
    "    if not art_aut_id in already_ok:\n",
    "        print ('Article Author: ', art_aut_id)\n",
    "        cr_lines = get_cr_lines_for_article_author_ids(ukchapp_db, art_aut_id)\n",
    "        print('{0:*^80}'.format('CR Affilitations found:'), \"\\n\", cr_lines)\n",
    "        all_one_liners = True\n",
    "        print('{0:*^80}'.format('Check if CR lines are one liners:'))\n",
    "        for a_cr_line in cr_lines:\n",
    "            one_line_affi = is_one_line_affi(affi_parser, a_cr_line[1])\n",
    "            print( a_cr_line[1], one_line_affi)\n",
    "            if not one_line_affi:\n",
    "                all_one_liners = False\n",
    "        if all_one_liners:\n",
    "            assigned_ok = False\n",
    "            print('{0:*^80}'.format('verify one liners'))\n",
    "            for a_cr_line in cr_lines:\n",
    "                assigned_ok = check_assigned_affi_ol(ukchapp_db, affi_parser, a_cr_line)\n",
    "                print(assigned_ok)\n",
    "                if not assigned_ok:\n",
    "                    print(\"Problems with \", a_cr_line[0])\n",
    "                    break\n",
    "            if not assigned_ok:\n",
    "                correct_oneline(ukchapp_db, affi_parser, cr_lines)\n",
    "                #break\n",
    "            else:\n",
    "                already_ok.append(art_aut_id)\n",
    "        else:\n",
    "            print('verify multiline affi')\n",
    "            assigned_ok = check_assigned_affi_ml(ukchapp_db, affi_parser, cr_lines, art_aut_id)\n",
    "            if not assigned_ok:\n",
    "                print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "                print(\"Problems with:\\n\", cr_lines[0][2], art_aut_id)\n",
    "                correct_multiline(ukchapp_db, affi_parser, cr_lines)\n",
    "                #break\n",
    "            else:\n",
    "                already_ok.append(art_aut_id)\n",
    "\n",
    "print(\"OK:\", len(already_ok))\n",
    "print(already_ok)\n",
    "save_ok_list(already_ok, 'ok_affi_list.txt')                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_ok.sort()\n",
    "print(\"OK:\", len(already_ok))\n",
    "print(already_ok)\n",
    "save_ok_list(already_ok, 'ok_affi_list.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cr_lines = get_cr_lines_for_article_author_ids(ukchapp_db, 207)\n",
    "print(cr_lines)\n",
    "parsed_affis = [[{'institution': 'University College London', 'school': '', 'department': 'Department of Chemistry', 'faculty': '', 'work_group': '', 'country': 'United Kingdom', 'address': 'London'}, [108, 109, 110, 111]], [{'institution': 'Research Complex at Harwell', 'school': '', 'department': '', 'faculty': '', 'work_group': '', 'country': '', 'address': ''}, [112]]]\n",
    "print(parsed_affis[0][0])\n",
    "del parsed_affis[0][0]['address']\n",
    "print(parsed_affis[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that pdf files exist \n",
    "\n",
    "Use the data on the articles table to verify if file are stored in the corresponding folder\n",
    "We also check that the files in the folder are all accounted for (have a corersponding record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if current_step == 2:\n",
    "    # get publication data from the ukch app\n",
    "    app_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "    dups = []\n",
    "    for idx, a_pub in enumerate(tqdm_notebook(app_pubs)):\n",
    "        pub_id = a_pub[0]\n",
    "        pub_title = a_pub[1]\n",
    "        pub_doi = a_pub[2]\n",
    "        pub_url = a_pub[3]\n",
    "        for i_indx in range(idx,len(app_pubs)):\n",
    "            if not (pub_doi is None) and pub_doi.strip().lower() ==  app_pubs[i_indx][2]:\n",
    "                print(pub_doi, \"duplicated at:\", i_indx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukchapp_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if current_step == 2:\n",
    "    for infile in tqdm_notebook(Path(\"pdf_files\").glob('*.pdf')):\n",
    "        file_found = False\n",
    "        for a_pub in app_pubs:\n",
    "            if infile.name == a_pub[4]:\n",
    "                file_found = True\n",
    "                break\n",
    "        if not file_found:\n",
    "            print(\"Not in DB:\", infile.name, \"DB Name\", a_pub[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get missing pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use regular expression to check if a given string\n",
    "# is a valid DOI, using pattern from CR\n",
    "def valid_doi(cr_doi):\n",
    "    # CR DOIS: https://www.crossref.org/blog/dois-and-matching-regular-expressions/\n",
    "    # CR DOIs re1\n",
    "    # /^10.\\d{4,9}/[-._;()/:A-Z0-9]+$/i\n",
    "    if cr_doi == None:\n",
    "        return False\n",
    "    cr_re_01 = '^10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.match(cr_re_01, cr_doi, re.IGNORECASE)\n",
    "    if compare != None and cr_doi == compare.group():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# get publication data from the ukch app\n",
    "db_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "if current_step == 2:\n",
    "    for a_pub in tqdm_notebook(db_pubs):\n",
    "        if a_pub[0] > 616:\n",
    "            pub_id = a_pub[0]\n",
    "            pub_title = a_pub[1]\n",
    "            pub_doi = a_pub[2]\n",
    "            pub_url = a_pub[3]\n",
    "            pub_pdf = a_pub[4]\n",
    "            pub_html = a_pub[5]\n",
    "            if pub_pdf == None:\n",
    "                not_in_url = True\n",
    "                print(\"ID: \", pub_id, \"Publication: \",pub_title,\n",
    "                      \"\\n\\tDOI: \", pub_doi, \" URL: \", pub_url)\n",
    "                if \"pdf\" in pub_url:\n",
    "                    print (\"\\tTry to get the pdf from URL: \", pub_url)\n",
    "                    try:\n",
    "                        response = requests.get(pub_url)\n",
    "                        content_type = response.headers['content-type']\n",
    "                        if not 'text' in content_type:\n",
    "                            #print(response.headers)\n",
    "                            cd= response.headers['content-disposition']\n",
    "                            #print(cd)\n",
    "                            fname = re.findall(\"filename=(.+)\", cd)[0]\n",
    "                            #print(fname)\n",
    "                            if not Path('pdf_files/' + pdf_file).is_file():\n",
    "                                with open('pdf_files/'+ fname +'.pdf', 'wb') as f:\n",
    "                                    f.write(response.content)\n",
    "                            else:\n",
    "                                set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            not_in_url = False\n",
    "                    except:\n",
    "                        print(\"ID: \", pub_id, \"\\nPublication: \",pub_title, \n",
    "                               \"\\nDOI: \", pub_doi, \"\\nDOI: \", pub_url) \n",
    "                if not_in_url:\n",
    "                    print(\"\\tTry to see if json file has link to pdf: \")\n",
    "                    if valid_doi(pub_doi):\n",
    "                        crjd, doi_file = pr_fns.get_cr_json_object(pub_doi)\n",
    "                        got_pdf = False\n",
    "                        if \"link\" in crjd.keys():\n",
    "                            for a_link in crjd[\"link\"]:\n",
    "                                if \"\\tURL\" in a_link.keys() and (\"pdf\" in a_link[\"URL\"] or \"pdf\" in a_link[\"content-type\"]):\n",
    "                                    cr_url = a_link[\"URL\"]\n",
    "                                    #print(\"URL: \", cr_url)\n",
    "                                    pdf_file = get_pdf_from_url(cr_url)\n",
    "                                    # if the name corresponds to a existing file, assign value to db_record\n",
    "                                    if Path('pdf_files/' + pdf_file).is_file():\n",
    "                                        print(\"\\tFile name:\", pdf_file)\n",
    "                                        set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                                        got_pdf = True\n",
    "                                    else:\n",
    "                                        print(\"\\tcould not get file from\", cr_url)\n",
    "                        else: \n",
    "                            print(\"\\tno links in json\", pub_doi)\n",
    "                    if not got_pdf and \"elsevier\" in pub_url:\n",
    "                        print(\"\\tTrying elsevier doi:\" )\n",
    "                        pdf_file = pr_fns.get_elsevier_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    elif not got_pdf and \"wiley\" in pub_url:\n",
    "                        print(\"\\tTrying wiley doi:\" )\n",
    "                        pdf_file = pr_fns.get_wiley_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    elif not got_pdf and \"pubs.acs\" in pub_url:\n",
    "                        print(\"\\tTrying acs doi:\" )\n",
    "                        pdf_file = pr_fns.get_acs_pdf(pub_doi)\n",
    "                        if Path('pdf_files/' + pdf_file).is_file():\n",
    "                            print(\"\\tFile name:\", pdf_file)\n",
    "                            pr_fns.set_pdf_file_value(pdf_file, pub_id, ukchapp_db)\n",
    "                            got_pdf = True\n",
    "                    if not got_pdf:\n",
    "                        print(\"\\tTry doi:  https://doi.org/\" + pub_doi)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pdfminer to get metadata from pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer import high_level as pdfmnr_hl\n",
    "\n",
    "# functions for PDFminer\n",
    "\n",
    "def get_pdf_text(pdf_file):\n",
    "    return pdfmnr_hl.extract_text(pdf_file)\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_ref_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if pr_fns.is_data_stmt(sentence.lower()):\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1,idx,idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the paragraph fragments with references to data\n",
    "def get_all_data_sentences(pdf_text):\n",
    "    sentences = pdf_text.split(\"\\n\")\n",
    "    groups=[]\n",
    "    for sentence in sentences:\n",
    "        if 'data' in sentence.lower() or 'inform' in sentence.lower():\n",
    "            idx = sentences.index(sentence)\n",
    "            groups.append([idx-1, idx, idx+1])\n",
    "    reduced_groups = []\n",
    "    for group in groups:\n",
    "        idx_group = groups.index(group)\n",
    "        if groups.index(group) > 0:\n",
    "            set_g = set(group)\n",
    "            # make the array before current a set\n",
    "            set_bg = set(groups[idx_group - 1])\n",
    "            # make the array after current a set\n",
    "            set_ag = set()\n",
    "            if idx_group + 1 < len(groups):    \n",
    "                set_ag = set(groups[idx_group + 1])\n",
    "            if len(set_bg.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_bg.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(set_ag.intersection(set_g)) > 0:\n",
    "                ordered_union = list(set_ag.union(set_g))\n",
    "                ordered_union.sort()\n",
    "                reduced_groups.append(ordered_union)\n",
    "            if len(reduced_groups) > 0:\n",
    "                is_in_rg = False\n",
    "                for a_rg in reduced_groups:\n",
    "                    if set_g.issubset(a_rg):\n",
    "                        is_in_rg = True\n",
    "                        break\n",
    "                if not is_in_rg:\n",
    "                    reduced_groups.append(list(set_g))\n",
    "    return_group = []\n",
    "    for sentence_group in reduced_groups:\n",
    "        full_sentence = \"\"\n",
    "        for single_sentence in sentence_group:\n",
    "            full_sentence += sentences[single_sentence].strip()\n",
    "        if not full_sentence in return_group:\n",
    "            return_group.append(full_sentence)\n",
    "    return return_group\n",
    "\n",
    "# get the http strings from references to data\n",
    "def get_http_ref(sentence):\n",
    "    http_frag = \"\"\n",
    "    if 'http' in sentence.lower():\n",
    "        idx_http = sentence.lower().index('http')\n",
    "        http_frag = sentence[idx_http:]\n",
    "        space_in_ref = True\n",
    "        while \" \" in http_frag:\n",
    "            space_idx = http_frag.rfind(\" \")\n",
    "            http_frag = http_frag[:space_idx]\n",
    "        if(http_frag[-1:]==\".\"):\n",
    "            http_frag = http_frag[:-1]\n",
    "    return http_frag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_step == 2:\n",
    "    # get publication data from the ukch app\n",
    "    db_pubs = pr_fns.get_pub_data(ukchapp_db)\n",
    "\n",
    "    # get the list of dois already mined for data \n",
    "    input_file = 'pub_data_add202012.csv'\n",
    "    id_field = 'num'\n",
    "    processed, headings = csvh.get_csv_data(input_file, id_field)\n",
    "    for id_num in processed:\n",
    "        current_title = processed[id_num]['doi']\n",
    "    processed[1]['num']\n",
    "\n",
    "    processed_dois = []\n",
    "    for entry in processed:\n",
    "        if not processed[entry]['doi'] in processed_dois:\n",
    "            processed_dois.append( processed[entry]['doi'])\n",
    "\n",
    "    data_records = {}\n",
    "    data_mentions = {}\n",
    "    ref_count = mention_count = 0\n",
    "    for a_pub in tqdm_notebook(db_pubs):\n",
    "        data_refs = []\n",
    "        data_sents = []\n",
    "        if a_pub[0] > 616:\n",
    "            pub_id = a_pub[0]\n",
    "            pub_title = a_pub[1]\n",
    "            pub_doi = a_pub[2]\n",
    "            pub_url = a_pub[3]\n",
    "            pub_pdf = a_pub[4]\n",
    "            pub_html = a_pub[5]\n",
    "            if pub_pdf == 'None':\n",
    "                print(\"*************************\")\n",
    "                print(\"Missing PDF for:\", pub_doi)\n",
    "                print(\"*************************\")\n",
    "            else:\n",
    "                pdf_file = \"pdf_files/\" + pub_pdf\n",
    "                if not Path(pdf_file).is_file():\n",
    "                    print(\"*************************\")\n",
    "                    print(\"Missing file for:\", pdf_file, \"for\", pub_doi)\n",
    "                    print(\"*************************\")\n",
    "                else: \n",
    "                    print(\"PDF filename\", pdf_file)\n",
    "                    pdf_text = get_pdf_text(pdf_file)\n",
    "                    ref_sentences = get_ref_sentences(pdf_text)\n",
    "                    data_sentences = get_all_data_sentences(pdf_text)\n",
    "                    for r_sentence in ref_sentences:\n",
    "                        dt_link = get_http_ref(r_sentence)\n",
    "                        if 'supplem' in r_sentence.lower():\n",
    "                            data_refs.append({'type':'supplementary',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                        else:\n",
    "                            data_refs.append({'type':'supporting',\"desc\":r_sentence, 'data_url':dt_link})\n",
    "                    for d_sentence in data_sentences:\n",
    "                        dt_link = get_http_ref(d_sentence)\n",
    "                        if 'supplem' in d_sentence.lower():\n",
    "                            data_sents.append({'type':'supplementary',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "                        else:\n",
    "                            data_sents.append({'type':'supporting',\"desc\":d_sentence, 'data_url':dt_link})\n",
    "            if data_refs != []:\n",
    "                for data_ref in data_refs:\n",
    "                    data_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                    data_record.update(data_ref)\n",
    "                    data_records[ref_count] = data_record\n",
    "                    ref_count += 1\n",
    "            if data_sents != []:\n",
    "                for data_sent in data_sents:\n",
    "                    sentence_record = {'id':pub_id, 'doi':pub_doi}    \n",
    "                    sentence_record.update(data_sent)\n",
    "                    data_mentions[mention_count] = sentence_record\n",
    "                    mention_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the results to a csv file to be checked to verify if data mentions are actually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if len(data_records) > 0:\n",
    "#    csvh.write_csv_data(data_records, 'pdf_data.csv')\n",
    "if current_step == 2:    \n",
    "    if len(data_mentions) > 0:\n",
    "        csvh.write_csv_data(data_mentions, 'pdf_mentions202110.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify if the mentions of data or information actually can be linked to data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "if current_step == 3:\n",
    "    print(ukchapp_db)\n",
    "    print(len(app_pubs))\n",
    "    # Open results file\n",
    "    data_mentions, dm_headers = csvh.get_csv_data('pdf_mentions202110.csv')\n",
    "    print(dm_headers)\n",
    "    art_id = ''\n",
    "    for dm in data_mentions:\n",
    "        if data_mentions[dm]['action']=='':\n",
    "            clear_output()\n",
    "            print (\"*******************************************\")\n",
    "            print (\"Article id  :\", data_mentions[dm]['id'])\n",
    "            print (\"DOI         :\", data_mentions[dm]['doi'])\n",
    "            print (\"Type        :\", data_mentions[dm]['type'], '\\tLine:', dm)\n",
    "            print (\"Description :\\n\\t\", data_mentions[dm]['desc'])\n",
    "            print (\"data_url :\", data_mentions[dm]['data_url'])\n",
    "            print (\"*******************************************\")\n",
    "            decide_action = False\n",
    "            while not decide_action:\n",
    "                print('Action:')\n",
    "                print('\\ta) review')\n",
    "                print('\\tb) none')\n",
    "                print('\\tSelect a or b:')\n",
    "                lts = input()\n",
    "                if lts == \"a\":\n",
    "                    data_mentions[dm]['action'] = 'review'\n",
    "                    decide_action = True\n",
    "                elif lts == \"b\":\n",
    "                    data_mentions[dm]['action'] = 'none'\n",
    "                    decide_action = True\n",
    "        art__id = data_mentions[dm]['id']\n",
    "        if dm > 1700:\n",
    "            break\n",
    "    if len(data_mentions) > 0:\n",
    "       csvh.write_csv_data(data_mentions, 'pdf_mentions202110.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the output after each loop cycle\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# display editable spreadsheet\n",
    "import ipysheet\n",
    "\n",
    "\n",
    "# show gds parameters in a spreadsheet on jupyter\n",
    "def show_gds(gds_group):\n",
    "    gds_list = gds_to_list(gds_group)\n",
    "    #print(gds_list)\n",
    "    #add 10 more rows in case we need more parameters\n",
    "    for i in range(10):\n",
    "        gds_list.append([(len(gds_list)-1)+1,None,None,None,None])\n",
    "    a_sheet = ipysheet.sheet(rows=len(gds_list), columns=len(gds_list[0]))\n",
    "    ipysheet.cell_range(gds_list)\n",
    "    display(a_sheet)\n",
    "    return a_sheet\n",
    "\n",
    "if current_step == 3:\n",
    "    print(ukchapp_db)\n",
    "    print(len(app_pubs))\n",
    "    # Open results file\n",
    "    data_mentions, dm_headers = csvh.get_csv_data('pdf_mentions202110.csv')\n",
    "    print(dm_headers)\n",
    "    art_id = ''\n",
    "    terminate = False\n",
    "    additional_rows = {}\n",
    "    for dm in data_mentions:\n",
    "        if data_mentions[dm]['action']=='review':\n",
    "            clear_output()\n",
    "            print (\"*******************************************\")\n",
    "            print (\"Article id  :\", data_mentions[dm]['id'])\n",
    "            print (\"DOI         :\", data_mentions[dm]['doi'])\n",
    "            print (\"Type        :\", data_mentions[dm]['type'], '\\tLine:', dm)\n",
    "            print (\"Description :\\n\\t\", data_mentions[dm]['desc'])\n",
    "            print (\"data_url :\", data_mentions[dm]['data_url'])\n",
    "            print (\"*******************************************\")\n",
    "            decide_action = False\n",
    "            while not decide_action:\n",
    "                print('Action:')\n",
    "                print('\\ta) review: https://doi.org/'+data_mentions[dm]['doi'])\n",
    "                print('\\ts) add new row')\n",
    "                print('\\td) next')\n",
    "                print('\\tf) terminate')\n",
    "                print('\\tSelect a, s, d, f:')\n",
    "                lts = input()\n",
    "                if lts == \"a\":\n",
    "                    data_mentions[dm]['action'] = 'reviewed'\n",
    "                    print ('https://doi.org/'+data_mentions[dm]['doi'])\n",
    "                    print ('link:',data_mentions[dm]['link'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['link'] = add_this\n",
    "                    print ('issue:',data_mentions[dm]['issue'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['issue'] = add_this\n",
    "                    print ('name:',data_mentions[dm]['name'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['name'] = add_this\n",
    "                    print ('file:',data_mentions[dm]['file'])\n",
    "                    add_this = input()\n",
    "                    data_mentions[dm]['file'] = add_this\n",
    "                if lts == \"s\":\n",
    "                    #add a new row\n",
    "                    new_idx = len(data_mentions) + len(additional_rows) + 1\n",
    "                    additional_rows[new_idx] = data_mentions[dm]\n",
    "                    print ('link:',additional_rows[new_idx]['link'])\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['link'] = add_this\n",
    "                    print ('issue:',additional_rows[new_idx]['issue'])\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['issue'] = add_this\n",
    "                    print ('name:',additional_rows[new_idx]['name'])\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['name'] = add_this\n",
    "                    print ('file:',additional_rows[new_idx]['file'])\n",
    "                    add_this = input()\n",
    "                    additional_rows[new_idx]['file'] = add_this\n",
    "                elif lts == \"d\":\n",
    "                    if data_mentions[dm]['action'] != 'reviewed':\n",
    "                        data_mentions[dm]['action'] = 'none'\n",
    "                    decide_action = True\n",
    "                elif lts == 'f':\n",
    "                    decide_action = True\n",
    "                    terminate = True\n",
    "        art__id = data_mentions[dm]['id']\n",
    "        if dm > 1700 or terminate:\n",
    "            break\n",
    "    if len(additional_rows)> 0 :\n",
    "        for nr in additional_rows:\n",
    "            for a_field in dm_headers:\n",
    "                data_mentions[nr][a_field] = additional_rows[nr][a_field]\n",
    "    if len(data_mentions) > 0:\n",
    "       csvh.write_csv_data(data_mentions, 'pdf_mentions202110.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_mentions = {}\n",
    "for dm in data_mentions:\n",
    "    if data_mentions[dm]['action'] in ['add', 'reviewed']:\n",
    "        filter_mentions[dm]={}\n",
    "        for a_field in dm_headers:\n",
    "            filter_mentions[dm][a_field] = data_mentions[dm][a_field]\n",
    "print('filtered mentions:', len(filter_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_do_id_list =[]\n",
    "for fm in filter_mentions:\n",
    "    art_id = int(filter_mentions[fm][\"id\"])\n",
    "    if not art_id in new_do_id_list:\n",
    "        new_do_id_list.append(art_id)\n",
    "\n",
    "# currend app DB\n",
    "ukchapp_db = \"db_files/app_db20211005.sqlite3\"\n",
    "\n",
    "no_data_pubs = pr_fns.get_pub_app_no_data(ukchapp_db)\n",
    "#print(len(ids_w_data))\n",
    "print(len(no_data_pubs))\n",
    "print(new_do_id_list, len(new_do_id_list))\n",
    "filter_mentions\n",
    "\n",
    "\n",
    "int_idx = 0\n",
    "revised_list = {}\n",
    "if Path(\"./html_revised202111.csv\").is_file():\n",
    "    revised_list, rl_headers = csvh.get_csv_data('html_revised202111.csv')\n",
    "    int_idx = len(revised_list)\n",
    "    \n",
    "already_revised =[]\n",
    "for fm in revised_list:\n",
    "    art_id = int(revised_list[fm][\"id\"])\n",
    "    if not art_id in already_revised:\n",
    "        already_revised.append(art_id)\n",
    "    \n",
    "for ndp in no_data_pubs:\n",
    "    if not ndp[0] in new_do_id_list and ndp[0] > 616 and not ndp[0] in already_revised:\n",
    "        int_idx += 1\n",
    "        pub_id = ndp[0]\n",
    "        pub_title = ndp[1]\n",
    "        pub_doi = ndp[2]\n",
    "        pub_url = ndp[3]\n",
    "        data_record = {'id':pub_id, 'doi':pub_doi, 'title':pub_title} \n",
    "        print ('id',pub_id, '\\n', pub_title)\n",
    "        decide_action = False\n",
    "        terminate = False\n",
    "        while not decide_action:\n",
    "            print('Action:')\n",
    "            print(pub_url)\n",
    "            print(\"https://doi.org/\"+pub_doi)\n",
    "            print('\\ta) no data' )\n",
    "            print('\\ts) review')\n",
    "            print('\\td) next')\n",
    "            print('\\tf) terminate')\n",
    "            print('\\tSelect a, s, d, f:')\n",
    "            lts = input()\n",
    "            if lts == \"a\":\n",
    "                data_record['action'] = 'no data'\n",
    "                data_record['issue'] = \"no data availability or supplementary data mentioned in html or pdf versions or article\"\n",
    "                revised_list[int_idx] = data_record\n",
    "                decide_action = True\n",
    "            if lts == \"s\":\n",
    "                data_record['action'] = 'review'\n",
    "                print ('issue:',data_mentions[dm]['issue'])\n",
    "                add_this = input()\n",
    "                data_record['issue'] = add_this\n",
    "                revised_list[int_idx] = data_record\n",
    "                decide_action = True\n",
    "            if lts == \"d\":\n",
    "                decide_action = True\n",
    "            elif lts == 'f':\n",
    "                decide_action = True\n",
    "                terminate = True\n",
    "        if terminate:\n",
    "            break\n",
    "\n",
    "if len(revised_list) > 0:\n",
    "    csvh.write_csv_data(revised_list, 'html_revised202111.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(revised_list) > 0:\n",
    "    csvh.write_csv_data(revised_list, 'html_revised202111.csv')\n",
    "revised_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for ChemDataExtractor\n",
    "# not used for mining data references (suplementary/raw) or to get pdf metadata\n",
    "from chemdataextractor import Document\n",
    "\n",
    "# A function for getting a list of files from the directory\n",
    "# This will be modified to get the list from a csv file\n",
    "def get_files_list (source_dir):\n",
    "    i_counter = 0\n",
    "    files_list = []\n",
    "    for filepath in sorted(source_dir.glob('*.pdf')):\n",
    "        i_counter += 1\n",
    "        files_list.append(filepath)\n",
    "    return files_list\n",
    "\n",
    "def cde_read_pdfs(a_file):\n",
    "    pdf_f = open(a_file, 'rb')\n",
    "    doc = Document.from_file(pdf_f)\n",
    "    return doc\n",
    "\n",
    "def find_doi(element_text):\n",
    "    cr_re_01 = '10.\\d{4,9}/[-._;()/:A-Z0-9]+'\n",
    "    compare = re.search(cr_re_01, element_text, re.IGNORECASE)\n",
    "    if compare != None:\n",
    "        return compare.group()\n",
    "    return \"\"\n",
    "\n",
    "def get_db_id(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"id\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_db_title(doi_value, db_name = \"app_db.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    table = 'articles'   \n",
    "    id_val = db_conn.get_value(table, \"title\", \"doi\", doi_value)\n",
    "    db_conn.close()\n",
    "    if id_val != None:\n",
    "        return id_val[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_close_dois(str_name, db_name = \"prev_search.sqlite3\"):\n",
    "    db_conn = dbh.DataBaseAdapter(db_name)\n",
    "    search_in = 'articles'\n",
    "    fields_required = \"id, doi, title, pdf_file\"\n",
    "    filter_str = \"doi like '%\"+str_name+\"%';\"\n",
    "\n",
    "    db_titles = db_conn.get_values(search_in, fields_required, filter_str)\n",
    "    db_conn.close()\n",
    "    return db_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the current app db file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app db file with path: db_files/app_db.sqlite3\n",
    "ukchapp_db = \"db_files/app_db2.sqlite3\"\n",
    "while not Path(ukchapp_db).is_file():\n",
    "    print('Please enter the name of app db file:')\n",
    "    ukchapp_db = input()\n",
    "ukchapp_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names and links for references in data mentions\n",
    "data_mentions, dm_fields = csvh.get_csv_data('pdf_mentions_filtered_02.csv', 'num')\n",
    "\n",
    "for dm in data_mentions:\n",
    "    print(\"https://doi.org/\" + data_mentions[dm]['doi'])\n",
    "    ref_name = data_mentions[dm]['ref_name']\n",
    "    while ref_name == \"\":\n",
    "        print('Please enter the name of data object:')\n",
    "        ref_name = input()\n",
    "    ref_link = data_mentions[dm]['ref_link']\n",
    "    while ref_link == \"\":\n",
    "        print('Please enter the data object link:')\n",
    "        ref_link = input()\n",
    "    data_mentions[dm]['ref_name'] = ref_name\n",
    "    data_mentions[dm]['ref_link'] = ref_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getmembers, isfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pdfminer.high_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
