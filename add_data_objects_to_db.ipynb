{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare loading of data objects to App DB\n",
    "\n",
    "A curated list of data objects referenced from a set of publications is formated to facilitate loading into the App DB.\n",
    "\n",
    "Instead of only referencing to data, these process refer to **data objects**, which are any data which is published to complement the publication, this includes raw data, supplementary data, processing data, tables, images, movies, and compilations containing one or more of such resources (corrections to publications may fall in this category but need to discuss it with stakeholders).\n",
    "\n",
    "The operations to be performed are: \n",
    "- get metadata from objects identifed with DOIs and arrange it in a way that it can be loaded to the AppDB.\n",
    "- format all objects without DOI (mostly supplementary materials) to align with the metadata from DOI identified objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library containign read and write functions to csv file\n",
    "import lib.handle_csv as csvh\n",
    "\n",
    "# managing files and file paths\n",
    "from pathlib import Path\n",
    "\n",
    "# library for handling url searchs\n",
    "import lib.handle_urls as urlh\n",
    "\n",
    "# add a progress bar\n",
    "from tqdm import tqdm_notebook\n",
    "    \n",
    "# library for accessing system functions\n",
    "import os\n",
    "\n",
    "# import custom functions (common to various notebooks)\n",
    "import processing_functions as pr_fns\n",
    "\n",
    "# Connecting to the db\n",
    "import lib.handle_db as dbh\n",
    "\n",
    "# get the publications list from the app database\n",
    "ukchapp_db = \"./db_files/production.sqlite3\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DOI objects metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get names and links for references in data mentions\n",
    "# data_reference, do_keys = csvh.get_csv_data('./new_references202111.csv')\n",
    "data_reference, do_keys = csvh.get_csv_data('./new_references202112.csv', 'num')\n",
    "for dr in tqdm_notebook(data_reference):\n",
    "    # start copying data to do fields\n",
    "    if not 'num' in do_keys:\n",
    "        data_reference[dr]['num'] = dr\n",
    "    \n",
    "    if pr_fns.valid_doi(data_reference[dr]['target_id']):\n",
    "        data_reference[dr]['do_doi'] = data_reference[dr]['target_id']\n",
    "        data_reference[dr]['do_location'] = \"https://doi.org/\" + data_reference[dr]['target_id']\n",
    "        data_reference[dr]['do_metadata'] = \"\"\n",
    "    else:\n",
    "        if not pr_fns.valid_doi(data_reference[dr]['do_doi']):\n",
    "            data_reference[dr]['do_doi'] = \"\"\n",
    "        data_reference[dr]['do_location'] = data_reference[dr]['target_id']\n",
    "        data_reference[dr]['do_metadata'] = \"\"\n",
    "\n",
    "# ast needed to parse string saved dictionary\n",
    "import ast\n",
    "\n",
    "for dr in tqdm_notebook(data_reference):\n",
    "    # get metadata if it is missing\n",
    "    'do_doi' in do_keys\n",
    "    if data_reference[dr]['do_metadata'] == \"\" and data_reference[dr]['do_doi'] != \"\":\n",
    "        ref_link = \"https://doi.org/\" + data_reference[dr]['do_doi']\n",
    "        data_object = urlh.getObjectMetadata(ref_link)\n",
    "        if 'metadata' in data_object.keys():\n",
    "            data_reference[dr]['do_metadata'] = data_object['metadata']\n",
    "        else:\n",
    "            data_reference[dr]['do_metadata'] = \"\"\n",
    "    if data_reference[dr]['do_metadata'] != \"\":\n",
    "        do_metadata = ast.literal_eval(str(data_reference[dr]['do_metadata']))\n",
    "        data_reference[dr]['do_title'] = do_metadata['title']\n",
    "        print('Title: ', do_metadata['title'])\n",
    "        if 'abstract' in do_metadata:\n",
    "            print('Abstract: ', do_metadata['abstract'])\n",
    "            data_reference[dr]['do_description'] = do_metadata['abstract']\n",
    "        print('URL: ', do_metadata['URL'])\n",
    "        data_reference[dr]['do_location'] = do_metadata['URL']\n",
    "        print('DOI: ', do_metadata['DOI'])\n",
    "        data_reference[dr]['do_doi'] = do_metadata['DOI']\n",
    "        repo_address = urlh.getBaseUrl(do_metadata['URL'])\n",
    "        print('repository:', repo_address)\n",
    "        data_reference[dr]['do_repository'] = repo_address\n",
    "        print('Type:',do_metadata['type']) \n",
    "        data_reference[dr]['do_type'] = do_metadata['type']\n",
    "        if do_metadata['type'] != 'dataset':\n",
    "            data_reference[dr]['do_inferred_type'] = 'dataset'\n",
    "# write to csv file\n",
    "if len(data_reference) > 0:\n",
    "    csvh.write_csv_data(data_reference, './data_load_202112.csv')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add metadata to file objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names and links for references in data mentions\n",
    "data_reference, do_keys = csvh.get_csv_data('./data_load_202112.csv', 'num')\n",
    "\n",
    "db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "\n",
    "for dr in tqdm_notebook(data_reference):\n",
    "    \n",
    "    # get publication metadata to fill in missing fields in DO metadata\n",
    "    ref_link = \"https://doi.org/\" + data_reference[dr]['pub_doi']\n",
    "    publication_title = db_conn.get_title(data_reference[dr]['pub_doi'])\n",
    "    if data_reference[dr]['do_doi'] == \"\":\n",
    "        if data_reference[dr]['do_file']!=\"\":\n",
    "            do_title = data_reference[dr]['do_file'].split(\"/\")[1]\n",
    "            print(\"Title: \", do_title)\n",
    "            data_reference[dr]['do_title'] = do_title\n",
    "            print(\"Description: Supplementary information for \", publication_title)\n",
    "            data_reference[dr]['do_description'] = \"Supplementary data for \" + publication_title[0]\n",
    "            repo_address = urlh.getBaseUrl(data_reference[dr]['do_location'])\n",
    "            print('URL:', data_reference[dr]['do_location'])\n",
    "            print('Repository:', repo_address)\n",
    "            data_reference[dr]['do_repository'] = repo_address\n",
    "            do_type = data_reference[dr]['do_file'][data_reference[dr]['do_file'].rfind(\".\")+1:]\n",
    "            print(\"Type: \", do_type)\n",
    "            data_reference[dr]['do_type'] = do_type\n",
    "            \n",
    "# write to csv file\n",
    "if len(data_reference) > 0:\n",
    "    csvh.write_csv_data(data_reference, './data_load_202112.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert into datasets table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names and links for references in data mentions\n",
    "data_reference, _ = csvh.get_csv_data('./data_load_202112.csv', 'num')\n",
    "\n",
    "db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "\n",
    "db_table = \"datasets\"\n",
    "table_columns = [\"dataset_complete\", \"dataset_description\",\"dataset_doi\",\"dataset_enddate\", \"dataset_location\",\n",
    "                  \"dataset_name\",\"dataset_startdate\",\"created_at\",\"updated_at\", \"ds_type\", \"repository\"]\n",
    "for dr in tqdm_notebook(data_reference):\n",
    "    if data_reference[dr]['do_location']!= \"\":\n",
    "        if data_reference[dr]['do_inferred_type'] != \"\":\n",
    "            do_type = data_reference[dr]['do_inferred_type']\n",
    "        else:\n",
    "            do_type = data_reference[dr]['do_type']\n",
    "        table_values = [None, data_reference[dr]['do_description'], data_reference[dr]['do_doi'], None, data_reference[dr]['do_location'],data_reference[dr]['do_title'], data_reference[dr]['target_published'],\n",
    "                        \"2021-11-23 14:17:00\", \"2021-11-23 14:17:00\" , do_type, data_reference[dr]['do_repository']]\n",
    "        db_conn.put_values_table(db_table, table_columns, table_values)\n",
    "        #get the id of inserted record\n",
    "        new_do_id = db_conn.get_value( db_table, \"id\", \"dataset_location\", data_reference[dr]['do_location'])[0]\n",
    "        print(new_do_id)\n",
    "        linktable = \"article_datasets\"\n",
    "        linktable_columns = [\"doi\", \"article_id\", \"dataset_id\", \"created_at\", \"updated_at\"]\n",
    "        linktable_values = [data_reference[dr]['pub_doi'], data_reference[dr]['pub_id'], new_do_id, \"2021-11-23 14:17:00\", \"2021-11-23 14:17:00\"]\n",
    "        db_conn.put_values_table(linktable, linktable_columns, linktable_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix for adding start date \n",
    "Add date of publication as start date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# create connection to the DB\n",
    "db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "# get a list of the datasets in the DB\n",
    "db_datasets = db_conn.get_full_table('datasets')\n",
    "\n",
    "for db_ds in db_datasets:\n",
    "    if db_ds[7] == None or db_ds[7] == \"\":\n",
    "        #print (db_ds)\n",
    "        # get article id\n",
    "        art_id = db_conn.get_value(\"article_datasets\", \"article_id\", \"dataset_id\", db_ds[0])\n",
    "        \n",
    "        art_pub_year = db_conn.get_value(\"articles\", \"pub_year\", \"id\", art_id[0])[0]\n",
    "        art_poy = db_conn.get_value(\"articles\", \"pub_ol_year\", \"id\", art_id[0])[0]\n",
    "        art_pom = db_conn.get_value(\"articles\", \"pub_ol_month\", \"id\", art_id[0])[0]\n",
    "        art_pod = db_conn.get_value(\"articles\", \"pub_ol_day\", \"id\", art_id[0])[0]\n",
    "        art_ppy = db_conn.get_value(\"articles\", \"pub_print_year\", \"id\", art_id[0])[0]\n",
    "        art_ppm = db_conn.get_value(\"articles\", \"pub_print_month\", \"id\", art_id[0])[0]\n",
    "        art_ppd = db_conn.get_value(\"articles\", \"pub_print_day\", \"id\", art_id[0])[0]\n",
    "        print (art_id[0],art_pub_year, art_poy, art_pom, art_pod, art_ppy, art_ppm, art_ppd)\n",
    "        if art_poy != '' and art_pom != '' and art_pod != '' and art_poy != None and art_pom != None and art_pod != None:\n",
    "            print (\"use online date: \", art_poy, art_pom, art_pod, art_ppy, art_ppm, art_ppd)\n",
    "            print(date(int(art_poy), int(art_pom), int(art_pod)))\n",
    "            db_conn.set_value_table('datasets', db_ds[0], \"dataset_startdate\", date(art_poy, art_pom, art_pod).isoformat())\n",
    "        #db_conn.set_value_table('datasets', db_ds[0], \"dataset_startdate\",art_pub_year[0])\n",
    "        elif art_poy != '' and art_pom != '' and art_poy != None and art_pom != None:\n",
    "            print (\"use online date: \", art_poy, art_pom, art_pod, art_ppy, art_ppm, art_ppd)\n",
    "            print(date(int(art_poy), int(art_pom), 1))\n",
    "            db_conn.set_value_table('datasets', db_ds[0], \"dataset_startdate\", date(art_poy, art_pom, 1).isoformat())\n",
    "        elif art_ppy != '' and art_ppm != '' and art_ppd != '' and art_ppy != None and art_ppm != None and art_ppd != None:\n",
    "            print (\"use print date: \",art_ppy, art_ppm, art_ppd)\n",
    "            print(date(art_ppy, art_ppm, art_ppd))\n",
    "            db_conn.set_value_table('datasets', db_ds[0], \"dataset_startdate\", date(art_ppy, art_ppm, art_ppd).isoformat())\n",
    "        elif art_ppy != '' and art_ppm != '' and art_ppy != None and art_ppm != None:\n",
    "            print (\"use print date: \",art_ppy, art_ppm, 1)\n",
    "            print(date(art_ppy, art_ppm, 1))\n",
    "            db_conn.set_value_table('datasets', db_ds[0], \"dataset_startdate\", date(art_ppy, art_ppm, 1).isoformat())\n",
    "        elif art_poy != '' and art_poy != None:\n",
    "            print (\"use online date: \", art_poy, 1, 1)\n",
    "            db_conn.set_value_table('datasets', db_ds[0], \"dataset_startdate\", date(art_poy, 1, 1).isoformat())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection to the DB\n",
    "db_conn = dbh.DataBaseAdapter(ukchapp_db)\n",
    "# get a list of the datasets in the DB\n",
    "db_datasets = db_conn.get_full_table('datasets')\n",
    "\n",
    "for db_ds in db_datasets:\n",
    "    saved_address = db_ds[11] \n",
    "    repo_address = urlh.getBaseUrl(db_ds[11])\n",
    "    #print(repo_address)\n",
    "    if repo_address != saved_address:\n",
    "        print(repo_address, \" replaces \", saved_address)\n",
    "        db_conn.set_value_table('datasets', db_ds[0], \"repository\", repo_address)\n",
    "    #break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
